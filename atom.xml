<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>cattle_coder</title>
  <icon>https://www.gravatar.com/avatar/7410c1dfb17495099670a2c6d610bb87</icon>
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2021-05-05T11:49:48.526Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>cattle_coder</name>
    <email>344285081@qq.com</email>
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>一. GBDT+LR简介</title>
    <link href="http://example.com/2021/05/03/GBDT+LR%E7%AE%80%E4%BB%8B/"/>
    <id>http://example.com/2021/05/03/GBDT+LR%E7%AE%80%E4%BB%8B/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T11:49:48.526Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC]</p><h2 id="一-GBDT-LR简介"><a href="#一-GBDT-LR简介" class="headerlink" title="一. GBDT+LR简介"></a>一. GBDT+LR简介</h2><p> 在协同过滤和矩阵分解存在的<strong>劣势</strong>就是仅利用了用户与物品相互行为信息进行推荐， 忽视了用户自身特征， 物品自身特征以及上下文信息等，导致生成的结果往往会比较片面。<br>     2014年由Facebook提出的GBDT+LR模型， 该模型利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 该模型能够综合利用用户、物品和上下文等多种不同的特征， 生成较为全面的推荐结果， 在CTR点击率预估场景下使用较为广泛。</p><p> 下面首先会介绍逻辑回归和GBDT模型各自的原理及优缺点， 然后介绍GBDT+LR模型的工作原理和细节。</p><h2 id="二-逻辑回归模型"><a href="#二-逻辑回归模型" class="headerlink" title="二. 逻辑回归模型"></a>二. 逻辑回归模型</h2><p>在推荐领域里面， 相比于传统的协同过滤， 逻辑回归模型能够综合利用用户、物品、上下文等多种不同的特征生成较为“全面”的推荐结果。</p><p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归成为了分类算法， 学习逻辑回归模型， 首先应该记住一句话：逻辑回归假设数据服从伯努利分布,通过极大化似然函数的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</p><p>相比于协同过滤和矩阵分解利用用户的物品“相似度”进行推荐， 逻辑回归模型将问题看成了一个分类问题， 通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某个商品或者“观看”了某个视频， 均是推荐系统希望用户产生的“正反馈”行为， 因此逻辑回归模型将推荐问题转化成了一个点击率预估问题。而点击率预测就是一个典型的二分类， 正好适合逻辑回归进行处理， 那么逻辑回归是如何做推荐的呢？ 过程如下：</p><ol><li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量</li><li>确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题， 这样就可以得到分类问题常用的损失作为目标， 训练模型</li><li> 在预测的时候， 将特征向量输入模型产生预测， 得到用户“点击”物品的概率</li><li>利用点击概率对候选物品排序， 得到推荐列表</li></ol><p> 推断过程可以用下图来表示：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201030205659103.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center"><br>这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$， 然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度， 然后进行更新$w$</p><p>这里的目标函数长下面这样：</p><p>$$ J(w)=-\frac{1}{m}\left(\sum_{i=1}^{m}\left(y^{i} \log f_{w}\left(x^{i}\right)+\left(1-y^{i}\right) \log \left(1-f_{w}\left(x^{i}\right)\right)\right)\right. $$ 求导之后的方式长这样： $$ w_{j} \leftarrow w_{j}-\gamma \frac{1}{m} \sum_{i=1}^{m}\left(f_{w}\left(x^{i}\right)-y^{i}\right) x_{j}^{i} $$ 这样通过若干次迭代， 就可以得到最终的$w$了， 关于这些公式的推导，可以参考下面给出的文章链接， 下面我们分析一下逻辑回归模型的优缺点。</p><p> <strong>优点</strong>：</p><ol><li>LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。</li><li>训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量id类特征，用id类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述</li><li>资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。</li><li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)</li></ol><p><strong>缺点</strong>：</p><ol><li>表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失</li><li>准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布</li><li>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行离散化（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。</li><li>LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</li></ol><p>所以如何自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期，是亟需解决的问题， 而GBDT模型， 正好可以自动发现特征并进行有效组合。 </p><h2 id="三-GBDT模型"><a href="#三-GBDT模型" class="headerlink" title="三. GBDT模型"></a>三. GBDT模型</h2><p>GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，GBDT在各种竞赛是大放异彩。原因有：<br>一是效果很好。<br>二是即可以用于分类也可以用于回归。<br>三是可以筛选特征。<br>所以这个模型依然是一个非常重要的模型。</p><p>GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201030211030645.png#pic_center" alt="在这里插入图片描述"></p><p>GBDT通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 GBDT对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。<br> GBDT如何来进行二分类的，因为我们要明确一点就是gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而gbdt 无论用于分类还是回归一直都是使用的CART 回归树， 那么既然是回归树， 是如何进行二分类问题的呢？</p><p>GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值， 但是二分类问题和回归问题的损失函数是不同的， 关于GBDT在回归问题上的树的生成过程， 损失函数和迭代原理可以参考给出的链接， 回归问题中一般使用的是平方损失， 而二分类问题中， GBDT和逻辑回归一样， 使用的下面这个：</p><p>$$ L=\arg \min \left[\sum_{i}^{n}-\left(y_{i} \log \left(p_{i}\right)+\left(1-y_{i}\right) \log \left(1-p_{i}\right)\right)\right] $$ 其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而$p_i$是第$i$个样本的预测值， 取值是0-1之间的概率，由于我们知道GBDT拟合的残差是当前模型的负梯度， 那么我们就需要求出这个模型的导数， 即$\frac{dL}{dp_i}$， 对于某个特定的样本， 求导的话就可以只考虑它本身， 去掉加和号， 那么就变成了$\frac{dl}{dp_i}$， 其中$l$如下： $$ \begin{aligned} l &amp;=-y_{i} \log \left(p_{i}\right)-\left(1-y_{i}\right) \log \left(1-p_{i}\right) \ &amp;=-y_{i} \log \left(p_{i}\right)-\log \left(1-p_{i}\right)+y_{i} \log \left(1-p_{i}\right) \ &amp;=-y_{i}\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)-\log \left(1-p_{i}\right) \end{aligned} $$ 如果对逻辑回归非常熟悉的话， $\left(\log \left(\frac{p_{i}}{1-p_{i}}\right)\right)$一定不会陌生吧， 这就是对几率比取了个对数， 并且在逻辑回归里面这个式子会等于$\theta X$， 所以才推出了$p_i=\frac{1}{1+e^-{\theta X}}$的那个形式。 这里令$\eta_i=\frac{p_i}{1-p_i}$, 即$p_i=\frac{\eta_i}{1+\eta_i}$, 则上面这个式子变成了：</p><p>$$ \begin{aligned} l &amp;=-y_{i} \log \left(\eta_{i}\right)-\log \left(1-\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}\right) \ &amp;=-y_{i} \log \left(\eta_{i}\right)-\log \left(\frac{1}{1+e^{\log \left(\eta_{i}\right)}}\right) \ &amp;=-y_{i} \log \left(\eta_{i}\right)+\log \left(1+e^{\log \left(\eta_{i}\right)}\right) \end{aligned} $$ 这时候，我们对$log(\eta_i)$求导， 得 $$ \frac{d l}{d \log (\eta_i)}=-y_{i}+\frac{e^{\log \left(\eta_{i}\right)}}{1+e^{\log \left(\eta_{i}\right)}}=-y_i+p_i $$ 这样， 我们就得到了某个训练样本在当前模型的梯度值了， 那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\frac{p}{1-p}=\theta^Tx$， GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为： $$ P(Y=1 \mid x)=\frac{1}{1+e^{-F_{M}(x)}} $$</p><p><strong>GBDT的优缺点</strong>：</p><p>我们可以把树的生成过程理解成自动进行多维度的特征组合的过程，从根结点到叶子节点上的整个路径(多个特征值判断)，才能最终决定一棵树的预测值， 另外，对于连续型特征的处理，GBDT 可以拆分出一个临界阈值，比如大于 0.027 走左子树，小于等于 0.027（或者 default 值）走右子树，这样很好的规避了人工离散化的问题。这样就非常轻松的解决了逻辑回归那里自动发现特征并进行有效组合的问题， 这也是GBDT的优势所在。</p><p>但是GBDT也会有一些局限性， 对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。</p><p>所以， 我们发现其实GBDT和LR的优缺点可以进行互补。</p><h2 id="四-GBDT-LR模型"><a href="#四-GBDT-LR模型" class="headerlink" title="四. GBDT+LR模型"></a>四. GBDT+LR模型</h2><p>这个模型也就是将是上面的进行结合：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201030212342597.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>训练时</strong> GBDT 建树的过程相当于自动进行的特征组合和离散化，然后从根结点到叶子节点的这条路径就可以看成是不同特征进行的特征组合，用叶子节点可以唯一的表示这条路径，并作为一个离散特征传入 LR 进行<strong>二次训练</strong>。<br>比如上图中， 有两棵树，x为一条输入样本，遍历两棵树后，x样本分别落到两颗树的叶子节点上，每个叶子节点对应LR一维特征，那么通过遍历树，就得到了该样本对应的所有LR特征。构造的新特征向量是取值0/1的。 比如左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第二个节点，编码[0,1,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[0,1,0,0,1]，这类编码作为特征，输入到线性分类模型（LR or FM）中进行分类。</p><p><strong>预测时</strong>，会先走 GBDT 的每棵树，得到某个叶子节点对应的一个离散特征(即一组特征组合)，然后把该特征以 one-hot 形式传入 LR 进行线性加权预测。</p><p>这个方案应该比较简单了， 下面有几个关键的点我们需要了解：</p><ol><li>通过GBDT进行特征组合之后得到的离散向量是和训练数据的原特征一块作为逻辑回归的输入， 而不仅仅全是这种离散特征</li><li>建树的时候用ensemble建树的原因就是一棵树的表达能力很弱，不足以表达多个有区分性的特征组合，多棵树的表达能力更强一些。GBDT每棵树都在学习前面棵树尚存的不足，迭代多少次就会生成多少棵树。</li><li>RF也是多棵树，但从效果上有实践证明不如GBDT。且GBDT前面的树，特征分裂主要体现对多数样本有区分度的特征；后面的树，主要体现的是经过前N颗树，残差仍然较大的少数样本。优先选用在整体上有区分度的特征，再选用针对少数样本有区分度的特征，思路更加合理，这应该也是用GBDT的原因。</li><li>在CRT预估中， GBDT一般会建立两类树(非ID特征建一类， ID类特征建一类)， AD，ID类特征在CTR预估中是非常重要的特征，直接将AD，ID作为feature进行建树不可行，故考虑为每个AD，ID建GBDT树。<pre><code> 1) 、非ID类树：不以细粒度的ID建树，此类树作为base，即便曝光少的广告、广告主，仍可以通过此类树得到有区分性的特征、特征组合</code></pre>2）、ID类树：以细粒度 的ID建一类树，用于发现曝光充分的ID对应有区分性的特征、特征组合</li></ol>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC]&lt;/p&gt;
&lt;h2 id=&quot;一-GBDT-LR简介&quot;&gt;&lt;a href=&quot;#一-GBDT-LR简介&quot; class=&quot;headerlink&quot; title=&quot;一. GBDT+LR简介&quot;&gt;&lt;/a&gt;一. GBDT+LR简介&lt;/h2&gt;&lt;p&gt; 在协同过滤和矩阵分解存在的&lt;str</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>Wide和Deep模型</title>
    <link href="http://example.com/2021/05/03/Wide%E5%92%8CDeep%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2021/05/03/Wide%E5%92%8CDeep%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T11:53:35.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Wide和Deep模型"><a href="#Wide和Deep模型" class="headerlink" title="Wide和Deep模型"></a>Wide和Deep模型</h1><p>@[TOC]</p><h2 id="1-点击率预估简介"><a href="#1-点击率预估简介" class="headerlink" title="1. 点击率预估简介"></a>1. 点击率预估简介</h2><p> 点击率预估是对每次广告<strong>点击情况作出预测</strong>，可以输出点击或者不点击，也可以输出该次点击的概率，后者有时候也称为pClick.</p><ul><li>点击率预估模型需要做什么？<br>点击率预估问题就是一个二分类的问题，在机器学习中可以使用逻辑回归作为模型的输出，其输出的就是一个概率值，我们可以将机器学习输出的这个概率值认为是某个用户点击某个广告的概率。</li></ul><ul><li>点击率预估与推荐算法有什么不同？<br>  广告点击率预估是需要得到某个用户对某个广告的点击率，然后结合广告的出价用于排序；而推荐算法很多大多数情况下只需要得到一个最优的推荐次序，即TopN推荐的问题。当然也可以利用广告的点击率来排序，作为广告的推荐。<h2 id="2、Wide-amp-Deep模型"><a href="#2、Wide-amp-Deep模型" class="headerlink" title="2、Wide &amp; Deep模型"></a>2、Wide &amp; Deep模型</h2>Memorization 和 Generalization是推荐系统很常见的两个概念。<br>其中Memorization指的是通过用户与商品的交互信息矩阵学习规则， 而Generalization则是泛化规则。<br>  我们前面介绍的FM算法就是很好的Generalization的例子，它可以根据交互信息学习到一个比较短的矩阵$V$，其中$v_{i}$储存着每个用户特征的压缩表示（embedding），而协同过滤与SVD都是靠记住用户之前与哪些物品发生了交互从而推断出的推荐结果，这两者推荐结果当然存在一些差异，我们的Wide&amp;Deep模型就能够融合这两种推荐结果做出最终的推荐，得到一个比之前的推荐结果都好的模型。</li></ul><p> 可以说Memorization趋向于更加保守，推荐用户之前有过行为的items。相比之下，generalization更加趋向于提高推荐系统的多样性（diversity）。Memorization只需要使用一个线性模型即可实现，而Generalization需要使用DNN实现。</p><p>下面是wide&amp;deep模型的结构图，由左边的wide部分(一个简单的线性模型)，右边的deep部分(一个典型的DNN模型)。<img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201027202444721.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>wide模型</strong>：其实就是 lr 加上 人工构造得交叉特征， 受限与训练数据，wide模型无法实现训练数据中未曾出现过得泛化。</p><p><strong>deep模型介绍</strong>：像FM，和dnn这种embedding类得模型，可以通过学习到得低维稠密向量实现模型得泛化能力，包括可以实现对未见过得内容进行泛化推荐。但是当 query-item矩阵比较稀疏得时候，模型会过分泛化，推荐出很多无相关得内同，准确性得不到保证。</p><ul><li><p>如何理解Wide部分有利于增强模型的“记忆能力”，Deep部分有利于增强模型的“泛化能力”？</p><ul><li>wide部分是一个广义的线性模型，输入的特征主要有两部分组成，一部分是原始的部分特征，另一部分是原始特征的交互特征(cross-product transformation)，对于交互特征可以定义为： $$ \phi_{k}(x)=\prod_{i=1}^d x_i^{c_{ki}}, c_{ki}\in {0,1} $$ 这个式子什么意思读者可以自行找原论文看看，大体意思就是两个特征都同时为1这个新的特征才能为1，否则就是0，说白了就是一个特征组合。 </li><li>对于wide部分训练时候使用的优化器是带$L_1$正则的FTRL算法(Follow-the-regularized-leader)，而L1 FTLR是非常注重模型稀疏性质的，也就是说W&amp;D模型采用L1 FTRL是想让Wide部分变得更加的稀疏，即Wide部分的大部分参数都为0，这就大大压缩了模型权重及特征向量的维度。<strong>Wide部分模型训练完之后留下来的特征都是非常重要的，那么模型的“记忆能力”就可以理解为发现”直接的”，“暴力的”，“显然的”关联规则的能力。</strong>例如Google W&amp;D期望wide部分发现这样的规则：用户安装了应用A，此时曝光应用B，用户安装应用B的概率大。Deep部分是一个DNN模型，输入的特征主要分为两大类，一类是数值特征(可直接输入DNN)，一类是类别特征(需要经过Embedding之后才能输入到DNN中)，Deep部分的数学形式如下： $$ a^{(l+1)} = f(W^{l}a^{(l)} + b^{l}) $$ <strong>我们知道DNN模型随着层数的增加，中间的特征就越抽象，也就提高了模型的泛化能力。</strong>对于Deep部分的DNN模型作者使用了深度学习常用的优化器AdaGrad，这也是为了使得模型可以得到更精确的解。</li></ul></li></ul><p>Wide部分与Deep部分的结合</p><p>W&amp;D模型是将两部分输出的结果结合起来联合训练，将deep和wide部分的输出重新使用一个逻辑回归模型做最终的预测，输出概率值。联合训练的数学形式如下： $$ P(Y=1|x)=\delta(w_{wide}^T[x,\phi(x)] + w_{deep}^T a^{(lf)} + b) $$</p><h2 id="3-代码实战"><a href="#3-代码实战" class="headerlink" title="3. 代码实战"></a>3. 代码实战</h2><p>Tensorflow内置的WideDeepModel<br>全局实现</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tf.keras.experimental.WideDeepModel(</span><br><span class="line">    linear_model, dnn_model, activation=None, **kwargs</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>这一步很容易看出来就是将linear_model与dnn_model拼接在了一起，对应于Wide-Deep FM中的最后一步。比如我们可以将linear_model与dnn_model做一个最简单的实现</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">linear_model &#x3D; LinearModel()</span><br><span class="line">dnn_model &#x3D; keras.Sequential([keras.layers.Dense(units&#x3D;64),</span><br><span class="line">                             keras.layers.Dense(units&#x3D;1)])</span><br><span class="line">combined_model &#x3D; WideDeepModel(linear_model, dnn_model)</span><br><span class="line">combined_model.compile(optimizer&#x3D;[&#39;sgd&#39;, &#39;adam&#39;], &#39;mse&#39;, [&#39;mse&#39;])</span><br><span class="line"># define dnn_inputs and linear_inputs as separate numpy arrays or</span><br><span class="line"># a single numpy array if dnn_inputs is same as linear_inputs.</span><br><span class="line">combined_model.fit([linear_inputs, dnn_inputs], y, epochs)</span><br><span class="line"># or define a single &#96;tf.data.Dataset&#96; that contains a single tensor or</span><br><span class="line"># separate tensors for dnn_inputs and linear_inputs.</span><br><span class="line">dataset &#x3D; tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))</span><br><span class="line">combined_model.fit(dataset, epochs)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">linear_model = LinearModel()</span><br><span class="line">linear_model.compile(<span class="string">&#x27;adagrad&#x27;</span>, <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line">linear_model.fit(linear_inputs, y, epochs)</span><br><span class="line">dnn_model = keras.Sequential([keras.layers.Dense(units=1)])</span><br><span class="line">dnn_model.compile(<span class="string">&#x27;rmsprop&#x27;</span>, <span class="string">&#x27;mse&#x27;</span>)</span><br><span class="line">dnn_model.fit(dnn_inputs, y, epochs)</span><br><span class="line">combined_model = WideDeepModel(linear_model, dnn_model)</span><br><span class="line">combined_model.compile(optimizer=[<span class="string">&#x27;sgd&#x27;</span>, <span class="string">&#x27;adam&#x27;</span>], <span class="string">&#x27;mse&#x27;</span>, [<span class="string">&#x27;mse&#x27;</span>])</span><br><span class="line">combined_model.fit([linear_inputs, dnn_inputs], y, epochs)</span><br></pre></td></tr></table></figure><h2 id="4-深度学习推荐系统的发展"><a href="#4-深度学习推荐系统的发展" class="headerlink" title="4. 深度学习推荐系统的发展"></a>4. 深度学习推荐系统的发展</h2><p>Wide&amp;Deep模型在深度学习发展中起到了非常重要的作用，从下图中我们就可以看到它对后续模型发展的一个影响。<img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/2020102721225366.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最后。如果文章中有不足之处，请务必指出，一定迅速改正。谢谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Wide和Deep模型&quot;&gt;&lt;a href=&quot;#Wide和Deep模型&quot; class=&quot;headerlink&quot; title=&quot;Wide和Deep模型&quot;&gt;&lt;/a&gt;Wide和Deep模型&lt;/h1&gt;&lt;p&gt;@[TOC]&lt;/p&gt;
&lt;h2 id=&quot;1-点击率预估简介&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>Mindspore快速上手</title>
    <link href="http://example.com/2021/05/03/Mindspore%E7%9A%84%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/"/>
    <id>http://example.com/2021/05/03/Mindspore%E7%9A%84%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T10:44:37.788Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Mindspore快速上手"><a href="#Mindspore快速上手" class="headerlink" title="Mindspore快速上手"></a>Mindspore快速上手</h3><p>本周末参加了华为mindspore深度学习框架的第三期训练营。在这次训练营中，mindspore由上次的0.3版本迅速的增加到了0.6版本。课程安排是这样的：<br>第一天：<br>                  1、快速上手mindspore指南<br>        2、学习mindspore训练yolov3模型。<br>        3、如何快速用Mindspore训练bert模型<br>        4、用Mindspore Serving完成模型部署</p><p>第二天：<br>            1、用MIndspore训练ResNet50模型<br>            2、性能大杀器——如何用Mindspore实现量化训练<br>            3、用Mindspore实现Wide&amp;Deep模型<br>通过这两天的爆肝学习，学习到很多知识，但是还有许多不明白的地方。这一篇就写出快速上手指南嘞。</p><h2 id="一、环境部署"><a href="#一、环境部署" class="headerlink" title="一、环境部署"></a>一、环境部署</h2><p>Mindspore架构：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809203002392.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><h4 id="使用环境："><a href="#使用环境：" class="headerlink" title="使用环境："></a>使用环境：</h4><h5 id="win10-anaconda-Mindspore"><a href="#win10-anaconda-Mindspore" class="headerlink" title="win10+anaconda+Mindspore"></a>win10+anaconda+Mindspore</h5><h4 id="步骤如下："><a href="#步骤如下：" class="headerlink" title="步骤如下："></a>步骤如下：</h4><p>安装配置如下：<br>使用是Mindspore的0.5版本滴<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809203317128.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>1、先创建虚拟环境<br>先点击这个<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809203614416.png" alt="在这里插入图片描述"><br>使用下面命令进行创建环境并进入环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mindspore python=3.7.5 </span><br><span class="line">conda activate mindspore</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/2020080920383195.png" alt="在这里插入图片描述"><br>2、安装Mindspore框架</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install https://ms-release.obs.cn-north-4.myhuaweicloud.com/0.5.0-beta/MindSpore/cpu/windows_x64/mindspore-0.5.0-cp37-cp37m-win_amd64.whl</span><br></pre></td></tr></table></figure><p>这里会自动安装MIndspore运行其他的包或者库的</p><p>3.下载代码</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/mindspore-ai/docs.git</span><br></pre></td></tr></table></figure><p>4、进入目录，使用cd命令<br>lenet.py文件在./doc/tutorials/tutorial_code目录下面，使用cd命令依次进入<br>如图：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204254110.png" alt="在这里插入图片描述"><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204314176.png" alt="在这里插入图片描述"><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204323653.png" alt="在这里插入图片描述"></p><p>运行lenet.py文件<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204332819.png" alt="在这里插入图片描述"><br>5.结果、结束。<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204713534.png" alt="在这里插入图片描述"><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809204725827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;Mindspore快速上手&quot;&gt;&lt;a href=&quot;#Mindspore快速上手&quot; class=&quot;headerlink&quot; title=&quot;Mindspore快速上手&quot;&gt;&lt;/a&gt;Mindspore快速上手&lt;/h3&gt;&lt;p&gt;本周末参加了华为mindspore深度学习框架的第三</summary>
      
    
    
    
    <category term="Mindspore" scheme="http://example.com/categories/Mindspore/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>python开发先创建虚拟环境呀</title>
    <link href="http://example.com/2021/05/03/python%E5%BC%80%E5%8F%91%E5%85%88%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%91%80/"/>
    <id>http://example.com/2021/05/03/python%E5%BC%80%E5%8F%91%E5%85%88%E5%88%9B%E5%BB%BA%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E5%91%80/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T11:52:15.060Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、python虚拟环境概述（virtual-environment）"><a href="#一、python虚拟环境概述（virtual-environment）" class="headerlink" title="一、python虚拟环境概述（virtual environment）"></a>一、python虚拟环境概述（virtual environment）</h3><p>它是一个虚拟化，从电脑独立开辟出来的环境。通俗的来讲，虚拟环境就是借助虚拟机docker（容器）来把一部分内容独立出来，我们把这部分独立出来的东西称作“容器”，在这个容器中，我们可以只安装我们需要的依赖包，各个容器之间互相隔离，互不影响。譬如，本次学习需要用到mindspore学习，我就创建一个mindspore的虚拟环境，其中的一些mindspore的项目使用到的numpy，pandas等版本问题，就可以重新安装而不影响。</p><h2 id="二、虚拟环境作用"><a href="#二、虚拟环境作用" class="headerlink" title="二、虚拟环境作用"></a>二、虚拟环境作用</h2><p>不同的项目，使用的包或库版本是不同，如果新老项目都使用同一个环境话，可能会导致最新的项目在包升级的时候把包升级到最新版，导致老项目代码将出现问题，这样将会非常的混乱。 虚拟环境就可以很好地把他们给分割出来<br>比如：我在安装mindspore的版本号时，他就需要pandas在版本1.8以下的，我只有重新安装才可以。但是之前我电脑中已经存在了，就需要重新安装，但是我重新安装后，我的TensorFlow框架就需要1.8以上，这就产生冲突，导致以前面目不能使用。这时候我使用虚拟环境就可以使他们隔离，两者运行环境不再影响。</p><h3 id="三、虚拟环境使用"><a href="#三、虚拟环境使用" class="headerlink" title="三、虚拟环境使用"></a>三、虚拟环境使用</h3><p>在这里我只在anaconda中创建了虚拟环境，就给出anaconda中虚拟环境使用<br>1.打开anaconda的 prompt<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809142219889.png" alt="在这里插入图片描大述"><br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809142342746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="anaconda描述"></p><p>2.使用conda命令创建虚拟环境</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n environment_name python=X.X</span><br></pre></td></tr></table></figure><p>我这里是创建名字为mindspore的虚拟环境使用python=3.7.5</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n mindspore python=3.7.5 </span><br></pre></td></tr></table></figure><p>3.进入mindspore环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda activate mindspore</span><br></pre></td></tr></table></figure><p>之后就ok啦，就创建好了<br>创建的环境就可以在jupyter等上面使用了。</p><h4 id="打开这个"><a href="#打开这个" class="headerlink" title="打开这个"></a>打开这个</h4><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809143028871.png"></p><p>在这里更改虚拟环境<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809143402107.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"><br>点击launch进入就行啦<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200809143504398.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述">conda env list<br>conda info -e</p><h3 id="三、常见虚拟环境中命令"><a href="#三、常见虚拟环境中命令" class="headerlink" title="三、常见虚拟环境中命令"></a>三、常见虚拟环境中命令</h3><p>在anaconda中虚拟环镜中，这几个是常见的一些命令，我们可以先看一下，多敲几遍就会啦：</p><p>列出有哪些虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda env list</span><br><span class="line">conda info -e</span><br></pre></td></tr></table></figure><p>创建虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda create -n your_env_name python=X.X</span><br></pre></td></tr></table></figure><p>激活虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux: <span class="built_in">source</span> activate your_env_name</span><br><span class="line">     </span><br><span class="line">Windows: activate your_env_name</span><br></pre></td></tr></table></figure><p>在虚拟环境中安装额外的包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install -n your_env_name [package]</span><br></pre></td></tr></table></figure><p>关闭虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Linux:<span class="built_in">source</span> deactivate</span><br><span class="line">     </span><br><span class="line">Windows:deactivate</span><br></pre></td></tr></table></figure><p>删除虚拟环境</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove -n your_env_name --all</span><br></pre></td></tr></table></figure><p>删除环境中某一个包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda remove --name <span class="variable">$your_env_name</span> <span class="variable">$package_name</span></span><br></pre></td></tr></table></figure><p>最后这章虚拟说明，让我们知道在进行一个项目时候，我们创建一个虚拟环境是对我们项目开发是非常好滴。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h3 id=&quot;一、python虚拟环境概述（virtual-environment）&quot;&gt;&lt;a href=&quot;#一、python虚拟环境概述（virtual-environment）&quot; class=&quot;headerlink&quot; title=&quot;一、python虚拟环境概述（virtual</summary>
      
    
    
    
    <category term="霍乱时期的python" scheme="http://example.com/categories/%E9%9C%8D%E4%B9%B1%E6%97%B6%E6%9C%9F%E7%9A%84python/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统task01：简介概述</title>
    <link href="http://example.com/2021/05/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9Ftask01%EF%BC%9A%E7%AE%80%E4%BB%8B%E6%A6%82%E8%BF%B0/"/>
    <id>http://example.com/2021/05/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9Ftask01%EF%BC%9A%E7%AE%80%E4%BB%8B%E6%A6%82%E8%BF%B0/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:29:06.361Z</updated>
    
    <content type="html"><![CDATA[<h2 id="推荐系统task01：简介概述"><a href="#推荐系统task01：简介概述" class="headerlink" title="推荐系统task01：简介概述"></a>推荐系统task01：简介概述</h2><h3 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h3><p>推荐系统这一个概念大家都知道，只要上过淘宝，京东等都是会了解到推荐系统的。比如我们在淘宝时候，在一件商品页面上停留时间长一些，网页都会可有可无的刷新出类似的商品。下面介绍一下推荐系统。<br>@[TOC] </p><h3 id="一、推荐系统作用"><a href="#一、推荐系统作用" class="headerlink" title="一、推荐系统作用"></a>一、推荐系统作用</h3><ul><li><strong>用户</strong>：在现在信息爆炸时代，信息及其多，我们在浏览作业时，琳琅满目的。但是推荐系统就可以帮助用户快速发现有用信息的工具</li><li><strong>商家</strong>：通过推荐系统可以提供个性化服务，提高用户信任度和粘性也就是购买性</li><li>其本质也就是实现将用户-商家-平台之间利益最大化的手段.满足用户，满足商家，满足平台。三方共赢<h3 id="二、推荐系统应用"><a href="#二、推荐系统应用" class="headerlink" title="二、推荐系统应用"></a>二、推荐系统应用</h3>推荐系统应用在各种场景中，拥有着广泛案例。比如个性化音乐，电子商务（淘宝），电影视频(抖音)，社交网络等等<h3 id="三、推荐系统的分类"><a href="#三、推荐系统的分类" class="headerlink" title="三、推荐系统的分类"></a>三、推荐系统的分类</h3></li><li>根据实时性分类<br>1）离线推荐<br>2）实时推荐</li><li>根据推荐原则分类<br>1）基于相似度推荐<br>2）基于知识推荐<br>3）基于模型推荐</li><li>根据推荐是否个性化分类<br>1）基于统计推荐<br>2）个性化推挤</li></ul><blockquote><p><strong>注</strong>：还有其他不同的分类，可以项亮的《推荐系统实践》</p></blockquote><h3 id="四-常用评测指标"><a href="#四-常用评测指标" class="headerlink" title="四. 常用评测指标"></a>四. 常用评测指标</h3><h5 id="4-1-用户满意度"><a href="#4-1-用户满意度" class="headerlink" title="4.1.用户满意度"></a>4.1.用户满意度</h5><p>用户是推荐系统中非常重要的参与者,他们的满意度也直接决定了推荐系统的好坏. 但用户满意度这个指标无法离线计算, 只能通过用户调查或者在线获得. 一般是通过用户的线上行为统计得到的, 比如电商场景中, 用户如果购买了推荐的商品说明一定程度上他们是满意的, 因此可以通过<strong>购买率度量用户的满意度</strong>,与购买率类似的<strong>点击率</strong>, 用户<strong>停留时间</strong>和<strong>转化率</strong>等指标都可以用来度量用户的满意度.</p><h5 id="4-2-预测准确度"><a href="#4-2-预测准确度" class="headerlink" title="4.2.预测准确度"></a>4.2.预测准确度</h5><p>预测准确度是用来度量用户的实际行为与推荐系统预测结果的准确度, 该指标是最重要的离线评价指标, 因为可以通过离线计算得到. 下面是预测准确度最常用的两个指标.</p><ul><li><p>评分预测<br>预测用户对物品的评分行为成为评分预测, 通过对用户的历史物品评分记录进行建模, 进而得到用户的兴趣模型, 然后使用该模型预测用户未未见过商品的评分.评分预测的预测准确度一般通过均方根误差(RMSE)和平均绝对误差(MAE)计算.对于测试集中的一个用户$u$和物品$i$,令$r_{ui}$是用户$u$对物品$i$的实际评分,而$\hat{r_{ui}}$是推荐模型预测出的评分,那么RMSE可以定义为: $$ RMSE = \sqrt{\frac{\sum_{u,i \in T}(r_{ui} - \hat{r}{ui})^2}{|T|}} $$ MAE定义为: $$ MAE = \frac{\sum{u,i \in T}|r_{ui} - \hat{r}_{ui}|}{|T|} $$ RMSE由于存在平方项，使得使得用户真实评分与推荐系统预测评分相差较大的用户加大了惩罚，即该评测指标对系统要求更加的苛刻</p></li><li><p>TopN推荐<br>推荐系统在给用户推荐物品的时候,往往会给用户一个列表的推荐物品,这种场景下的推荐成为是TopN推荐,该推荐方式最常用的预测准确率指标一般是精确率(precision)和召回率(recall),令$R(u)$为通过推荐模型得到的推荐列表,$T(u)$为用户在实际场景中(测试集)的行为列表.</p><p>  1)、 精确率(precision): 分类正确的正样本个数占分类器判定为正样本的样本个数比例(这里$R(u)$相当于是模型判定的正样本) $$ Precision= \frac{\sum_{u \in U}|R(u) \cap T(u)|}{\sum_{u \in U}|R(u)|} $$</p><p> 2）、召回率(recall): 分类正确的正样本个数占真正的正样本个数的比例<br> (这里的$T(u)$相当于真正的正样本集合)</p></li></ul><p>$$ Recall= \frac{\sum_{u \in U}|R(u) \cap T(u)|}{\sum_{u \in U}|T(u)|} $$</p><p>有时候为了更加全面的评估TopN推荐,通常会选取不同的推荐列表长度计算多组精确率与召回率然后分别绘制出精确率曲线和召回率曲线,需要注意的是这里并不是PR曲线,感兴趣的可以了解一下PR曲线相关的知识.</p><h5 id="4-3-覆盖率"><a href="#4-3-覆盖率" class="headerlink" title="4.3  覆盖率"></a>4.3  覆盖率</h5><p>他是用来描述一个推荐系统对物品长尾的发掘能力,一个简单的定义可以是:推荐系统所有推荐出来的商品集合数占总物品集合数的比例.但是对于相同的覆盖率,不同物品的数量分布,或者说是物品的流行度分布是可以不一样的.为了更好的描述推荐系统挖掘长尾的能力,需要统计不同物品出现次数的分布.如果所有的物品都出现在推荐列表中,并且出现的次数都差不多,那么推荐系统发掘长尾的能力就很好.所以可以通过研究物品在推荐列表中出现的次数分布来描述推荐系统挖掘长尾的能力,如果这个分布比较平缓说明推荐系统的覆盖率比较高,而如果分布比较陡说明推荐系统的覆盖率比较低.下面分别使用信息熵和基尼系数来定义覆盖率.</p><p>信息熵定义覆盖率: 其中$p(i)$是物品$i$的流行度除以所有物品流行度之和 $$ H = -\sum_{i=1}^n p(i) logp(i) $$ 基尼系数定义覆盖率: 其中$i_j$是按照物品流行度p从小到大排序的物品列表中第$j$个物品 $$ G=\frac{1}{n-1} \sum_{j=1}^{n}(2j-n-1)p(i_{j}) $$</p><h5 id="4-4、-多样性"><a href="#4-4、-多样性" class="headerlink" title="4.4、 多样性"></a>4.4、 多样性</h5><p>人的兴趣爱好是比较广泛的,一个好的推荐系统得到的推荐列表中应该尽可能多的包含用户的兴趣,只有这样才能增加用户找到感兴趣物品的概率.度量推荐列表中物品的多样性换句话说就是度量推荐列表中所有物品之间的不相似性,可以通过不同的相似性函数来度量推荐列表中商品的相似性,比如商品基于内容的相似,基于协同过滤的相似,这样就可以得到不同角度的多样性.令函数$s(i,j)$为物品$i$和物品$j$的相似性,那么用户推荐列表的多样性可以定义为: $$ Diversity(R(u))=1-\frac{\sum_{i,j \in R(u)}s(i,j)}{\frac{1}{2}|R(u)|(|R(u)|-1)} $$ 推荐系统整体的多样性可以定义为所有用户推荐列表多样性的平均值: $$ Diversity = \frac{1}{U} \sum_{u\in U}Diversity(R(u)) $$</p><h5 id="4-5、-新颖性"><a href="#4-5、-新颖性" class="headerlink" title="4.5、 新颖性"></a>4.5、 新颖性</h5><p>满足推荐的新颖性最简单的方法就是给用户推荐他们之前没有看过的物品,但是每个用户没见过的物品数量是非常庞大的,所以一般会计算推荐物品的平均流行度,流行度越低的物品越有可能让用户觉得新颖,因此,如果推荐结果中的物品平均热门程度比较低说明推荐的结果就可能比较新颖.</p><h5 id="4-6、AUC曲线"><a href="#4-6、AUC曲线" class="headerlink" title="4.6、AUC曲线"></a>4.6、AUC曲线</h5><p>AUC（Area Under Curve），ROC曲线下与坐标轴围成的面积</p><p>在讲AUC前需要理解混淆矩阵，召回率，精确率，ROC曲线等概念<br>TP：真的真了（真实值是真的，预测也是真）<br>FN：真的假了（真实值是真的，预测为假了）<br>FP：假的真了（真实值是假的，预测为真了）<br>TN：假的假了（真实值是假的，预测也是假）</p><p>召回率与准确率(上述已经进行了说明)，下面是另一种形势的定义，本质上都是一样的： $$ Recall = \frac{TP}{TP+FN}\ Precise=\frac{TP}{TP+FP} $$ ROC曲线：<br>ROC曲线的横坐标为假阳性率（False Positive Rate, FPR），N是真实负样本的个数， FP是N个负样本中被分类器预测为正样本的个数。纵坐标为真阳性率（True Positive Rate, TPR），P是真实正样本的个数，TP是P个正样本中被分类器预测为正样本的个数。</p><h5 id="4-6、商业目标"><a href="#4-6、商业目标" class="headerlink" title="4.6、商业目标"></a>4.6、商业目标</h5><p>目标商家想要目的</p><p><strong>评价指标有12这样</strong></p><blockquote><p><strong>注</strong>：还有其他的评价指标具体可以参考项亮的《推荐系统实践》</p></blockquote><h3 id="五、召回"><a href="#五、召回" class="headerlink" title="五、召回"></a>五、召回</h3><h5 id="5-1、召回层在推荐系统架构中的位置及作用"><a href="#5-1、召回层在推荐系统架构中的位置及作用" class="headerlink" title="5.1、召回层在推荐系统架构中的位置及作用"></a>5.1、召回层在推荐系统架构中的位置及作用</h5><p>在推荐系统架构中<strong>召回层与排序层</strong>是推荐系统的<strong>核心算法层</strong>，而将推荐过程分成召回层与排序层主要是基于工程上的考虑，召回阶段负责将海量的候选集快速缩小为几万到几千的规模；而排序层则负责对缩小后的候选集进行精准排序。所以在召回阶段往往会利用少量的特征和简单的模型对大规模的数据集进行快速的筛选，而在排序层一般会使用更多的特征和更加复杂的模型进行精准的排序。</p><p>下面是召回层与排序层的特点</p><ul><li><p>召回层<br>待计算的候选集合大、计算速度快、模型简单、特征较少，尽量让用户感兴趣的物品在这个阶段能够被快速召回，即保证相关物品的召回率</p></li><li><p>排序层<br>首要目标是得到精准的排序结果。需要处理的物品数量少，可以利用较多的特征，使用比较复杂的模型。</p></li></ul><p>注意：<br>在设计召回层时，“计算速度”和“召回率”其实是矛盾的两个指标，为提高“计算速度”，需要使召回策略尽量简单一些；而为了提高“召回率”，要求召回策略尽量选出排序模型所需要的候选集，这也就要求召回策略不能过于简单。在权衡计算速度和召回率后，目前工业界主流的召回方法是采用多个简单策略叠加的“多路召回策略”</p><h5 id="5-2、多路召回策略"><a href="#5-2、多路召回策略" class="headerlink" title="5.2、多路召回策略"></a>5.2、多路召回策略</h5><p>所谓的“多路召回”策略，就是指采用不同的策略、特征或简单模型，分别召回一部分候选集，然后把候选集混合在一起供后续排序模型使用，可以明显的看出，“多路召回策略”是在“计算速度”和“召回率”之间进行权衡的结果。其中，各种简单策略保证候选集的快速召回，从不同角度设计的策略保证召回率接近理想的状态，不至于损伤排序效果。<img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201019221646967.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h5 id="5-3、-Embedding召回"><a href="#5-3、-Embedding召回" class="headerlink" title="5.3、 Embedding召回"></a>5.3、 Embedding召回</h5><p>在当前的主流推荐系统中，Embedding的身影已经无处不在，从一定意义上可以说，把Embedding做好了，整个推荐系统的一个难题就攻克了。</p><ul><li>Embedding是什么？<br>Embedding其实是一种思想，主要目的是将稀疏的向量(如one-hot编码)表示转换成稠密的向量。</li><li>常见的Embedding技术有哪些？<br>目前主流的Embedding技术主要可以分为三大类。<br> 1）text embedding<br>2）image embedding<br>3）graph embedding</li></ul><h3 id="六、推荐系统核心算法"><a href="#六、推荐系统核心算法" class="headerlink" title="六、推荐系统核心算法"></a>六、推荐系统核心算法</h3><ul><li>协同过滤算法:<br>包括基于用户的协同过滤(UserCF)和基于商品的协同过滤(ItemCF)，这是入门推荐系统的人必看的内容，因为这些算法可以让初学者更加容易的理解推荐算法的思想。</li><li>矩阵分解算法:<br>矩阵分解算法通过引入了隐向量的概念，加强了模型处理稀疏矩阵的能力，也为后续深度学习推荐系统算法中Embedding的使用打下了基础。</li><li>FM(Factorization Machines):<br>该算法属于对逻辑回归(LR)算法应用在推荐系统上的一个改进，在LR模型的基础上加上了特征交叉项，该思想不仅在传统的推荐算法中继续使用过，在深度学习推荐算法中也对其进行了改进与应用。<br>GBDT+LR: 该模型仍然是对LR模型的改进，使用树模型做特征交叉，相比于FM的二阶特征交叉，树模型可以对特征进行深度的特征交叉，充分利用了特征之间的相关性。</li><li>Wide&amp;Deep:<br>该模型在推荐系统发展中的重要地位。该算法模型的思想与实现都比较的简单，非常适合初学深度学习推荐系统的学习者们去学习。</li></ul><p>最后。如果文章中有不足之处，请务必指出，一定迅速改正。谢谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;推荐系统task01：简介概述&quot;&gt;&lt;a href=&quot;#推荐系统task01：简介概述&quot; class=&quot;headerlink&quot; title=&quot;推荐系统task01：简介概述&quot;&gt;&lt;/a&gt;推荐系统task01：简介概述&lt;/h2&gt;&lt;h3 id=&quot;前言&quot;&gt;&lt;a href=&quot;</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>数据结构中迷宫问题求解</title>
    <link href="http://example.com/2021/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%AD%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3/"/>
    <id>http://example.com/2021/05/03/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%AD%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98%E6%B1%82%E8%A7%A3/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:26:08.075Z</updated>
    
    <content type="html"><![CDATA[<h2 id="迷宫问题求解"><a href="#迷宫问题求解" class="headerlink" title="迷宫问题求解"></a>迷宫问题求解</h2><p>问题描述：<br>    给定一个迷宫从入口到出口的路径，具体要求如下：<br>       1.迷宫以16*16的矩阵存储在本地文件中，格式自定义。<br>       2.迷宫障碍占一定的比列，<br>       3.非递归形式求解问题<br>       4.汇出迷宫路径（以命令行或者GUI）<br>       5.无路可走时，请给出提示</p><h4 id="本节是数据结构中运用栈思想去求解迷宫问题，本算法实现简单，代码仅供参考，如有疑问请评论联系："><a href="#本节是数据结构中运用栈思想去求解迷宫问题，本算法实现简单，代码仅供参考，如有疑问请评论联系：" class="headerlink" title="本节是数据结构中运用栈思想去求解迷宫问题，本算法实现简单，代码仅供参考，如有疑问请评论联系："></a>本节是数据结构中运用栈思想去求解迷宫问题，本算法实现简单，代码仅供参考，如有疑问请评论联系：</h4><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COUNT_I 17</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> COUNT_J 17</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> START_I 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> START_J 0</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> END_I 15</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> END_J 15</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> MAXSIZE 1000</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//坐标位置结构体</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">local</span>&#123;</span></span><br><span class="line"><span class="keyword">int</span> x;</span><br><span class="line"><span class="keyword">int</span> y;</span><br><span class="line">&#125;LOCAL;</span><br><span class="line"></span><br><span class="line"><span class="comment">//栈结构</span></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">stack</span>&#123;</span></span><br><span class="line"></span><br><span class="line">LOCAL data[MAXSIZE];</span><br><span class="line"><span class="keyword">int</span> top;</span><br><span class="line"><span class="keyword">int</span> base;</span><br><span class="line"></span><br><span class="line">&#125;STACK;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化栈</span></span><br><span class="line"><span class="function">STACK *<span class="title">InitStack</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">STACK *maze;</span><br><span class="line">maze = (STACK *)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(STACK));</span><br><span class="line">maze-&gt;top = maze-&gt;base=<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> maze;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判栈空</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">EmptyStack</span><span class="params">(STACK *maze)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maze-&gt;top ==maze-&gt;base)</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//判栈满</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">IsFull</span><span class="params">(STACK *maze)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maze-&gt;top-maze-&gt;base== MAXSIZE)</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//入栈</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PushStack</span><span class="params">(STACK *maze, LOCAL *x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maze-&gt;top &lt;= MAXSIZE - <span class="number">1</span>)&#123;</span><br><span class="line">maze-&gt;data[++maze-&gt;top] = *x;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;栈已满\n&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//出栈</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">PopStack</span><span class="params">(STACK *maze, LOCAL *x)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">if</span> (maze-&gt;top &gt;maze-&gt;base)&#123;</span><br><span class="line">*x = maze-&gt;data[maze-&gt;top];</span><br><span class="line">maze-&gt;top--;</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;栈已空\n&quot;</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//走迷宫函数</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">VistMaze</span><span class="params">(<span class="keyword">int</span> maze[][COUNT_J], LOCAL path[][COUNT_J])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">int</span> i, j;</span><br><span class="line"></span><br><span class="line"><span class="comment">//初始化栈</span></span><br><span class="line">STACK *<span class="built_in">stack</span>;</span><br><span class="line">LOCAL temp;</span><br><span class="line"><span class="built_in">stack</span> = InitStack();</span><br><span class="line">temp.x = <span class="number">0</span>; temp.y = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">if</span> (maze[START_I][START_J] == <span class="number">0</span>)</span><br><span class="line">PushStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(!EmptyStack(<span class="built_in">stack</span>))&#123;</span><br><span class="line">PopStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line">i = temp.x;j = temp.y;</span><br><span class="line">maze[i][j] = <span class="number">2</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (i == END_I &amp;&amp; j == END_J)</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">//下</span></span><br><span class="line"><span class="keyword">if</span> (i + <span class="number">1</span> &lt;= END_I &amp;&amp; maze[i + <span class="number">1</span>][j] == <span class="number">0</span>)&#123;</span><br><span class="line">maze[i + <span class="number">1</span>][j] = <span class="number">2</span>;</span><br><span class="line">path[i + <span class="number">1</span>][j].x = i;</span><br><span class="line">            path[i + <span class="number">1</span>][j].y = j;</span><br><span class="line">temp.x = i + <span class="number">1</span>;</span><br><span class="line">temp.y = j;</span><br><span class="line">PushStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//右</span></span><br><span class="line"><span class="keyword">if</span> (j + <span class="number">1</span> &lt;= END_J &amp;&amp; maze[i][j + <span class="number">1</span>] == <span class="number">0</span>)&#123;</span><br><span class="line">maze[i][j + <span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line">path[i][j + <span class="number">1</span>].x = i;path[i][j + <span class="number">1</span>].y = j;</span><br><span class="line">temp.x = i;</span><br><span class="line">temp.y = j + <span class="number">1</span>;</span><br><span class="line">PushStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//左</span></span><br><span class="line"><span class="keyword">if</span> (j - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; maze[i][j - <span class="number">1</span>] == <span class="number">0</span>)&#123;</span><br><span class="line">maze[i][j - <span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line">path[i][j - <span class="number">1</span>].x = i;path[i][j - <span class="number">1</span>].y = j;</span><br><span class="line">temp.x = i;</span><br><span class="line">temp.y = j - <span class="number">1</span>;</span><br><span class="line">PushStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//上</span></span><br><span class="line"><span class="keyword">if</span> (i - <span class="number">1</span> &gt;= <span class="number">0</span> &amp;&amp; maze[i - <span class="number">1</span>][j] == <span class="number">0</span>)&#123;</span><br><span class="line">maze[i - <span class="number">1</span>][j] = <span class="number">2</span>;</span><br><span class="line">path[i - <span class="number">1</span>][j].x = i;path[i - <span class="number">1</span>][j].y = j;</span><br><span class="line">temp.x = i - <span class="number">1</span>;</span><br><span class="line">temp.y = j;</span><br><span class="line">PushStack(<span class="built_in">stack</span>, &amp;temp);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//如果到达终点而退出的循环则将路径标识出来</span></span><br><span class="line"><span class="keyword">if</span> (i == END_I &amp;&amp; j == END_J)&#123;</span><br><span class="line">maze[i][j] = <span class="number">3</span>;</span><br><span class="line"><span class="keyword">while</span>(path[temp.x][temp.y].x != <span class="number">-1</span>)&#123;</span><br><span class="line">temp = path[temp.x][temp.y];</span><br><span class="line">maze[temp.x][temp.y] = <span class="number">3</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span>&#123;</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="comment">//迷宫</span></span><br><span class="line"><span class="keyword">int</span> i, j;</span><br><span class="line">FILE *fp;</span><br><span class="line"><span class="keyword">int</span> maze[COUNT_I][COUNT_J] ;</span><br><span class="line">    fp=fopen(<span class="string">&quot;keshedemo.txt&quot;</span>,<span class="string">&quot;r&quot;</span>);</span><br><span class="line">    <span class="keyword">if</span>(!fp)</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;文件错误&quot;</span>);</span><br><span class="line">    <span class="keyword">for</span>(i=<span class="number">0</span>;i&lt;=<span class="number">16</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span>(j=<span class="number">0</span>;j&lt;=<span class="number">16</span>;j++)</span><br><span class="line">        &#123;</span><br><span class="line">            printf(&#x27;da&#x27;);</span><br><span class="line">            <span class="built_in">fscanf</span>(fp,<span class="string">&quot;%3d&quot;</span>,&amp;maze[i][j]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    fclose(fp);</span><br><span class="line">    fp=<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/*        for (i=0;i&lt;=16;i++)</span></span><br><span class="line"><span class="comment">        &#123;</span></span><br><span class="line"><span class="comment">            for(j=0;j&lt;=16;j++)</span></span><br><span class="line"><span class="comment">                fprintf(fp,&quot;%3d&quot;,maze[i][j]);</span></span><br><span class="line"><span class="comment">            fprintf(fp,&quot;\n&quot;);</span></span><br><span class="line"><span class="comment">        &#125;</span></span><br><span class="line"><span class="comment">        fclose(fp);</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">//定义路径数组,将到(x,y)点的路径保存进数组</span></span><br><span class="line">LOCAL path[COUNT_I][COUNT_J];</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; COUNT_I; i++)&#123;</span><br><span class="line"><span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; COUNT_J; j++)&#123;</span><br><span class="line">path[i][j].x = <span class="number">-1</span>;</span><br><span class="line">path[i][j].y = <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印出迷宫</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;原迷宫：\n&quot;</span>);</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= COUNT_I; i++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;-&quot;</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; COUNT_I; i++)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;|&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; COUNT_J; j++)&#123;</span><br><span class="line"><span class="keyword">if</span> (maze[i][j] == <span class="number">1</span>)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;*&quot;</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;|\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= COUNT_I; i++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;-&quot;</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (VistMaze(maze, path) == <span class="number">0</span>)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;没有路径可走\n&quot;</span>);</span><br><span class="line"><span class="built_in">exit</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">//打印出迷宫和路径</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;迷宫和路径：\n&quot;</span>);</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= COUNT_I; i++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;-&quot;</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; COUNT_I; i++)&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;|&quot;</span>);</span><br><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; COUNT_J; j++)&#123;</span><br><span class="line"><span class="keyword">if</span> (maze[i][j] == <span class="number">1</span>)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;*&quot;</span>);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (maze[i][j] == <span class="number">3</span>)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;0&quot;</span>);</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot; &quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;|\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt;= COUNT_I; i++)</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;-&quot;</span>);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">&quot;\n&quot;</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;迷宫问题求解&quot;&gt;&lt;a href=&quot;#迷宫问题求解&quot; class=&quot;headerlink&quot; title=&quot;迷宫问题求解&quot;&gt;&lt;/a&gt;迷宫问题求解&lt;/h2&gt;&lt;p&gt;问题描述：&lt;br&gt;    给定一个迷宫从入口到出口的路径，具体要求如下：&lt;br&gt;       1.迷宫以16</summary>
      
    
    
    
    <category term="数据结构" scheme="http://example.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>爬取携程中评论的数据</title>
    <link href="http://example.com/2021/05/03/%E7%88%AC%E5%8F%96%E6%90%BA%E7%A8%8B%E4%B8%AD%E8%AF%84%E8%AE%BA%E7%9A%84%E6%95%B0%E6%8D%AE/"/>
    <id>http://example.com/2021/05/03/%E7%88%AC%E5%8F%96%E6%90%BA%E7%A8%8B%E4%B8%AD%E8%AF%84%E8%AE%BA%E7%9A%84%E6%95%B0%E6%8D%AE/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:21:16.260Z</updated>
    
    <content type="html"><![CDATA[<h2 id="爬取携程中评论的数据"><a href="#爬取携程中评论的数据" class="headerlink" title="爬取携程中评论的数据"></a>爬取携程中评论的数据</h2><p>1、爬取评论的发布者<br>2、爬取评论发布的时间<br>3、爬取评论的内容</p><p>在爬取这个携程数据时，将使用selenium自动化的去获取网页数据将网页数据下载下来，使用的是chrom驱动程序，打开网页，如果不会配置，请在评论区提出，我会补录此段：望本文对您有所帮助：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 声明浏览器</span></span><br><span class="line">browser = webdriver.Chrome ()</span><br><span class="line">browser.get (<span class="string">&quot;URL(请自行补充携程网页地址)&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_page</span>():</span></span><br><span class="line">    sel = Selector (text=browser.page_source)</span><br><span class="line">    time.sleep (<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    authors = sel.xpath (<span class="string">&#x27;//div[@class=&quot;user-date&quot;]/span/text()&#x27;</span>).extract ()</span><br><span class="line">    <span class="comment"># write_times=sel.xpath(&#x27;//div[@class=&quot;user-date&quot;]/span/text()&#x27;).extract()[i]</span></span><br><span class="line">    comments = sel.xpath (<span class="string">&#x27; //ul[@class=&quot;comments&quot;]/li/p/text()&#x27;</span>).extract ()</span><br><span class="line">    <span class="comment"># print (authors)</span></span><br><span class="line">    <span class="comment"># # print(write_times)</span></span><br><span class="line">    <span class="comment"># print (comments)</span></span><br><span class="line">    author = authors[::<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># print (author)</span></span><br><span class="line">    time_comments = authors[<span class="number">2</span>::<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> author, time_comment, comment <span class="keyword">in</span> <span class="built_in">zip</span> (author, time_comments, comments):</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span> (<span class="string">&#x27;评论.txt&#x27;</span>, <span class="string">&#x27;a+&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            f.write (</span><br><span class="line">                <span class="string">&quot;评论人：&quot;</span> + author + <span class="string">&#x27;\t&#x27;</span> + <span class="string">&quot;评论时间&quot;</span> + time_comment + <span class="string">&#x27;\t&#x27;</span> + <span class="string">&quot;评论内容：&quot;</span> + comment.strip (</span><br><span class="line">                    <span class="string">&#x27;\n&#x27;</span>) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    bonwon = browser.find_element_by_xpath (<span class="string">&#x27;//ul[@class=&quot;pkg_page&quot;]/a[last()]&#x27;</span>)</span><br><span class="line">    bonwon.click ()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="number">0</span>, <span class="number">15</span>):</span><br><span class="line">        parse_page ()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    parse_page ()</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;爬取携程中评论的数据&quot;&gt;&lt;a href=&quot;#爬取携程中评论的数据&quot; class=&quot;headerlink&quot; title=&quot;爬取携程中评论的数据&quot;&gt;&lt;/a&gt;爬取携程中评论的数据&lt;/h2&gt;&lt;p&gt;1、爬取评论的发布者&lt;br&gt;2、爬取评论发布的时间&lt;br&gt;3、爬取评论的内容</summary>
      
    
    
    
    <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>零基础入门金融风控Task5 模型融合</title>
    <link href="http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/"/>
    <id>http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7Task5%20%E6%A8%A1%E5%9E%8B%E8%9E%8D%E5%90%88/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T11:54:36.973Z</updated>
    
    <content type="html"><![CDATA[<h2 id="零基础入门金融风控Task5-模型融合"><a href="#零基础入门金融风控Task5-模型融合" class="headerlink" title="零基础入门金融风控Task5 模型融合"></a>零基础入门金融风控Task5 模型融合</h2><p>这一节是在上一节基础上进行的，请合起来一起看。<br>@<a href="">TOC</a></p><h3 id="1-学习目标"><a href="#1-学习目标" class="headerlink" title="1 学习目标"></a>1 学习目标</h3><p>将之前建模调参的结果进行模型融合。 尝试多种融合方案。</p><h3 id="2-内容介绍"><a href="#2-内容介绍" class="headerlink" title="2 内容介绍"></a>2 内容介绍</h3><p>模型融合是比赛后期上分的重要手段，在多人组队的比赛中，将不同的模型进行融合，可能会收获意想不到的效果，往往模型相差越大且模型表现都不错的前提下，模型融合后结果会有大幅提升，模型融合的方式有。</p><ul><li>平均法：</li></ul><p>———简单平均法<br>———加权平均法</p><ul><li>投票法：</li></ul><p>———简单投票法<br>———加权投票法</p><ul><li>综合法：</li></ul><p>———排序融合<br>———log融合</p><ul><li>stacking:</li></ul><p>———构建多层模型，并利用预测结果再拟合预测。</p><ul><li>blending：</li></ul><p>———选取部分数据预测训练得到预测结果作为新特征，带入剩下的数据中</p><ul><li>预测。</li></ul><p>———boosting/bagging </p><h3 id="3-stacking和blending详解"><a href="#3-stacking和blending详解" class="headerlink" title="3 stacking和blending详解"></a>3 stacking和blending详解</h3><p>stacking 将若干基学习器获得的预测结果，将预测结果作为新的训练集来训练一个学习器。假设有五个基学习器，将数据带入五基学习器中得到预测结果，再带入模型六中进行训练预测。但是由于直接由五个基学习器获得结果直接带入模型六中，容易导致过拟合。所以在使用五个及模型进行预测的时候，可以考虑使用K折验证，防止过拟合。</p><ul><li>Blending与stacking的不同<br>stacking中由于两层使用的数据不同，所以可以避免信息泄露的问题。在组队竞赛的过程中，不需要给队友分享自己的随机种子。</li><li>Blending<br>由于blending对将数据划分为两个部分，在最后预测时有部分数据信息将被忽略。同时在使用第二层数据时可能会因为第二层数据较少产生过拟合现象。<h3 id="4-代码"><a href="#4-代码" class="headerlink" title="4 代码"></a>4 代码</h3><h4 id="4-1-平均法："><a href="#4-1-平均法：" class="headerlink" title="4.1 平均法："></a>4.1 平均法：</h4></li><li>简单加权平均<br>结果直接求和，然后除长，得到预测结果的平均值。</li><li>加权平均法<br>一般根据之前预测模型的准确率，进行加权融合，将准确性高的模型赋予更高的权重。给每一个值加一个加权值，进行平均化。<h4 id="4-2-投票法"><a href="#4-2-投票法" class="headerlink" title="4.2 投票法"></a>4.2 投票法</h4></li><li>简单投票<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from xgboost import XGBClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier, VotingClassifier</span><br><span class="line">clf1 = LogisticRegression(random_state=1)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=1)</span><br><span class="line">clf3 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=2, subsample=0.7,objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">vclf = VotingClassifier(estimators=[(<span class="string">&#x27;lr&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;xgb&#x27;</span>, clf3)])</span><br><span class="line">vclf = vclf .fit(x_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(vclf .predict(x_test))</span><br></pre></td></tr></table></figure></li></ul><ul><li>加权投票<br>在VotingClassifier中加入参数 voting=’soft’, weights=[2, 1, 1]，weights用于调节基模型的权重</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">from xgboost import XGBClassifier</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.ensemble import RandomForestClassifier, VotingClassifier</span><br><span class="line">clf1 = LogisticRegression(random_state=1)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=1)</span><br><span class="line">clf3 = XGBClassifier(learning_rate=0.1, n_estimators=150, max_depth=4, min_child_weight=2, subsample=0.7,objective=<span class="string">&#x27;binary:logistic&#x27;</span>)</span><br><span class="line"> </span><br><span class="line">vclf = VotingClassifier(estimators=[(<span class="string">&#x27;lr&#x27;</span>, clf1), (<span class="string">&#x27;rf&#x27;</span>, clf2), (<span class="string">&#x27;xgb&#x27;</span>, clf3)], voting=<span class="string">&#x27;soft&#x27;</span>, weights=[2, 1, 1])</span><br><span class="line">vclf = vclf .fit(x_train,y_train)</span><br><span class="line"><span class="built_in">print</span>(vclf .predict(x_test))</span><br></pre></td></tr></table></figure><h4 id="4-3-Stacking："><a href="#4-3-Stacking：" class="headerlink" title="4.3 Stacking："></a>4.3 Stacking：</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">%matplotlib inline</span><br><span class="line">import itertools</span><br><span class="line">import numpy as np</span><br><span class="line">import seaborn as sns</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import matplotlib.gridspec as gridspec</span><br><span class="line">from sklearn import datasets</span><br><span class="line">from sklearn.linear_model import LogisticRegression</span><br><span class="line">from sklearn.neighbors import KNeighborsClassifier</span><br><span class="line">from sklearn.naive_bayes import GaussianNB </span><br><span class="line">from sklearn.ensemble import RandomForestClassifier</span><br><span class="line">from mlxtend.classifier import StackingClassifier</span><br><span class="line">from sklearn.model_selection import cross_val_score, train_test_split</span><br><span class="line">from mlxtend.plotting import plot_learning_curves</span><br><span class="line">from mlxtend.plotting import plot_decision_regions</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 以python自带的鸢尾花数据集为例</span></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X, y = iris.data[:, 1:3], iris.target</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf1 = KNeighborsClassifier(n_neighbors=1)</span><br><span class="line">clf2 = RandomForestClassifier(random_state=1)</span><br><span class="line">clf3 = GaussianNB()</span><br><span class="line">lr = LogisticRegression()</span><br><span class="line">sclf = StackingClassifier(classifiers=[clf1, clf2, clf3], </span><br><span class="line">                          meta_classifier=lr)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">label = [<span class="string">&#x27;KNN&#x27;</span>, <span class="string">&#x27;Random Forest&#x27;</span>, <span class="string">&#x27;Naive Bayes&#x27;</span>, <span class="string">&#x27;Stacking Classifier&#x27;</span>]</span><br><span class="line">clf_list = [clf1, clf2, clf3, sclf]</span><br><span class="line">    </span><br><span class="line">fig = plt.figure(figsize=(10,8))</span><br><span class="line">gs = gridspec.GridSpec(2, 2)</span><br><span class="line">grid = itertools.product([0,1],repeat=2)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">clf_cv_mean = []</span><br><span class="line">clf_cv_std = []</span><br><span class="line"><span class="keyword">for</span> clf, label, grd <span class="keyword">in</span> zip(clf_list, label, grid):</span><br><span class="line">        </span><br><span class="line">    scores = cross_val_score(clf, X, y, cv=5, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Accuracy: %.2f (+/- %.2f) [%s]&quot;</span> %(scores.mean(), scores.std(), label))</span><br><span class="line">    clf_cv_mean.append(scores.mean())</span><br><span class="line">    clf_cv_std.append(scores.std())</span><br><span class="line">        </span><br><span class="line">    clf.fit(X, y)</span><br><span class="line">    ax = plt.subplot(gs[grd[0], grd[1]])</span><br><span class="line">    fig = plot_decision_regions(X=X, y=y, clf=clf)</span><br><span class="line">    plt.title(label)</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><blockquote><p>Accuracy: 0.91 (+/- 0.07) [KNN]<br>Accuracy: 0.93 (+/- 0.05) [Random Forest]<br>Accuracy: 0.91 (+/- 0.04) [Naive Bayes]<br>Accuracy: 0.93 (+/- 0.04) [Stacking Classifier]</p></blockquote><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200927223950751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h4 id="4-2-blending"><a href="#4-2-blending" class="headerlink" title="4.2 blending"></a>4.2 blending</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以python自带的鸢尾花数据集为例</span></span><br><span class="line">data_0 = iris.data</span><br><span class="line">data = data_0[:100,:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">target_0 = iris.target</span><br><span class="line">target = target_0[:100]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#模型融合中基学习器</span></span><br><span class="line">clfs = [LogisticRegression(),</span><br><span class="line">        RandomForestClassifier(),</span><br><span class="line">        ExtraTreesClassifier(),</span><br><span class="line">        GradientBoostingClassifier()]</span><br><span class="line"> </span><br><span class="line"><span class="comment">#切分一部分数据作为测试集</span></span><br><span class="line">X, X_predict, y, y_predict = train_test_split(data, target, test_size=0.3, random_state=914)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#切分训练数据集为d1,d2两部分</span></span><br><span class="line">X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=914)</span><br><span class="line">dataset_d1 = np.zeros((X_d2.shape[0], len(clfs)))</span><br><span class="line">dataset_d2 = np.zeros((X_predict.shape[0], len(clfs)))</span><br><span class="line"> </span><br><span class="line"><span class="keyword">for</span> j, clf <span class="keyword">in</span> enumerate(clfs):</span><br><span class="line">    <span class="comment">#依次训练各个单模型</span></span><br><span class="line">    clf.fit(X_d1, y_d1)</span><br><span class="line">    y_submission = clf.predict_proba(X_d2)[:, 1]</span><br><span class="line">    dataset_d1[:, j] = y_submission</span><br><span class="line">    <span class="comment">#对于测试集，直接用这k个模型的预测值作为新的特征。</span></span><br><span class="line">    dataset_d2[:, j] = clf.predict_proba(X_predict)[:, 1]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;val auc Score: %f&quot;</span> % roc_auc_score(y_predict, dataset_d2[:, j]))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#融合使用的模型</span></span><br><span class="line">clf = GradientBoostingClassifier()</span><br><span class="line">clf.fit(dataset_d1, y_d2)</span><br><span class="line">y_submission = clf.predict_proba(dataset_d2)[:, 1]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Val auc Score of Blending: %f&quot;</span> % (roc_auc_score(y_predict, y_submission)))</span><br></pre></td></tr></table></figure><h3 id="5-总结学习"><a href="#5-总结学习" class="headerlink" title="5 总结学习"></a>5 总结学习</h3><ul><li>简单平均和加权平均是常用的两种比赛中模型融合的方式。其优点是快速、简单。</li><li>stacking在众多比赛中大杀四方，但是跑过代码的小伙伴想必能感受到速度之慢，同时stacking多层提升幅度并不能抵消其带来的时间和内存消耗，所以实际环境中应用还是有一定的难度，同时在有答辩环节的比赛中，主办方也会一定程度上考虑模型的复杂程度，所以说并不是模型融合的层数越多越好的。</li><li>当然在比赛中将加权平均、stacking、blending等混用也是一种策略，可能会收获意想不到的效果哦！</li><li>最后模型的融合在本次学习中，介绍了几种融合方法，使用融合可以很好的提高准确率这也是我们提高成绩的一大法宝。另外文章中有不足之处，请务必指出，一定迅速改正。谢谢</li><li>五次打卡已经结束，最后还有一篇基线学习，感谢支持。</li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;零基础入门金融风控Task5-模型融合&quot;&gt;&lt;a href=&quot;#零基础入门金融风控Task5-模型融合&quot; class=&quot;headerlink&quot; title=&quot;零基础入门金融风控Task5 模型融合&quot;&gt;&lt;/a&gt;零基础入门金融风控Task5 模型融合&lt;/h2&gt;&lt;p&gt;这一</summary>
      
    
    
    
    <category term="学习赛" scheme="http://example.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%9B/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>零基础入门金融风控之贷款违约预测Task1：赛题理解</title>
    <link href="http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E7%9A%84Task1%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/"/>
    <id>http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E7%9A%84Task1%EF%BC%9A%E8%B5%9B%E9%A2%98%E7%90%86%E8%A7%A3/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:05:37.283Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零基础入门金融风控之贷款违约预测"><a href="#零基础入门金融风控之贷款违约预测" class="headerlink" title="零基础入门金融风控之贷款违约预测"></a>零基础入门金融风控之贷款违约预测</h1><h2 id="Task1：赛题理解"><a href="#Task1：赛题理解" class="headerlink" title="Task1：赛题理解"></a>Task1：赛题理解</h2><h3 id="1-赛题概况"><a href="#1-赛题概况" class="headerlink" title="1.赛题概况"></a>1.赛题概况</h3><p>赛题以预测金融风险为任务，数据集报名后可见并可下载，该数据来自某信贷平台的贷款记录，总数据量超过120w，包含47列变量信息，其中15列为匿名变量。为了保证比赛的公平性，将会从中抽取80万条作为训练集，20万条作为测试集A，20万条作为测试集B，同时会对employmentTitle、purpose、postCode和title等信息进行脱敏。</p><h3 id="2-数据概况"><a href="#2-数据概况" class="headerlink" title="2.数据概况"></a>2.数据概况</h3><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200915192252870.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200915192453652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200915192453561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h2 id="3-评价指标"><a href="#3-评价指标" class="headerlink" title="3.评价指标"></a>3.评价指标</h2><p>竞赛采用的是AUC作为评价指标，至于其他的评价指标，这里列举一下：<br>1.混淆矩阵<br>2.准确率<br>3.精确率<br>4.召回率<br>5.F1-SCore<br>6.P-R曲线<br>7.ROC曲线<br>8.AUC曲线<br>对于金融风控预测常见的评价指标是：KS</p><h2 id="4、代码示例"><a href="#4、代码示例" class="headerlink" title="4、代码示例"></a>4、代码示例</h2><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200915185033119.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200915185050427.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;a href=&quot;#零基础入门金融风控之贷款违约预测&quot; class=&quot;headerlink&quot; title=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;/a&gt;零基础入门金融风控之贷款违约预测&lt;/h1&gt;&lt;h2 id=&quot;Task1：赛题理</summary>
      
    
    
    
    <category term="学习赛" scheme="http://example.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%9B/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>零基础入门金融风控之贷款违约预测Task2：数据分析</title>
    <link href="http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E7%9A%84Task2%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/"/>
    <id>http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8B%E7%9A%84Task2%EF%BC%9A%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:20:28.986Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零基础入门金融风控之贷款违约预测"><a href="#零基础入门金融风控之贷款违约预测" class="headerlink" title="零基础入门金融风控之贷款违约预测"></a>零基础入门金融风控之贷款违约预测</h1><h1 id="Task2：数据分析"><a href="#Task2：数据分析" class="headerlink" title="Task2：数据分析"></a>Task2：数据分析</h1><p>@<a href="%E8%BF%99%E9%87%8C%E5%86%99%E7%9B%AE%E5%BD%95%E6%A0%87%E9%A2%98">TOC</a></p><h3 id="2-1-学习目标"><a href="#2-1-学习目标" class="headerlink" title="2.1 学习目标"></a>2.1 学习目标</h3><p>1、学习如何对数据集整体概况进行分析，包括数据集的基本情况（缺失值，异常值）<br>2、学习了解变量间的相互关系、变量与预测值之间的存在关系<br>完成相应学习打卡任务</p><h3 id="2-2-分析内容"><a href="#2-2-分析内容" class="headerlink" title="2.2 分析内容"></a>2.2 分析内容</h3><ul><li>1、数据总体的了解<br>一般查看数据的纬度，数据类型，基本了解一下数据各个统计量说的啥，对此有一个大概的了解</li><li>2、查看数据缺失和唯一值</li><li>3、深入了解数据的类型。每一个特征是连续的还是离散的。对此要有了解</li><li>4、分析各个特征之间的关系，和目标变量的关系。</li></ul><h3 id="2-3-分析过程"><a href="#2-3-分析过程" class="headerlink" title="2.3 分析过程"></a>2.3 分析过程</h3><p>导入相关的库</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import seaborn as sns</span><br><span class="line">import datetime</span><br><span class="line">import warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)<span class="comment">#对于一些警告的，可以忽略</span></span><br></pre></td></tr></table></figure><p>读取文件，并简单了解</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_train = pd.read_csv(<span class="string">&#x27;./train.csv&#x27;</span>)</span><br><span class="line">data_test_a = pd.read_csv(<span class="string">&#x27;./testA.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_test_a.shape</span><br><span class="line">data_train.shape</span><br></pre></td></tr></table></figure><p>这里的特征列太长，可以使用T进行转置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.describe().T</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918212627312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>实时对比数据内容</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.head().append(data_train.tail())</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918213047891.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.isnull().any().sum()</span><br></pre></td></tr></table></figure><p>上面得到训练集有22列特征有缺失值，进一步查看缺失特征中缺失率大于50%的特征</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">have_null_fea_dict = (data_train.<span class="built_in">isnull</span>().<span class="built_in">sum</span>()/<span class="built_in">len</span>(data_train)).<span class="built_in">to_dict</span>()</span><br><span class="line">fea_null_moreThanHalf = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> key,value in have_null_fea_dict.<span class="built_in">items</span>():</span><br><span class="line">    <span class="keyword">if</span> value &gt; <span class="number">0.5</span>:</span><br><span class="line">        fea_null_moreThanHalf[key] = value</span><br></pre></td></tr></table></figure><p>具体的查看缺失特征及缺失率</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nan可视化</span></span><br><span class="line">missing = data_train.isnull().sum()/len(data_train)</span><br><span class="line">missing = missing[missing &gt; 0]</span><br><span class="line">missing.sort_values(inplace=True)</span><br><span class="line">missing.plot.bar()</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918213536994.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">one_value_fea = [col <span class="keyword">for</span> col <span class="keyword">in</span> data_train.columns <span class="keyword">if</span> data_train[col].nunique() &lt;= 1]</span><br><span class="line"></span><br><span class="line">numerical_fea = list(data_train.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>]).columns)</span><br><span class="line">category_fea = list(filter(lambda x: x not <span class="keyword">in</span> numerical_fea,list(data_train.columns)))</span><br></pre></td></tr></table></figure><p>数值型变量分析，数值型肯定是包括连续型变量和离散型变量的，找出来 划分数值型变量中的连续变量和离散型变量</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#过滤数值型类别特征</span><br><span class="line"><span class="function">def <span class="title">get_numerical_serial_fea</span><span class="params">(data,feas)</span>:</span></span><br><span class="line"><span class="function">    numerical_serial_fea =</span> []</span><br><span class="line">    numerical_noserial_fea = []</span><br><span class="line">    <span class="keyword">for</span> fea in feas:</span><br><span class="line">        temp = data[fea].<span class="built_in">nunique</span>()</span><br><span class="line">        <span class="keyword">if</span> temp &lt;= <span class="number">10</span>:</span><br><span class="line">            numerical_noserial_fea.<span class="built_in">append</span>(fea)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        numerical_serial_fea.<span class="built_in">append</span>(fea)</span><br><span class="line">    <span class="keyword">return</span> numerical_serial_fea,numerical_noserial_fea</span><br><span class="line"></span><br><span class="line">numerical_serial_fea,numerical_noserial_fea = <span class="built_in">get_numerical_serial_fea</span>(data_train,numerical_fea)</span><br></pre></td></tr></table></figure><p>了解离散型变量，这里建议去查看一下这两组数据</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train[ &#x27;policyCode&#x27;].value_counts()#离散型变量</span><br></pre></td></tr></table></figure><p> <img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918214056618.png#pic_center" alt="在这里插入图片描述"><br> 数值连续型变量分析可视化。<br> 这里melt方法比较重要，数据透视表和使用方格可视化。由于本文篇幅可能过于长，不在详述。想要了解可以翻找我的博客</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#每个数字特征得分布可视化</span></span><br><span class="line">f = pd.melt(data_train, value_vars=numerical_serial_fea)</span><br><span class="line">g = sns.FacetGrid(f, col=<span class="string">&quot;variable&quot;</span>,  col_wrap=2, sharex=False, sharey=False)</span><br><span class="line">g = g.map(sns.distplot, <span class="string">&quot;value&quot;</span>)</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918214853429.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p> 这里可以看出一些连续型的变量是近似符合正态分布的。了解正态分布是对于数据处理非常重要的。基本上所有的变量都是符合正态分布才比较合理。一些不符合正态分布的可以使用Log在观察。</p><ul><li>正态Transaction并可视化</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Ploting Transaction Amount Values Distribution</span></span><br><span class="line">plt.figure(figsize=(16,12))</span><br><span class="line">plt.suptitle(<span class="string">&#x27;Transaction Values Distribution&#x27;</span>, fontsize=22)</span><br><span class="line">plt.subplot(221)</span><br><span class="line">sub_plot_1 = sns.distplot(data_train[<span class="string">&#x27;loanAmnt&#x27;</span>])</span><br><span class="line">sub_plot_1.set_title(<span class="string">&quot;loanAmnt Distribuition&quot;</span>, fontsize=18)</span><br><span class="line">sub_plot_1.set_xlabel(<span class="string">&quot;&quot;</span>)</span><br><span class="line">sub_plot_1.set_ylabel(<span class="string">&quot;Probability&quot;</span>, fontsize=15)</span><br><span class="line"></span><br><span class="line">plt.subplot(222)</span><br><span class="line">sub_plot_2 = sns.distplot(np.log(data_train[<span class="string">&#x27;loanAmnt&#x27;</span>]))</span><br><span class="line">sub_plot_2.set_title(<span class="string">&quot;loanAmnt (Log) Distribuition&quot;</span>, fontsize=18)</span><br><span class="line">sub_plot_2.set_xlabel(<span class="string">&quot;&quot;</span>)</span><br><span class="line">sub_plot_2.set_ylabel(<span class="string">&quot;Probability&quot;</span>, fontsize=15)</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/2020091821550879.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>变量分布可视化：<br>单一变量可视化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(8, 8))</span><br><span class="line">sns.barplot(data_train[<span class="string">&quot;employmentLength&quot;</span>].value_counts(dropna=False)[:20],</span><br><span class="line">            data_train[<span class="string">&quot;employmentLength&quot;</span>].value_counts(dropna=False).keys()[:20])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><ul><li>根据Y值不同可视化特征分布<br>查看类别型变量在不同y值上的分布：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rain_loan_fr = data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 1]</span><br><span class="line">train_loan_nofr = data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 0]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 8))</span><br><span class="line">train_loan_fr.groupby(<span class="string">&#x27;grade&#x27;</span>)[<span class="string">&#x27;grade&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax1, title=<span class="string">&#x27;Count of grade fraud&#x27;</span>)</span><br><span class="line">train_loan_nofr.groupby(<span class="string">&#x27;grade&#x27;</span>)[<span class="string">&#x27;grade&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax2, title=<span class="string">&#x27;Count of grade non-fraud&#x27;</span>)</span><br><span class="line">train_loan_fr.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;employmentLength&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax3, title=<span class="string">&#x27;Count of employmentLength fraud&#x27;</span>)</span><br><span class="line">train_loan_nofr.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;employm</span></span><br><span class="line"><span class="string">entLength&#x27;</span>].count().plot(kind=<span class="string">&#x27;barh&#x27;</span>, ax=ax4, title=<span class="string">&#x27;Count of employmentLength non-fraud&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>查看连续型变量在不同的y值上的分布</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 6))</span><br><span class="line">data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 1] \</span><br><span class="line">    [<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log) \</span><br><span class="line">    .plot(kind=<span class="string">&#x27;hist&#x27;</span>,</span><br><span class="line">          bins=100,</span><br><span class="line">          title=<span class="string">&#x27;Log Loan Amt - Fraud&#x27;</span>,</span><br><span class="line">          color=<span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">          xlim=(-3, 10),</span><br><span class="line">         ax= ax1)</span><br><span class="line">data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 0] \</span><br><span class="line">    [<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log) \</span><br><span class="line">    .plot(kind=<span class="string">&#x27;hist&#x27;</span>,</span><br><span class="line">          bins=100,</span><br><span class="line">          title=<span class="string">&#x27;Log Loan Amt - Not Fraud&#x27;</span>,</span><br><span class="line">          color=<span class="string">&#x27;b&#x27;</span>,</span><br><span class="line">          xlim=(-3, 10),</span><br><span class="line">         ax=ax2</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918220059490.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">fig, ((ax1, ax2)) = plt.subplots(1, 2, figsize=(15, 6))</span><br><span class="line">data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 1] \</span><br><span class="line">    [<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log) \</span><br><span class="line">    .plot(kind=<span class="string">&#x27;hist&#x27;</span>,</span><br><span class="line">          bins=100,</span><br><span class="line">          title=<span class="string">&#x27;Log Loan Amt - Fraud&#x27;</span>,</span><br><span class="line">          color=<span class="string">&#x27;r&#x27;</span>,</span><br><span class="line">          xlim=(-3, 10),</span><br><span class="line">         ax= ax1)</span><br><span class="line">data_train.loc[data_train[<span class="string">&#x27;isDefault&#x27;</span>] == 0] \</span><br><span class="line">    [<span class="string">&#x27;loanAmnt&#x27;</span>].apply(np.log) \</span><br><span class="line">    .plot(kind=<span class="string">&#x27;hist&#x27;</span>,</span><br><span class="line">          bins=100,</span><br><span class="line">          title=<span class="string">&#x27;Log Loan Amt - Not Fraud&#x27;</span>,</span><br><span class="line">          color=<span class="string">&#x27;b&#x27;</span>,</span><br><span class="line">          xlim=(-3, 10),</span><br><span class="line">         ax=ax2)</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918220235752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">total = len(data_train)</span><br><span class="line">total_amt = data_train.groupby([<span class="string">&#x27;isDefault&#x27;</span>])[<span class="string">&#x27;loanAmnt&#x27;</span>].sum().sum()</span><br><span class="line">plt.figure(figsize=(12,5))</span><br><span class="line">plt.subplot(121)<span class="comment">##1代表行，2代表列，所以一共有2个图，1代表此时绘制第一个图。</span></span><br><span class="line">plot_tr = sns.countplot(x=<span class="string">&#x27;isDefault&#x27;</span>,data=data_train)<span class="comment">#data_train‘isDefault’这个特征每种类别的数量**</span></span><br><span class="line">plot_tr.set_title(<span class="string">&quot;Fraud Loan Distribution \n 0: good user | 1: bad user&quot;</span>, fontsize=14)</span><br><span class="line">plot_tr.set_xlabel(<span class="string">&quot;Is fraud by count&quot;</span>, fontsize=16)</span><br><span class="line">plot_tr.set_ylabel(<span class="string">&#x27;Count&#x27;</span>, fontsize=16)</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> plot_tr.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    plot_tr.text(p.get_x()+p.get_width()/2.,</span><br><span class="line">            height + 3,</span><br><span class="line">            <span class="string">&#x27;&#123;:1.2f&#125;%&#x27;</span>.format(height/total*100),</span><br><span class="line">            ha=<span class="string">&quot;center&quot;</span>, fontsize=15) </span><br><span class="line">    </span><br><span class="line">percent_amt = (data_train.groupby([<span class="string">&#x27;isDefault&#x27;</span>])[<span class="string">&#x27;loanAmnt&#x27;</span>].sum())</span><br><span class="line">percent_amt = percent_amt.reset_index()</span><br><span class="line">plt.subplot(122)</span><br><span class="line">plot_tr_2 = sns.barplot(x=<span class="string">&#x27;isDefault&#x27;</span>, y=<span class="string">&#x27;loanAmnt&#x27;</span>,  dodge=True, data=percent_amt)</span><br><span class="line">plot_tr_2.set_title(<span class="string">&quot;Total Amount in loanAmnt  \n 0: good user | 1: bad user&quot;</span>, fontsize=14)</span><br><span class="line">plot_tr_2.set_xlabel(<span class="string">&quot;Is fraud by percent&quot;</span>, fontsize=16)</span><br><span class="line">plot_tr_2.set_ylabel(<span class="string">&#x27;Total Loan Amount Scalar&#x27;</span>, fontsize=16)</span><br><span class="line"><span class="keyword">for</span> p <span class="keyword">in</span> plot_tr_2.patches:</span><br><span class="line">    height = p.get_height()</span><br><span class="line">    plot_tr_2.text(p.get_x()+p.get_width()/2.,</span><br><span class="line">            height + 3,</span><br><span class="line">            <span class="string">&#x27;&#123;:1.2f&#125;%&#x27;</span>.format(height/total_amt * 100),</span><br><span class="line">            ha=<span class="string">&quot;center&quot;</span>, fontsize=15)   </span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918220419637.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>时间格式数据处理及查看</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转化成时间格式  issueDateDT特征表示数据日期离数据集中日期最早的日期（2007-06-01）的天数</span></span><br><span class="line">data_train[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data_train[<span class="string">&#x27;issueDate&#x27;</span>],format=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data_train[<span class="string">&#x27;issueDateDT&#x27;</span>] = data_train[<span class="string">&#x27;issueDate&#x27;</span>].apply(lambda x: x-startdate).dt.days</span><br><span class="line"></span><br><span class="line"><span class="comment">#转化成时间格式</span></span><br><span class="line">data_test_a[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data_train[<span class="string">&#x27;issueDate&#x27;</span>],format=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data_test_a[<span class="string">&#x27;issueDateDT&#x27;</span>] = data_test_a[<span class="string">&#x27;issueDate&#x27;</span>].apply(lambda x: x-startdate).dt.days</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.hist(data_train[<span class="string">&#x27;issueDateDT&#x27;</span>], label=<span class="string">&#x27;train&#x27;</span>);</span><br><span class="line">plt.hist(data_test_a[<span class="string">&#x27;issueDateDT&#x27;</span>], label=<span class="string">&#x27;test&#x27;</span>);</span><br><span class="line">plt.legend();</span><br><span class="line">plt.title(<span class="string">&#x27;Distribution of issueDateDT dates&#x27;</span>);</span><br><span class="line"><span class="comment">#train 和 test issueDateDT 日期有重叠 所以使用基于时间的分割进行验证是不明智的</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918221335445.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#透视图 索引可以有多个，“columns（列）”是可选的，聚合函数aggfunc最后是被应用到了变量“values”中你所列举的项目上。</span></span><br><span class="line">pivot = pd.pivot_table(data_train, index=[<span class="string">&#x27;grade&#x27;</span>], columns=[<span class="string">&#x27;issueDateDT&#x27;</span>], values=[<span class="string">&#x27;loanAmnt&#x27;</span>], aggfunc=np.sum)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pivot</span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200918220653999.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>最后可以使用这行命令生成数据报告，这个非常棒的，我也是第一次使用，但生成的比较慢</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import pandas_profiling</span><br><span class="line">pfr = pandas_profiling.ProfileReport(data_train)</span><br><span class="line">pfr.to_file(<span class="string">&quot;./example.html&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h3><p>如果没有pandas_profiling这个模块可以是使用</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pandas_profiling</span><br></pre></td></tr></table></figure><p>安装一下即可</p><p>本次任务二学习到不少知识，文章中如有不足之处，感谢指出，一定迅速改正。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;a href=&quot;#零基础入门金融风控之贷款违约预测&quot; class=&quot;headerlink&quot; title=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;/a&gt;零基础入门金融风控之贷款违约预测&lt;/h1&gt;&lt;h1 id=&quot;Task2：数据分</summary>
      
    
    
    
    <category term="学习赛" scheme="http://example.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%9B/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>1. 协同过滤算法介绍</title>
    <link href="http://example.com/2021/05/03/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/"/>
    <id>http://example.com/2021/05/03/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:34:31.035Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC]</p><h2 id="1-协同过滤算法介绍"><a href="#1-协同过滤算法介绍" class="headerlink" title="1. 协同过滤算法介绍"></a>1. 协同过滤算法介绍</h2><p>协同过滤（<strong>Collaborative Filtering）</strong>* 推荐算法是最经典、最常用的推荐算法。简称CF|</p><p>所谓协同过滤， 基本思想是根据<strong>用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品</strong> (基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐)，一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。目前应用比较广泛的协同过滤算法是<strong>基于邻域的方法</strong>去分类，有下面两种算法：</p><ul><li>基于用户的协同过滤算法(UserCF): 给用户推荐和他兴趣相似的其他用户喜欢的产品</li><li>基于物品的协同过滤算法(ItemCF): 给用户推荐和他之前喜欢的物品相似的物品<br>不管是UserCF还是ItemCF算法， 非常重要的步骤之一就是计算用户和用户或者物品和物品之间的相似度， 所以下面先整理常用的相似性度量方法， 然后再对每个算法的具体细节进行展开</li></ul><p><strong>基于模型的协同过滤</strong></p><ul><li>奇异值分解（SVD）</li><li>潜在语义分析（LSA） </li><li>支持向量机（SVM）</li></ul><h2 id="2-相似性度量方法"><a href="#2-相似性度量方法" class="headerlink" title="2. 相似性度量方法"></a>2. 相似性度量方法</h2><ul><li>余弦相似度：<br>余弦相似度衡量了两个向量的夹角，夹角越小越相似。公式如下： $$ sim_{uv}=\frac{|N(u)| \cap |N(v)|}{\sqrt{|N(u)|\cdot|N(v)|}} $$<br>  从向量的角度进行描述，令矩阵$A$为用户-商品交互矩阵(因为是TopN推荐并不需要用户对物品的评分，只需要知道用户对商品是否有交互就行)，即矩阵的每一行表示一个用户对所有商品的交互情况，有交互的商品值为1没有交互的商品值为0，矩阵的列表示所有商品。若用户和商品数量分别为$m,n$的话，交互矩阵$A$就是一个$m$行$n$列的矩阵。此时用户的相似度可以表示为(其中$u\cdot v$指的是向量点积)： $$ sim_{uv} = cos(u,v) =\frac{u\cdot v}{|u|\cdot |v|} $$  </li></ul><p>其取值范围是[-1,1],夹角越小，余弦值越接近于1，它们的方向越吻合，则越相似</p><ul><li>皮尔逊相关系数：</li></ul><p>皮尔逊相关系数的公式与余弦相似度的计算公式非常的类似,其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)： $$ sim_{uv} = \frac{\sum_i r_{ui}<em>r_{vi}}{\sqrt{\sum_i r_{ui}^2}\sqrt{\sum_i r_{vi}^2}} $$ 如下是皮尔逊相关系数计算公式，其中$r_{ui},r_{vi}$分别表示用户$u$和用户$v$对商品$i$是否有交互(或者具体的评分值)，$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的*<em>所有商品交互数量或者具体评分的平均值。</em></em> $$ sim(u,v)=\frac{\sum_{i\in I}(r_{ui}-\bar r_u)(r_{vi}-\bar r_v)}{\sqrt{\sum_{i\in I }(r_{ui}-\bar r_u)^2}\sqrt{\sum_{i\in I }(r_{vi}-\bar r_v)^2}} $$ 所以相比余弦相似度，皮尔逊相关系数通过使用用户的平均分对各独立评分进行修正，减小了用户评分偏置的影响。具体实现， 我们也是可以调包。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这就是一种方法</span></span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br><span class="line"><span class="comment">#返回值的第一项是皮尔森相关系数，第二项是p_value值。</span></span><br><span class="line"><span class="comment"># 一般来说皮尔森相关系数越大，p_value越小，线性相关性就越大。</span></span><br></pre></td></tr></table></figure><pre><code>(0.8164965809277258, 0.1835034190722742)</code></pre><ul><li>杰卡德(Jaccard)相似系数：</li></ul><p>一般计算交集与并集的，计算评分0,1布尔值相似。两个用户$u$和$v$交互商品交集的数量占这两个用户交互商品并集的数量的比例，称为两个集合的杰卡德相似系数，用符号$sim_{uv}$表示，其中$N(u),N(v)$分别表示用户$u$和用户$v$交互商品的集合。 $$ sim_{uv}=\frac{|N(u) \cap N(v)|}{\sqrt{|N(u)| \cup|N(v)|}} $$  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="3-基于用户的协同过滤"><a href="#3-基于用户的协同过滤" class="headerlink" title="3. 基于用户的协同过滤"></a>3. 基于用户的协同过滤</h2><p>图像中的ALice和cary两者的信息比较相似，所以我们可以基于用户，根据alice的信息去推算出Cary的泰坦尼克号评分信息</p><ul><li><p>UserCF算法主要包括两个步骤：</p><p>1、找到和目标用户兴趣相似的集合<br>2、找到这个集合中的用户喜欢的， 且目标用户没有听说过的物品推荐给目标用户。</p></li></ul><p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-xsDgP4fl-1603368735944)(attachment:image.png)]</p><h2 id="4-UserCF编程实现"><a href="#4-UserCF编程实现" class="headerlink" title="4. UserCF编程实现"></a>4. UserCF编程实现</h2><p>首先， 先把数据表给建立起来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    items=&#123;<span class="string">&#x27;A&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">5</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">1</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;B&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">3</span>, <span class="number">2</span>: <span class="number">1</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">3</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;C&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">2</span>, <span class="number">3</span>: <span class="number">4</span>, <span class="number">4</span>: <span class="number">1</span>, <span class="number">5</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;D&#x27;</span>: &#123;<span class="number">1</span>: <span class="number">4</span>, <span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">3</span>, <span class="number">4</span>: <span class="number">5</span>, <span class="number">5</span>: <span class="number">2</span>&#125;,</span><br><span class="line">           <span class="string">&#x27;E&#x27;</span>: &#123;<span class="number">2</span>: <span class="number">3</span>, <span class="number">3</span>: <span class="number">5</span>, <span class="number">4</span>: <span class="number">4</span>, <span class="number">5</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    users=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> items,users</span><br><span class="line"></span><br><span class="line">items, users = loadData()</span><br><span class="line">item_df = pd.DataFrame(items).T</span><br><span class="line">user_df = pd.DataFrame(users).T</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(item_df)</span><br><span class="line"><span class="built_in">print</span>(user_df)</span><br></pre></td></tr></table></figure><pre><code>     1    2    3    4    5A  5.0  3.0  4.0  3.0  1.0B  3.0  1.0  3.0  3.0  5.0C  4.0  2.0  4.0  1.0  5.0D  4.0  3.0  3.0  5.0  2.0E  NaN  3.0  5.0  4.0  1.0     A    B    C    D    E1  5.0  3.0  4.0  4.0  NaN2  3.0  1.0  2.0  3.0  3.03  4.0  3.0  4.0  3.0  5.04  3.0  3.0  1.0  5.0  4.05  1.0  5.0  5.0  2.0  1.0</code></pre><ul><li><p>计算用户相似性矩阵:</p><p>这个是一个共现矩阵, 5*5，行代表每个用户， 列代表每个用户， 值代表用户和用户的相关性，这里的思路是这样， 因为要求用户和用户两两的相关性， 所以需要用双层循环遍历用户-物品评分数据， 当不是同一个用户的时候， 我们要去遍历物品-用户评分数据， 在里面去找这两个用户同时对该物品评过分的数据放入到这两个用户向量中。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="string">&quot;&quot;&quot;计算用户相似性矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(np.zeros((<span class="built_in">len</span>(users), <span class="built_in">len</span>(users))), index=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>], columns=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条用户-物品评分数据</span></span><br><span class="line"><span class="keyword">for</span> userID <span class="keyword">in</span> users:</span><br><span class="line">    <span class="keyword">for</span> otheruserId <span class="keyword">in</span> users:</span><br><span class="line">        vec_user = []</span><br><span class="line">        vec_otheruser = []</span><br><span class="line">        <span class="keyword">if</span> userID != otheruserId:</span><br><span class="line">            <span class="keyword">for</span> itemId <span class="keyword">in</span> items:   <span class="comment"># 遍历物品-用户评分数据</span></span><br><span class="line">                itemRatings = items[itemId]        <span class="comment"># 这也是个字典  每条数据为所有用户对当前物品的评分</span></span><br><span class="line">                <span class="keyword">if</span> userID <span class="keyword">in</span> itemRatings <span class="keyword">and</span> otheruserId <span class="keyword">in</span> itemRatings:  <span class="comment"># 说明两个用户都对该物品评过分</span></span><br><span class="line">                    vec_user.append(itemRatings[userID])</span><br><span class="line">                    vec_otheruser.append(itemRatings[otheruserId])</span><br><span class="line">            <span class="comment"># 这里可以获得相似性矩阵(共现矩阵)</span></span><br><span class="line">            similarity_matrix[userID][otheruserId] = np.corrcoef(np.array(vec_user), np.array(vec_otheruser))[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">            <span class="comment">#similarity_matrix[userID][otheruserId] = cosine_similarity(np.array(vec_user), np.array(vec_otheruser))[0][1]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">similarity_matrix</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>1</th>      <th>2</th>      <th>3</th>      <th>4</th>      <th>5</th>    </tr>  </thead>  <tbody>    <tr>      <td>1</td>      <td>0.000000</td>      <td>0.852803</td>      <td>0.707107</td>      <td>0.000000</td>      <td>-0.792118</td>    </tr>    <tr>      <td>2</td>      <td>0.852803</td>      <td>0.000000</td>      <td>0.467707</td>      <td>0.489956</td>      <td>-0.900149</td>    </tr>    <tr>      <td>3</td>      <td>0.707107</td>      <td>0.467707</td>      <td>0.000000</td>      <td>-0.161165</td>      <td>-0.466569</td>    </tr>    <tr>      <td>4</td>      <td>0.000000</td>      <td>0.489956</td>      <td>-0.161165</td>      <td>0.000000</td>      <td>-0.641503</td>    </tr>    <tr>      <td>5</td>      <td>-0.792118</td>      <td>-0.900149</td>      <td>-0.466569</td>      <td>-0.641503</td>      <td>0.000000</td>    </tr>  </tbody></table></div><p>  <strong>计算前n个相似的用户</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算前n个相似的用户&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similarity_users = similarity_matrix[<span class="number">1</span>].sort_values(ascending=<span class="literal">False</span>)[:n].index.tolist()    <span class="comment"># [2, 3]   也就是用户1和用户2</span></span><br></pre></td></tr></table></figure><p> <strong>计算最终得分</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算最终得分&quot;&quot;&quot;</span></span><br><span class="line">base_score = np.mean(np.array([value <span class="keyword">for</span> value <span class="keyword">in</span> users[<span class="number">1</span>].values()]))</span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> similarity_users:  <span class="comment"># [2, 3]</span></span><br><span class="line">    corr_value = similarity_matrix[<span class="number">1</span>][user]            <span class="comment"># 两个用户之间的相似性</span></span><br><span class="line">    mean_user_score = np.mean(np.array([value <span class="keyword">for</span> value <span class="keyword">in</span> users[user].values()]))    <span class="comment"># 每个用户的打分平均值</span></span><br><span class="line">    weighted_scores += corr_value * (users[user][<span class="string">&#x27;E&#x27;</span>]-mean_user_score)      <span class="comment"># 加权分数</span></span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line">final_scores = base_score + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;用户Alice对物品5的打分: &#x27;</span>, final_scores)</span><br><span class="line">user_df.loc[<span class="number">1</span>][<span class="string">&#x27;E&#x27;</span>] = final_scores</span><br><span class="line">user_df</span><br></pre></td></tr></table></figure><pre><code>用户Alice对物品5的打分:  4.871979899370592</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <td>1</td>      <td>5.0</td>      <td>3.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4.87198</td>    </tr>    <tr>      <td>2</td>      <td>3.0</td>      <td>1.0</td>      <td>2.0</td>      <td>3.0</td>      <td>3.00000</td>    </tr>    <tr>      <td>3</td>      <td>4.0</td>      <td>3.0</td>      <td>4.0</td>      <td>3.0</td>      <td>5.00000</td>    </tr>    <tr>      <td>4</td>      <td>3.0</td>      <td>3.0</td>      <td>1.0</td>      <td>5.0</td>      <td>4.00000</td>    </tr>    <tr>      <td>5</td>      <td>1.0</td>      <td>5.0</td>      <td>5.0</td>      <td>2.0</td>      <td>1.00000</td>    </tr>  </tbody></table></div><h2 id="5-、基于物品的协同过滤"><a href="#5-、基于物品的协同过滤" class="headerlink" title="5 、基于物品的协同过滤"></a>5 、基于物品的协同过滤</h2><p>基于物品的协同过滤，图片中泰坦尼克号和阿甘正传，阿甘正传和机器人总动员相似。我们目测这两行比较相似：因此喜欢泰坦尼克号的也有可能喜欢阿甘正传</p><ul><li><p>基于物品的协同过滤算法主要分为两步：</p><p>1)、计算物品之间的相似度</p><p>2)、根据物品的相似度和用户的历史行为给用户生成推荐列表（购买了该商品的用户也经常购买的其他商品）</p></li></ul><p>[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-H1wLnI7O-1603368735966)(attachment:image.png)]</p><h2 id="6-ItemCF编程实现"><a href="#6-ItemCF编程实现" class="headerlink" title="6.ItemCF编程实现"></a>6.ItemCF编程实现</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;计算物品的相似矩阵&quot;&quot;&quot;</span></span><br><span class="line">similarity_matrix = pd.DataFrame(np.ones((<span class="built_in">len</span>(items), <span class="built_in">len</span>(items))), index=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>], columns=[<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>, <span class="string">&#x27;E&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> itemId <span class="keyword">in</span> items:</span><br><span class="line">    <span class="keyword">for</span> otheritemId <span class="keyword">in</span> items:</span><br><span class="line">        vec_item = []         <span class="comment"># 定义列表， 保存当前两个物品的向量值</span></span><br><span class="line">        vec_otheritem = []</span><br><span class="line">        <span class="comment">#userRagingPairCount = 0     # 两件物品均评过分的用户数</span></span><br><span class="line">        <span class="keyword">if</span> itemId != otheritemId:    <span class="comment"># 物品不同</span></span><br><span class="line">            <span class="keyword">for</span> userId <span class="keyword">in</span> users:    <span class="comment"># 遍历用户-物品评分数据</span></span><br><span class="line">                userRatings = users[userId]    <span class="comment"># 每条数据为该用户对所有物品的评分， 这也是个字典</span></span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> itemId <span class="keyword">in</span> userRatings <span class="keyword">and</span> otheritemId <span class="keyword">in</span> userRatings:   <span class="comment"># 用户对这两个物品都评过分</span></span><br><span class="line">                    <span class="comment">#userRagingPairCount += 1</span></span><br><span class="line">                    vec_item.append(userRatings[itemId])</span><br><span class="line">                    vec_otheritem.append(userRatings[otheritemId])</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 这里可以获得相似性矩阵(共现矩阵)</span></span><br><span class="line">            similarity_matrix[itemId][otheritemId] = np.corrcoef(np.array(vec_item), np.array(vec_otheritem))[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">            <span class="comment">#similarity_matrix[itemId][otheritemId] = cosine_similarity(np.array(vec_item), np.array(vec_otheritem))[0][1]</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">similarity_matrix</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <td>A</td>      <td>1.000000</td>      <td>-0.476731</td>      <td>-0.123091</td>      <td>0.532181</td>      <td>0.969458</td>    </tr>    <tr>      <td>B</td>      <td>-0.476731</td>      <td>1.000000</td>      <td>0.645497</td>      <td>-0.310087</td>      <td>-0.478091</td>    </tr>    <tr>      <td>C</td>      <td>-0.123091</td>      <td>0.645497</td>      <td>1.000000</td>      <td>-0.720577</td>      <td>-0.427618</td>    </tr>    <tr>      <td>D</td>      <td>0.532181</td>      <td>-0.310087</td>      <td>-0.720577</td>      <td>1.000000</td>      <td>0.581675</td>    </tr>    <tr>      <td>E</td>      <td>0.969458</td>      <td>-0.478091</td>      <td>-0.427618</td>      <td>0.581675</td>      <td>1.000000</td>    </tr>  </tbody></table></div><p>然后也是得到与物品5相似的前n个物品， 计算出最终得分来</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;得到与物品5相似的前n个物品&quot;&quot;&quot;</span></span><br><span class="line">n = <span class="number">2</span></span><br><span class="line">similarity_items = similarity_matrix[<span class="string">&#x27;E&#x27;</span>].sort_values(ascending=<span class="literal">False</span>)[<span class="number">1</span>:n+<span class="number">1</span>].index.tolist()       <span class="comment"># [&#x27;A&#x27;, &#x27;D&#x27;]</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;计算最终得分&quot;&quot;&quot;</span></span><br><span class="line">base_score = np.mean(np.array([value <span class="keyword">for</span> value <span class="keyword">in</span> items[<span class="string">&#x27;E&#x27;</span>].values()]))</span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> similarity_items:  <span class="comment"># [&#x27;A&#x27;, &#x27;D&#x27;]</span></span><br><span class="line">    corr_value = similarity_matrix[<span class="string">&#x27;E&#x27;</span>][item]            <span class="comment"># 两个物品之间的相似性</span></span><br><span class="line">    mean_item_score = np.mean(np.array([value <span class="keyword">for</span> value <span class="keyword">in</span> items[item].values()]))    <span class="comment"># 每个物品的打分平均值</span></span><br><span class="line">    weighted_scores += corr_value * (users[<span class="number">1</span>][item]-mean_item_score)      <span class="comment"># 加权分数</span></span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line">final_scores = base_score + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;用户Alice对物品5的打分: &#x27;</span>, final_scores)</span><br><span class="line">user_df.loc[<span class="number">1</span>][<span class="string">&#x27;E&#x27;</span>] = final_scores</span><br><span class="line">user_df</span><br></pre></td></tr></table></figure><pre><code>用户Alice对物品5的打分:  4.6</code></pre><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>A</th>      <th>B</th>      <th>C</th>      <th>D</th>      <th>E</th>    </tr>  </thead>  <tbody>    <tr>      <td>1</td>      <td>5.0</td>      <td>3.0</td>      <td>4.0</td>      <td>4.0</td>      <td>4.6</td>    </tr>    <tr>      <td>2</td>      <td>3.0</td>      <td>1.0</td>      <td>2.0</td>      <td>3.0</td>      <td>3.0</td>    </tr>    <tr>      <td>3</td>      <td>4.0</td>      <td>3.0</td>      <td>4.0</td>      <td>3.0</td>      <td>5.0</td>    </tr>    <tr>      <td>4</td>      <td>3.0</td>      <td>3.0</td>      <td>1.0</td>      <td>5.0</td>      <td>4.0</td>    </tr>    <tr>      <td>5</td>      <td>1.0</td>      <td>5.0</td>      <td>5.0</td>      <td>2.0</td>      <td>1.0</td>    </tr>  </tbody></table></div><h2 id="7-算法评估"><a href="#7-算法评估" class="headerlink" title="7. 算法评估"></a>7. 算法评估</h2><p>由于UserCF和ItemCF结果评估部分是共性知识点， 所以在这里统一标识。 这里介绍评测指标：</p><ul><li><p>召回率</p><p>对用户u推荐N个物品记为$R(u)$, 令用户u在测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为： $$ \operatorname{Recall}=\frac{\sum_{u}|R(u) \cap T(u)|}{\sum_{u}|T(u)|} $$ 这个意思就是说， 在用户真实购买或者看过的影片里面， 我模型真正预测出了多少， 这个考察的是模型推荐的一个全面性。</p></li><li><p>准确率:</p><p>  准确率定义为： $$ \operatorname{Precision}=\frac{\sum_{u} \mid R(u) \cap T(u)|}{\sum_{u}|R(u)|} $$ 这个意思再说， 在我推荐的所有物品中， 用户真正看的有多少， 这个考察的是我模型推荐的一个准确性。 为了提高准确率， 模型需要把非常有把握的才对用户进行推荐， 所以这时候就减少了推荐的数量， 而这往往就损失了全面性， 真正预测出来的会非常少，所以实际应用中应该综合考虑两者的平衡。</p></li><li><p>覆盖率: </p><p>  覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。 $$ \text { Coverage }=\frac{\left|\bigcup_{u \in U} R(u)\right|}{|I|} $$该覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。</p></li><li><p>新颖度:</p><p>  用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数</p></li></ul><h2 id="8、协同过滤算法的问题分析"><a href="#8、协同过滤算法的问题分析" class="headerlink" title="8、协同过滤算法的问题分析"></a>8、协同过滤算法的问题分析</h2><p>协同过滤算法存在的问题之一就是泛化能力弱， 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐。</p><p>为了解决这个问题， 同时增加模型的泛化能力，2006年，矩阵分解技术(Matrix Factorization,MF)</p><h2 id="9、UserCF和ItemCF区别"><a href="#9、UserCF和ItemCF区别" class="headerlink" title="9、UserCF和ItemCF区别"></a>9、UserCF和ItemCF区别</h2><p>UserCF很强的社交特性， 这样的特点非常适于<strong>用户少， 物品多</strong>， 时效性较强的场合， 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。<br>对于用户较少， 要求时效性较强的场合， 就可以考虑UserCF。</p><p>ItemCF 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合， 比如推荐艺术品， 音乐， 电影。 下面是UserCF和ItemCF的优缺点对比： （来自项亮推荐系统实践）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure><h2 id="10、协同过滤优缺点"><a href="#10、协同过滤优缺点" class="headerlink" title="10、协同过滤优缺点"></a>10、协同过滤优缺点</h2><p>协同过滤作为一种经典的推荐算法种类，在工业界应用广泛，它的优点很多，模型通用性强，不需要太多对应数据领域的专业知识，工程实现简单，效果也不错。这些都是它流行的原因。</p><p>　　　　当然，协同过滤也有些难以避免的难题，比如令人头疼的“冷启动”问题，我们没有新用户任何数据的时候，无法较好的为新用户推荐物品。同时也没有考虑情景的差异，比如根据用户所在的场景和用户当前的情绪。当然，也无法得到一些小众的独特喜好，这块是基于内容的推荐比较擅长的。</p><p>最后。如果文章中有不足之处，请务必指出，一定迅速改正。谢谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC]&lt;/p&gt;
&lt;h2 id=&quot;1-协同过滤算法介绍&quot;&gt;&lt;a href=&quot;#1-协同过滤算法介绍&quot; class=&quot;headerlink&quot; title=&quot;1. 协同过滤算法介绍&quot;&gt;&lt;/a&gt;1. 协同过滤算法介绍&lt;/h2&gt;&lt;p&gt;协同过滤（&lt;strong&gt;Collabora</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>隐语义模型与矩阵分解</title>
    <link href="http://example.com/2021/05/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E5%92%8CFM%E6%A8%A1%E5%9E%8B/"/>
    <id>http://example.com/2021/05/03/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3%E5%92%8CFM%E6%A8%A1%E5%9E%8B/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:31:52.036Z</updated>
    
    <content type="html"><![CDATA[<p>@[TOC]</p><h1 id="一、-隐语义模型与矩阵分解"><a href="#一、-隐语义模型与矩阵分解" class="headerlink" title="一、 隐语义模型与矩阵分解"></a>一、 隐语义模型与矩阵分解</h1><h2 id="1-、矩阵分解理解"><a href="#1-、矩阵分解理解" class="headerlink" title="1 、矩阵分解理解"></a>1 、矩阵分解理解</h2><p>在之前已经说过协同过滤他对于稀疏矩阵处理能力是非常弱的</p><p>它的特点就是完全没有利用到物品本身或者是用户自身的属性， 仅仅利用了用户与物品的交互信息就可以实现推荐，是一个可解释性很强， 非常直观的模型。</p><p> 为了使得协同过滤更好处理稀疏矩阵问题， 增强泛化能力， 从协同过滤中衍生出矩阵分解模型(Matrix Factorization,MF)或者叫隐语义模型, 两者差不多说的一个意思， 就是在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征， 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</p><h2 id="2-隐语义模型"><a href="#2-隐语义模型" class="headerlink" title="2. 隐语义模型"></a>2. 隐语义模型</h2><p>隐语义模型最早在文本领域被提出，用于找到文本的隐含语义。在2006年， 被用于推荐中， 它的核心思想是通过隐含特征（latent factor）联系用户兴趣和物品（item）， 基于用户的行为找出潜在的主题和分类， 然后对item进行自动聚类，划分到不同类别/主题(用户的兴趣)。</p><p>其实也就是在原来两两对应的相似度上面加上了一个新的隐含层</p><p>先说说协同过滤算法， 这样好对比不同：</p><ul><li><p>对于UserCF：</p><p>  首先需要找到和他们看了同样书的其他用户（兴趣相似的用户），然后给他们推荐那些用户喜欢的其他书。</p></li><li><p>对于ItemCF：</p><p>  需要给他们推荐和他们已经看的书相似的书，比如作者B看了很多关于数据挖掘的书，可以给他推荐机器学习或者模式识别方面的书。</p></li></ul><p>如果是隐语义模型的话， 它会先通过一些角度把用户兴趣和这些书归一下类， 当来了用户之后， 首先得到他的兴趣分类， 然后从这个分类中挑选他可能喜欢的书籍。</p><p>这里就看到了隐语义模型和协同过滤的不同， 这里说的角度其实就是这个隐含特征， 比如书籍的话它的内容， 作者， 年份， 主题等都可以算隐含特征。王喆老师《深度学习推荐系统》的一个原理图作为对比， 区别简直一目了然：</p><p>  <img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214158652.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>我们下面拿一个音乐评分的例子来具体看一下隐特征矩阵的含义。</p><p>假设每个用户都有自己的听歌偏好， 比如A喜欢带有小清新的， 吉他伴奏的， 王菲的歌曲，如果一首歌正好是王菲唱的， 并且是吉他伴奏的小清新， 那么就可以将这首歌推荐给这个用户。 也就是说是小清新， 吉他伴奏， 王菲这些元素连接起了用户和歌曲。 当然每个用户对不同的元素偏好不同， 每首歌包含的元素也不一样， 所以我们就希望找到下面的两个矩阵：</p><ul><li>潜在因子—— 用户矩阵Q 这个矩阵表示不同用户对于不同元素的偏好程度， 1代表很喜欢， 0代表不喜欢， 比如下面这样：</li></ul><p> <img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214137974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><ul><li>潜在因子——音乐矩阵P 表示每种音乐含有各种元素的成分， 比如下表中， 音乐A是一个偏小清新的音乐， 含有小清新的Latent Factor的成分是0.9， 重口味的成分是0.1， 优雅成分0.2…..</li></ul><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214234924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-QSjbr4Xq-1603632614463)(attachment:image.png)]"></p><p>利用上面的这两个矩阵， 我们就能得出张三对音乐A的喜欢程度：</p><p>张三对小清新的偏好 * 音乐A含有小清新的成分 + 张三对重口味的偏好 * 音乐A含有重口味的成分 + 张三对优雅的偏好 * 音乐A含有优雅的成分….,</p><p>下面是对应的两个隐向量：</p><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214259961.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="["></p><p>根据隐向量其实就可以得到张三对音乐A的打分，即： $$0.6 * 0.9 + 0.8 * 0.1 + 0.1 * 0.2 + 0.1 * 0.4 + 0.7 * 0 = 0.69$$ 按照这个计算方式， 每个用户对每首歌其实都可以得到这样的分数， 最后就得到了我们的评分矩阵：</p><p>这里的红色表示用户没有打分，我们通过隐向量计算得到的。<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214318129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>上面例子中的小清晰， 重口味， 优雅这些就可以看做是隐含特征， 而通过这个隐含特征就可以把用户的兴趣和音乐的进行一个分类， 其实就是找到了每个用户每个音乐的一个隐向量表达形式（embedding的原理其实也是这样， 那里是找到每个词的隐向量表达）， 这个隐向量就可以反映出用户的兴趣和物品的风格，并能将相似的物品推荐给相似的用户等。 有没有感觉到是把协同过滤算法进行了一种延伸， 把用户的相似性和物品的相似性通过了一个叫做隐向量的方式进行表达</p><p>但是， 真实的情况下我们其实是没有上面那两个矩阵的， 音乐那么多， 用户那么多， 我们没有办法去找一些隐特征去表示出这些东西， 另外一个问题就是即使能表示也不一定准， 对于每个用户或者每个物品的风格，我们每个人都有不同的看法。 所以事实上， 我们有的只有用户的评分矩阵， 也就是最后的结果， 并且一般这种矩阵长这样：</p><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214342852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="[外链图片转存失败,源站可能有防盗链机制,建议将图片保存下来直接上传(img-hgOHjQjv-1603632614484)(attachment:image.png)]"></p><p>这种矩阵非常的稀疏，如果直接基于用户相似性或者物品相似性去填充这个矩阵是不太容易的， 并且很容易出现长尾问题， 所以矩阵分解就可以比较容易的解决这个问题。</p><p>矩阵分解模型其实就是在想办法基于这个评分矩阵去找到上面例子中的那两个矩阵， 也就是用户兴趣和物品的隐向量表达， 然后就把这个评分矩阵分解成Q和P两个矩阵乘积的形式， 这时候就可以基于这两个矩阵去预测某个用户对某个物品的评分了。 然后基于这个评分去进行推荐。这就是矩阵分解算法的原理。</p><h2 id="3-矩阵分解算法的原理"><a href="#3-矩阵分解算法的原理" class="headerlink" title="3. 矩阵分解算法的原理"></a>3. 矩阵分解算法的原理</h2><p>在矩阵分解的算法框架下， 我们就可以通过分解协同过滤的共现矩阵来得到用户和物品的隐向量， 就是上面的用户矩阵Q和物品矩阵P， 这也是“矩阵分解”名字的由来</p><p> <img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214453932.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>矩阵分解算法将$m\times n$维的共享矩阵$R$分解成$m \times k$维的用户矩阵$U$和$k \times n$维的物品矩阵$V$相乘的形式。 其中$m$是用户数量， $n$是物品数量， $k$是隐向量维度， 也就是隐含特征个数， 只不过这里的隐含特征变得不可解释了， 即我们不知道具体含义了， 要模型自己去学。$k$的大小决定了隐向量表达能力的强弱， $k$越大， 表达信息就越强， 理解起来就是把用户的兴趣和物品的分类划分的越具体。</p><p>那么如果有了用户矩阵和物品矩阵的话， 我们就知道了如果想计算用户$u$对物品$i$的评分， 只需要 $$ \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{f=1}^{F} p_{u, k} q_{k,i} $$ 这里的$p_u$就是用户$u$的隐向量， 就类似与上面的张三向量， 注意这是列向量， $q_i$是物品$i$的隐向量， 就类似于上面的音乐A向量， 这个也是列向量， 所以才用了$p_{u}^{T} q_{i}$得到了一个数， 也就是用户的最终评分， 计算过程其实和上面例子中一样。 这里的$p_{u,k}$和$q_{i,k}$是模型的参数， 也正是我们想办法要计算的， $p_{u,k}$度量的是用户$u$的兴趣和第$k$个隐类的关系， 而$q_{i,k}$度量了第$k$个隐类和物品$i$之间的关系。</p><h2 id="4-矩阵分解算法的求解"><a href="#4-矩阵分解算法的求解" class="headerlink" title="4. 矩阵分解算法的求解"></a>4. 矩阵分解算法的求解</h2><p>谈到矩阵分解， 最常用的方法是特征值分解(EVD)或者奇异值分解(SVD）， 关于这两个的具体原理可以参考下面的链接奇异值分解(SVD)的原理详解及推导，但是这两种方式在这里不适用。</p><p>首先是EVD， 它要求分解的矩阵是方阵， 显然用户-物品矩阵不满足这个要求， 而传统的SVD分解， 会要求原始矩阵是稠密的， 而我们这里的这种矩阵一般情况下是非常稀疏的， 如果想用奇异值分解， 就必须对缺失的元素进行填充， 而一旦补全， 空间复杂度就会非常高， 且补的不一定对。 然后就是SVD分解计算复杂度非常高， 而我们的用户-物品矩阵非常大， 所以基本上无法使用。</p><ol start="5"><li>Basic SVD<br>2006年的Netflix Prize之后， Simon Funk公布了一个矩阵分解算法叫做Funk-SVD, 后来被Netflix Prize的冠军Koren称为Latent Factor Model(LFM)。 Funk-SVD的思想很简单： 把求解上面两个矩阵的参数问题转换成一个最优化问题， 可以通过训练集里面的观察值利用最小化来学习用户矩阵和物品矩阵。</li></ol><p>我们上面已经知道了， 如果有了用户矩阵和物品矩阵的话， 我们就知道了如果想计算用户$u$对物品$i$的评分， 只需要 $$ \operatorname{Preference}(u, i)=r_{u i}=p_{u}^{T} q_{i}=\sum_{f=1}^{F} p_{u, k} q_{k,i} $$ 而现在， 我们有真实的$r_{u,i}$, 但是没有$p_{u}^{T} q_{i}$, 那么我们可以初始化一个啊， 随机初始化一个用户矩阵$U$和一个物品矩阵$V$， 然后不就有$p_{u}^{T} q_{i}$了？ 当然你说， 随机初始化的肯定不准啊， 但是， 有了$p_{u}^{T} q_{i}$之后， 我们就可以计算一个猜测的$\hat{r}{u i}$, 即 $$ \hat{r}{u i}=p_{u}^{T} q_{i} $$</p><p>这时候， 肯定是不准， 那么这个猜测的和真实值之间就会有一个误差： $$ e_{u i}=r_{u i}-\hat{r}_{u i} $$</p><p>有了误差， 我们就可以计算出总的误差平方和： $$ \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{k, i}\right)^{2} $$ 有了损失， 我们就可以想办法进行训练， 把SSE降到最小， 那么我们的两个矩阵参数就可以算出来。所以就把这个问题转成了最优化的的问题， 而我们的目标函数就是：</p><p>$$ \min {\boldsymbol{q}^{}, \boldsymbol{p}^{}} \sum{(u, i) \in K}\left(\boldsymbol{r}{\mathrm{ui}}-p{u}^{T} q_{i}\right)^{2} $$</p><p>这里的$K$表示所有用户评分样本的集合。</p><p>有了目标函数， 那么我们就可以使用梯度下降算法来降低损失。 那么我们需要对目标函数求偏导， 得到梯度。 我们的目标函数如果是上面的SSE， 我们下面来推导一下最后的导数：</p><p>$$ \operatorname{SSE}=\sum_{u, i} e_{u i}^{2}=\sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{k,i}\right)^{2} $$ 首先我们求SSE在$p_{u,k}$（也就是Q矩阵的第$u$行$k$列）的梯度： $$ \frac{\partial}{\partial p_{u,k}} S S E=\frac{\partial}{\partial p_{u,k}}\left(e_{u i}^{2}\right) =2e_{u i} \frac{\partial}{\partial p_{u,k}} e_{u i}=2e_{u i} \frac{\partial}{\partial p_{u,k}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{k,i}\right)=-2e_{u i} q_{k,i} $$ 然后求SSE在$q_{k,i}$处(也就是V矩阵的第$k$行$i$列）的梯度：</p><p>$$ \frac{\partial}{\partial q_{k,i}} S S E=\frac{\partial}{\partial p_{k,i}}\left(e_{u i}^{2}\right) =2e_{u i} \frac{\partial}{\partial p_{k,i}} e_{u i}=2e_{u i} \frac{\partial}{\partial p_{k,i}}\left(r_{u i}-\sum_{k=1}^{K} p_{u,k} q_{k,i}\right)=-2e_{u i} p_{u,k} $$ 为了让公式更为简单， 把前面的2给他越掉， 即可以令SSE等于： $$ \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}=\frac{1}{2} \sum_{u, i}\left(r_{u i}-\sum_{k=1}^{K} p_{u k} q_{k i}\right)^{2} $$</p><p>这时候， 梯度就没有前面的系数了， 有了梯度， 接下来我们就可以用梯度下降算法更新梯度了： $$ p_{u, k}=p_{u,k}-\eta (-e_{ui}q_{k,i})=p_{u,k}+\eta e_{ui}q_{k,i} \ q_{k, i}=q_{k, i}-\eta (-e_{ui}p_{u,k})=q_{k, i}+\eta e_{ui}p_{u,k} $$</p><p>这里的$\eta$是学习率， 控制步长用的， 但上面这个有个问题就是当参数很多的时候， 就是两个矩阵很大的时候， 往往容易陷入过拟合的困境， 这时候， 就需要在目标函数上面加上正则化的损失， 就变成了RSVD， 关于RSVD的详细内容， 可以参考下面给出的链接， 由于篇幅原因， 这里不再过多的赘述。</p><p>但在实际中， 单纯的$\hat{r}{u i}=p{u}^{T} q_{i}$也是不够的， 还要考虑其他的一些因素， 比如一个评分系统， 有些固有的属性和用户物品无关， 而用户也有些属性和物品无关， 物品也有些属性和用户无关。 因此， Netfix Prize中提出了另一种LFM， 在原来的基础上加了偏置项， 来消除用户和物品打分的偏差， 即预测公式如下： $$ \hat{r}{u i}=\mu+b{u}+b_{i}+p_{u}^{T} \cdot q_{i} $$ 这个预测公式加入了3项偏置$\mu,b_u,b_i$, 作用如下：</p><p>$\mu$: 训练集中所有记录的评分的全局平均数。 在不同网站中， 因为网站定位和销售物品不同， 网站的整体评分分布也会显示差异。 比如有的网站中用户就喜欢打高分， 有的网站中用户就喜欢打低分。 而全局平均数可以表示网站本身对用户评分的影响。<br>$b_u$: 用户偏差系数， 可以使用用户$u$给出的所有评分的均值， 也可以当做训练参数。 这一项表示了用户的评分习惯中和物品没有关系的那种因素。 比如有些用户比较苛刻， 对什么东西要求很高， 那么他评分就会偏低， 而有些用户比较宽容， 对什么东西都觉得不错， 那么评分就偏高<br>$b_i$: 物品偏差系数， 可以使用物品$i$收到的所有评分的均值， 也可以当做训练参数。 这一项表示了物品接受的评分中和用户没有关系的因素。 比如有些物品本身质量就很高， 因此获得的评分相对比较高， 有的物品本身质量很差， 因此获得的评分相对较低。<br>加了用户和物品的打分偏差之后， 矩阵分解得到的隐向量更能反映不同用户对不同物品的“真实”态度差异， 也就更容易捕捉评价数据中有价值的信息， 从而避免推荐结果有偏。 注意此时的$SSE$会发生变化： $$ \begin{array}{l} \operatorname{SSE}=\frac{1}{2} \sum_{u, i} e_{u i}^{2}+\frac{1}{2} \lambda \sum_{u}\left|\boldsymbol{p}{u}\right|^{2}+\frac{1}{2} \lambda \sum{i}\left|\boldsymbol{q}{i}\right|^{2}+\frac{1}{2} \lambda \sum{u} \boldsymbol{b}{u}^{2}+\frac{1}{2} \lambda \sum{u} \boldsymbol{b}{i}^{2} \ =\frac{1}{2} \sum{u, i}\left(\boldsymbol{r}{u i}-\boldsymbol{\mu}-\boldsymbol{b}{u}-\boldsymbol{b}{i}-\sum{k=1}^{K} \boldsymbol{p}{u k} \boldsymbol{q}{k i}\right)^{2}+\frac{1}{2} \lambda \sum_{u}\left|\boldsymbol{p}{u}\right|^{2}+\frac{1}{2} \lambda \sum{i}\left|\boldsymbol{q}{i}\right|^{2}+\frac{\mathbf{1}}{2} \lambda \sum{u} \boldsymbol{b}{u}^{2}+\frac{1}{2} \lambda \sum{u} \boldsymbol{b}_{i}^{2} \end{array} $$ 此时如果把$b_u$和$b_i$当做训练参数的话， 那么它俩的梯度是：</p><p>$$ \frac{\partial}{\partial b_{u}} S S E=-e_{u i}+\lambda b_{u} \ \frac{\partial}{\partial b_{i}} S S E=-e_{u i}+\lambda b_{i} $$ 更新公式为： $$ \begin{aligned} \boldsymbol{b}{u}&amp;=\boldsymbol{b}{\boldsymbol{u}}+\boldsymbol{\eta}\left(\boldsymbol{e}{u i}-\lambda \boldsymbol{b}{\boldsymbol{u}}\right) \ \boldsymbol{b}{\boldsymbol{i}} &amp;=\boldsymbol{b}{\boldsymbol{i}}+\boldsymbol{\eta}\left(\boldsymbol{e}{\boldsymbol{u} i}-\lambda \boldsymbol{b}{\boldsymbol{i}}\right) \end{aligned} $$ 而对于$p_{u,k}$和$p_{k,i}$， 导数没有变化， 更新公式也没有变化。</p><h2 id="6-编程实现"><a href="#6-编程实现" class="headerlink" title="6. 编程实现"></a>6. 编程实现</h2><p>我们这里用代码实现一下上面的算法来预测上一篇文章里面的那个预测Alice对物品5的评分， 看看矩阵分解到底是怎么进行预测或者是推荐的。 我把之前的例子拿过来：</p><p> <img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20201025214416937.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><p>任务就是根据这个评分矩阵， 猜测Alice对物品5的打分。</p><p>在实现SVD之前， 先来回忆一下ItemCF和UserCF对于这个问题的做法， 首先ItemCF的做法， 根据已有的用户打分计算物品之间的相似度， 得到物品的相似度矩阵， 根据这个相似度矩阵， 选择出前K个与物品5最相似的物品， 然后基于Alice对这K个物品的得分， 猜测Alice对物品5的得分， 有一个加权的计算公式。 UserCF的做法是根据用户对其他物品的打分， 计算用户之间的相似度， 选择出与Alice最相近的K个用户， 然后基于那K个用户对物品5的打分计算出Alice对物品5的打分。 但是， 这两种方式有个问题， 就是如果矩阵非常稀疏的话， 当然这个例子是个特例， 一般矩阵都是非常稀疏的， 那么预测效果就不好， 因为两个相似用户对同一物品打分的概率以及Alice同时对两个相似物品打分的概率可能都比较小。 另外， 这两种方法显然没有考虑到全局的物品或者用户， 只是基于了最相似的例子， 很可能有偏。</p><p>那么SVD在解决这个问题上是这么做的：</p><p>首先， 它会先初始化用户矩阵P和物品矩阵Q， P的维度是[users_num, F], Q的维度是[item_nums, F]， 这个F是隐向量的维度。 也就是把通过隐向量的方式把用户的兴趣和F的特点关联了起来。 初始化这两个矩阵的方式很多， 但根据经验， 随机数需要和1/sqrt(F)成正比。 下面代码中会发现。<br>有了两个矩阵之后， 我就可以根据用户已经打分的数据去更新参数， 这就是训练模型的过程， 方法很简单， 就是遍历用户， 对于每个用户， 遍历它打分的物品， 这样就拿到了该用户和物品的隐向量， 然后两者相乘加上偏置就是预测的评分， 这时候与真实评分有个差距， 根据上面的梯度下降就可以进行参数的更新<br>这样训练完之后， 我们就可以得到用户Alice和物品5的隐向量， 根据这个就可以预测Alice对物品5的打分。 下面的代码的逻辑就是上面这两步， 这里使用带有偏置项和正则项的那个SVD算法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SVD</span>():</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, rating_data, F=<span class="number">5</span>, alpha=<span class="number">0.1</span>, lmbda=<span class="number">0.1</span>, max_iter=<span class="number">100</span></span>):</span></span><br><span class="line">        self.F = F           <span class="comment"># 这个表示隐向量的维度</span></span><br><span class="line">        self.P = <span class="built_in">dict</span>()          <span class="comment">#  用户矩阵P  大小是[users_num, F]</span></span><br><span class="line">        self.Q = <span class="built_in">dict</span>()     <span class="comment"># 物品矩阵Q  大小是[item_nums, F]</span></span><br><span class="line">        self.bu = <span class="built_in">dict</span>()   <span class="comment"># 用户偏差系数</span></span><br><span class="line">        self.bi = <span class="built_in">dict</span>()    <span class="comment"># 物品偏差系数</span></span><br><span class="line">        self.mu = <span class="number">0.0</span>        <span class="comment"># 全局偏差系数</span></span><br><span class="line">        self.alpha = alpha   <span class="comment"># 学习率</span></span><br><span class="line">        self.lmbda = lmbda    <span class="comment"># 正则项系数</span></span><br><span class="line">        self.max_iter = max_iter    <span class="comment"># 最大迭代次数</span></span><br><span class="line">        self.rating_data = rating_data <span class="comment"># 评分矩阵</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 初始化矩阵P和Q, 方法很多， 一般用随机数填充， 但随机数大小有讲究， 根据经验， 随机数需要和1/sqrt(F)成正比</span></span><br><span class="line">        cnt = <span class="number">0</span>    <span class="comment"># 统计总的打分数， 初始化mu用</span></span><br><span class="line">        <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">            self.P[user] = [random.random() / math.sqrt(self.F)  <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">            self.bu[user] = <span class="number">0</span></span><br><span class="line">            cnt += <span class="built_in">len</span>(items) </span><br><span class="line">            <span class="keyword">for</span> item, rating <span class="keyword">in</span> items.items():</span><br><span class="line">                <span class="keyword">if</span> item <span class="keyword">not</span> <span class="keyword">in</span> self.Q:</span><br><span class="line">                    self.Q[item] = [random.random() / math.sqrt(self.F) <span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, F)]</span><br><span class="line">                    self.bi[item] = <span class="number">0</span></span><br><span class="line">        self.mu /= cnt</span><br><span class="line">        </span><br><span class="line">    <span class="comment"># 有了矩阵之后， 就可以进行训练, 这里使用随机梯度下降的方式训练参数P和Q</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.max_iter):</span><br><span class="line">            <span class="keyword">for</span> user, items <span class="keyword">in</span> self.rating_data.items():</span><br><span class="line">                <span class="keyword">for</span> item, rui <span class="keyword">in</span> items.items():</span><br><span class="line">                    rhat_ui = self.predict(user, item)   <span class="comment"># 得到预测评分</span></span><br><span class="line">                    <span class="comment"># 计算误差</span></span><br><span class="line">                    e_ui = rui - rhat_ui</span><br><span class="line">                    </span><br><span class="line">                    self.bu[user] += self.alpha * (e_ui - self.lmbda * self.bu[user])</span><br><span class="line">                    self.bi[item] += self.alpha * (e_ui - self.lmbda * self.bi[item])</span><br><span class="line">                    <span class="comment"># 随机梯度下降更新梯度</span></span><br><span class="line">                    <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F):</span><br><span class="line">                        self.P[user][k] += self.alpha * (e_ui*self.Q[item][k] - self.lmbda * self.P[user][k])</span><br><span class="line">                        self.Q[item][k] += self.alpha * (e_ui*self.P[user][k] - self.lmbda * self.Q[item][k])</span><br><span class="line">                    </span><br><span class="line">            self.alpha *= <span class="number">0.1</span>    <span class="comment"># 每次迭代步长要逐步缩小</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测user对item的评分， 这里没有使用向量的形式</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span>(<span class="params">self, user, item</span>):</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(self.P[user][f] * self.Q[item][f] <span class="keyword">for</span> f <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.F)) + self.bu[user] + self.bi[item] + self.mu   </span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># 定义数据集， 也就是那个表格， 注意这里我们采用字典存放数据， 因为实际情况中数据是非常稀疏的， 很少有情况是现在这样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span>():</span></span><br><span class="line">    rating_data=&#123;<span class="number">1</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">2</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">           <span class="number">3</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">           <span class="number">4</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">           <span class="number">5</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">          &#125;</span><br><span class="line">    <span class="keyword">return</span> rating_data</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 接下来就是训练和预测</span></span><br><span class="line">rating_data = loadData()</span><br><span class="line">basicsvd = SVD(rating_data, F=<span class="number">10</span>)</span><br><span class="line">basicsvd.train()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> [<span class="string">&#x27;E&#x27;</span>]:</span><br><span class="line">    <span class="built_in">print</span>(item, basicsvd.predict(<span class="number">1</span>, item))</span><br><span class="line"></span><br></pre></td></tr></table></figure><pre><code>E 3.174279615575356</code></pre><h2 id="7、矩阵分解的优缺点分析"><a href="#7、矩阵分解的优缺点分析" class="headerlink" title="7、矩阵分解的优缺点分析"></a>7、矩阵分解的优缺点分析</h2><p>优点：</p><ul><li><p>泛化能力强：</p><p>  一定程度上解决了稀疏问题</p></li><li><p>空间复杂度低： </p><p>  由于用户和物品都用隐向量的形式存放， 少了用户和物品相似度矩阵， 空间复杂度由$n^2$降到$(n+m)*f$</p></li><li><p>更好的扩展性和灵活性：</p><p>  矩阵分解的最终产物是用户和物品隐向量， 这个深度学习的embedding思想不谋而合， 因此矩阵分解的结果非常便于与其他特征进行组合和拼接， 并可以与深度学习无缝结合。<br>但是， 矩阵分解算法依然是只用到了评分矩阵， 没有考虑到用户特征， 物品特征和上下文特征， 这使得矩阵分解丧失了利用很多有效信息的机会， 同时在缺乏用户历史行为的时候， 无法进行有效的推荐。 所以为了解决这个问题， 逻辑回归模型及后续的因子分解机模型， 凭借其天然的融合不同特征的能力， 逐渐在推荐系统领域得到了更广泛的应用。</p></li></ul><h1 id="二、FM模型算法"><a href="#二、FM模型算法" class="headerlink" title="二、FM模型算法"></a>二、FM模型算法</h1><h3 id="1-FM模型理解"><a href="#1-FM模型理解" class="headerlink" title="1.FM模型理解"></a>1.FM模型理解</h3><p>FM模型其实是一种思路，具体的应用稍少。一般来说做推荐CTR预估时最简单的思路就是将特征做线性组合（逻辑回归LR），传入sigmoid中得到一个概率值，本质上这就是一个线性模型，因为sigmoid是单调增函数不会改变里面的线性模型的CTR预测顺序，因此逻辑回归模型效果会比较差。</p><h3 id="2-FM模型的应用"><a href="#2-FM模型的应用" class="headerlink" title="2. FM模型的应用"></a>2. FM模型的应用</h3><p>最直接的想法就是直接把FM得到的结果放进sigmoid中输出一个概率值，由此做CTR预估，事实上我们也可以做召回。</p><p>由于FM模型是利用两个特征的Embedding做内积得到二阶特征交叉的权重，那么我们可以将训练好的FM特征取出离线存好，之后用来做KNN向量检索。</p><p>工业应用的具体操作步骤：</p><ul><li>离线训练好FM模型（学习目标可以是CTR）</li><li>将训练好的FM模型Embedding取出</li><li>将每个uid对应的Embedding做avg pooling（平均）形成该用户最终的Embedding，item也做同样的操作</li><li>将所有的Embedding向量放入Faiss等</li><li>线上uid发出请求，取出对应的user embedding，进行检索召回</li></ul><h3 id="3-FM算法的优缺点"><a href="#3-FM算法的优缺点" class="headerlink" title="3.  FM算法的优缺点"></a>3.  FM算法的优缺点</h3><p>优点：</p><ul><li><p>引入了二阶项，能够自动进行特征组合</p></li><li><p>二阶项的参数为两个特征之间的隐向量（由矩阵分解算法我们可以知道，隐向量具备了特征之间更深层的信息，从而能为我们挖掘出从没出现过的特征组合）</p></li><li><p>隐向量之间具有相关性，能够解决样本稀疏时的参数轨迹问题。</p></li></ul><p>缺点：</p><p>当&lt;vh,vi&gt;和&lt;vi,vj&gt;，它们之间有共同项 vi时，vi是综合了vh和vj等多个和vi有交互的特征学习到的。虽然具有普遍意义，但是当vh和vj的差别很大时，使用同样的vi是不太合理的。因此我们引入了FFM。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;@[TOC]&lt;/p&gt;
&lt;h1 id=&quot;一、-隐语义模型与矩阵分解&quot;&gt;&lt;a href=&quot;#一、-隐语义模型与矩阵分解&quot; class=&quot;headerlink&quot; title=&quot;一、 隐语义模型与矩阵分解&quot;&gt;&lt;/a&gt;一、 隐语义模型与矩阵分解&lt;/h1&gt;&lt;h2 id=&quot;1-、矩阵分解</summary>
      
    
    
    
    <category term="推荐系统" scheme="http://example.com/categories/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
    <category term="中等" scheme="http://example.com/tags/%E4%B8%AD%E7%AD%89/"/>
    
  </entry>
  
  <entry>
    <title>零基础入门金融风控之贷款违约预测Task4：建模和调参</title>
    <link href="http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8BTask4%EF%BC%9A%E5%BB%BA%E6%A8%A1%E5%92%8C%E8%B0%83%E5%8F%82/"/>
    <id>http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8BTask4%EF%BC%9A%E5%BB%BA%E6%A8%A1%E5%92%8C%E8%B0%83%E5%8F%82/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T12:04:25.007Z</updated>
    
    <content type="html"><![CDATA[<h2 id="零基础入门金融风控之贷款违约预测Task4：建模和调参"><a href="#零基础入门金融风控之贷款违约预测Task4：建模和调参" class="headerlink" title="零基础入门金融风控之贷款违约预测Task4：建模和调参"></a>零基础入门金融风控之贷款违约预测Task4：建模和调参</h2><p>@[TOC]</p><h3 id="4-1-学习目标"><a href="#4-1-学习目标" class="headerlink" title="4.1 学习目标"></a>4.1 学习目标</h3><ul><li>学习在金融分控领域常用的机器学习模型</li><li>学习机器学习模型的建模过程与调参流程</li></ul><h3 id="4-2-理论基础"><a href="#4-2-理论基础" class="headerlink" title="4.2 理论基础"></a>4.2 理论基础</h3><p><strong>1.逻辑回归</strong><br>sigmoid函数：<br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200924224909334.png#pic_center" alt="在这里插入图片描述"></p><p>sigmoid是一种分类方法，主要用于两分类问题（即输出只有两种，分别代表两个类别）</p><p><strong>sigmoid函数的特点</strong>：[-5,5]之间快速变化由-1到1,；奇函数；当x≥0 时,y≥0.5,分类为1，当 x&lt;0时,y&lt;0.5,分类为0</p><p><strong>代价函数</strong>：适合于逻辑回归的代价函数是对hx取负对数<br><strong>梯度下降法</strong>：调整参数θ使得代价函数J(θ)取得最小值，形像的说也就是从山顶一步一步的向下挪动到极小值处。这其中挪动的步长被称为学习率；越接近极值点处越小的步长，也就是使用可变学习率的原因。<br><strong>2、树回归</strong><br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200924225116871.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br><strong>3、集成模型</strong><br><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200924225208603.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><h3 id="4-3、模型对比与性能评估"><a href="#4-3、模型对比与性能评估" class="headerlink" title="4.3、模型对比与性能评估"></a>4.3、模型对比与性能评估</h3><h4 id="4-3-1-逻辑回归"><a href="#4-3-1-逻辑回归" class="headerlink" title="4.3.1 逻辑回归"></a>4.3.1 逻辑回归</h4><ul><li><p>优点<br>1、训练速度较快，分类的时候，计算量仅仅只和特征的数目相关；<br>2、简单易理解，模型的可解释性非常好，从特征的权重可以看到不同的特征对最后结果的影响；<br>3、适合二分类问题，不需要缩放输入特征；<br>4、内存资源占用小，只需要存储各个维度的特征值；</p></li><li><p>缺点<br>1、逻辑回归需要预先处理缺失值和异常值<br>2、不能用Logistic回归去解决非线性问题，因为Logistic的决策面是线性的；<br>3、对多重共线性数据较为敏感，且很难处理数据不平衡的问题；<br>4、准确率并不是很高，因为形式非常简单，很难去拟合数据的真实分布；</p><h4 id="4-3-2-决策树模型"><a href="#4-3-2-决策树模型" class="headerlink" title="4.3.2 决策树模型"></a>4.3.2 决策树模型</h4></li><li><p>优点<br>1、简单直观，生成的决策树可以可视化展示<br>2、数据不需要预处理，不需要归一化，不需要处理缺失数据<br>3、既可以处理离散值，也可以处理连续值</p></li><li><p>缺点<br>1、决策树算法非常容易过拟合，导致泛化能力不强（可进行适当的剪枝）<br>2、采用的是贪心算法，容易得到局部最优解</p><h4 id="4-3-3-集成模型集成方法（ensemble-method）"><a href="#4-3-3-集成模型集成方法（ensemble-method）" class="headerlink" title="4.3.3 集成模型集成方法（ensemble method）"></a>4.3.3 集成模型集成方法（ensemble method）</h4><p>通过组合多个学习器来完成学习任务，通过集成方法，可以将多个弱学习器组合成一个强分类器，因此集成学习的泛化能力一般比单一分类器要好。集成方法主要包括Bagging和Boosting，Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个更加强大的分类。两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果。常见的基于Baggin思想的集成模型有：随机森林、基于Boosting思想的集成模型有：Adaboost、GBDT、XgBoost、LightGBM等。</p></li><li><p>Baggin和Boosting的区别总结如下：<br>1、样本选择上： Bagging方法的训练集是从原始集中有放回的选取，所以从原始集中选出的各轮训练集之间是独立的；而Boosting方法需要每一轮的训练集不变，只是训练集中每个样本在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整<br>2、样例权重上： Bagging方法使用均匀取样，所以每个样本的权重相等；而Boosting方法根据错误率不断调整样本的权值，错误率越大则权重越大<br>3、预测函数上： Bagging方法中所有预测函数的权重相等；而Boosting方法中每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重<br>4、并行计算上： Bagging方法中各个预测函数可以并行生成；而Boostin方法各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p><h4 id="4-3-4-模型评估方法"><a href="#4-3-4-模型评估方法" class="headerlink" title="4.3.4 模型评估方法"></a>4.3.4 模型评估方法</h4><p>在训练集上面的误差我们称之为训练误差或者经验误差，而在测试集上的误差称之为测试误差。</p></li><li><p><em>对于数据集的划分，我们通常要保证满足以下两个条件：</em>*</p><ul><li><p>训练集和测试集的分布要与样本真实分布一致，即训练集和测试集都要保证是从样本真实分布中独立同分布采样而得；</p></li><li><p>训练集和测试集要互斥</p></li><li><p><em>对于数据集的划分有三种方法：</em>*</p></li><li><p>留出法：大约2/3~4/5的样本作为训练集，其余的作为测试集。</p></li><li><p>交叉验证法</p></li><li><p>自助法，</p></li><li><p><em>数据集划分总结</em>*</p></li></ul></li><li><p>对于数据量充足的时候，通常采用留出法或者k折交叉验证法来进行训练/测试集的划分；</p></li><li><p>对于数据集小且难以有效划分训练/测试集时使用自助法；</p></li><li><p>对于数据集小且可有效划分的时候最好使用留一法来进行划分，因为这种方法最为准确</p></li></ul><h4 id="4-3-5-模型评价标准"><a href="#4-3-5-模型评价标准" class="headerlink" title="4.3.5 模型评价标准"></a>4.3.5 模型评价标准</h4><p>一般使用roc曲线，这一部分在task1中已经有所介绍。在这里就不加已介绍了，希望可以往回看，模型的评价还是挺重要的。</p><h3 id="4-4、调参方式"><a href="#4-4、调参方式" class="headerlink" title="4.4、调参方式"></a>4.4、调参方式</h3><ul><li>贪心调参方法：<br>先使用当前对模型影响最大的参数进行调优，达到当前参数下的模型最优化，再使用对模型影响次之的参数进行调优，如此下去，直到所有的参数调整完毕。这个方法的缺点就是可能会调到局部最优而不是全局最优，但是只需要一步一步的进行参数最优化调试即可，容易理解。需要注意的是在树模型中参数调整的顺序，也就是各个参数对模型的影响程度，这里列举一下日常调参过程中常用的参数和调参顺序：<br>①：max_depth、num_leaves<br>②：min_data_in_leaf、min_child_weight<br>③：bagging_fraction、 feature_fraction、bagging_freq<br>④：reg_lambda、reg_alpha<br>⑤：min_split_gain</li></ul><ul><li>网格搜索<br>sklearn 提供GridSearchCV用于进行网格搜索，只需要把模型的参数输进去，就能给出最优化的结果和参数。相比起贪心调参，网格搜索的结果会更优，但是网格搜索只适合于小数据集，一旦数据的量级上去了，很难得出结果。</li><li>贝叶斯调参<br>在使用之前需要先安装包bayesian-optimization，运行如下命令即可：<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install bayesian-optimization</span><br></pre></td></tr></table></figure>贝叶斯调参的主要思想是：给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。</li></ul><p>贝叶斯调参的步骤如下：</p><ol><li>定义优化函数(rf_cv）</li><li> 建立模型</li><li>定义待优化参数</li><li>得到优化结果并返回要优化的分数指标</li></ol><h3 id="4-5、代码实战"><a href="#4-5、代码实战" class="headerlink" title="4.5、代码实战"></a>4.5、代码实战</h3><h5 id="4-5-1建立模型"><a href="#4-5-1建立模型" class="headerlink" title="4.5.1建立模型"></a>4.5.1建立模型</h5><p>导入所需要的包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">import pandas as pd</span><br><span class="line">import numpy as np</span><br><span class="line">import lightgbm as lgb</span><br><span class="line">import warnings</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">from sklearn import metrics</span><br><span class="line">from sklearn.model_selection import KFold, train_test_split, cross_val_score, GridSearchCV, StratifiedKFold</span><br><span class="line">from sklearn.metrics import roc_auc_score</span><br><span class="line">from bayes_opt import BayesianOptimization</span><br><span class="line">import datetime</span><br><span class="line">import pickle</span><br><span class="line">import seaborn as sns</span><br><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">sns 相关设置</span></span><br><span class="line"><span class="string">@return:</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"># 声明使用 Seaborn 样式</span></span><br><span class="line"><span class="string">sns.set()</span></span><br><span class="line"><span class="string"># 有五种seaborn的绘图风格，它们分别是：darkgrid, whitegrid, dark, white, ticks。默认的主题是darkgrid。</span></span><br><span class="line"><span class="string">sns.set_style(&quot;whitegrid&quot;)</span></span><br><span class="line"><span class="string"># 有四个预置的环境，按大小从小到大排列分别为：paper, notebook, talk, poster。其中，notebook是默认的。</span></span><br><span class="line"><span class="string">sns.set_context(&#x27;</span>talk<span class="string">&#x27;)</span></span><br><span class="line"><span class="string"># 中文字体设置-黑体</span></span><br><span class="line"><span class="string">plt.rcParams[&#x27;</span>font.sans-serif<span class="string">&#x27;] = [&#x27;</span>SimHei<span class="string">&#x27;]</span></span><br><span class="line"><span class="string"># 解决保存图像是负号&#x27;</span>-<span class="string">&#x27;显示为方块的问题</span></span><br><span class="line"><span class="string">plt.rcParams[&#x27;</span>axes.unicode_minus<span class="string">&#x27;] = False</span></span><br><span class="line"><span class="string"># 解决Seaborn中文显示问题并调整字体大小</span></span><br><span class="line"><span class="string">sns.set(font=&#x27;</span>SimHei<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">pd.options.display.max_columns = None</span><br><span class="line">pd.set_option(<span class="string">&#x27;display.float_format&#x27;</span>, lambda x: <span class="string">&#x27;%.2f&#x27;</span> % x)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>读取数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train = pd.read_csv(<span class="string">&quot;./train.csv&quot;</span>)</span><br><span class="line">testA = pd.read_csv( <span class="string">&quot;./testA.csv&quot;</span>)</span><br></pre></td></tr></table></figure><p>压缩数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">def reduce_mem_usage(df):</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;</span></span><br><span class="line"><span class="string">    遍历DataFrame的所有列并修改它们的数据类型以减少内存使用</span></span><br><span class="line"><span class="string">    :param df: 需要处理的数据集</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    &#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">    start_mem = df.memory_usage().sum() / 1024 ** 2  <span class="comment"># 记录原数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage of dataframe is &#123;:.2f&#125; MB&#x27;</span>.format(start_mem))</span><br><span class="line">    <span class="keyword">for</span> col <span class="keyword">in</span> df.columns:</span><br><span class="line">        col_type = df[col].dtypes</span><br><span class="line">        <span class="keyword">if</span> col_type != object:  <span class="comment"># 这里只过滤了object格式，如果代码中还包含其他类型，要一并过滤</span></span><br><span class="line">            c_min = df[col].min()</span><br><span class="line">            c_max = df[col].max()</span><br><span class="line">            <span class="keyword">if</span> str(col_type)[:3] == <span class="string">&#x27;int&#x27;</span>:  <span class="comment"># 如果是int类型的话,不管是int64还是int32,都加入判断</span></span><br><span class="line">                <span class="comment"># 依次尝试转化成in8,in16,in32,in64类型,如果数据大小没溢出,那么转化</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.iinfo(np.int8).min and c_max &lt; np.iinfo(np.int8).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int8)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int16).min and c_max &lt; np.iinfo(np.int16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int32).min and c_max &lt; np.iinfo(np.int32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int32)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.iinfo(np.int64).min and c_max &lt; np.iinfo(np.int64).max:</span><br><span class="line">                    df[col] = df[col].astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:  <span class="comment"># 不是整形的话,那就是浮点型</span></span><br><span class="line">                <span class="keyword">if</span> c_min &gt; np.finfo(np.float16).min and c_max &lt; np.finfo(np.float16).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float16)</span><br><span class="line">                <span class="keyword">elif</span> c_min &gt; np.finfo(np.float32).min and c_max &lt; np.finfo(np.float32).max:</span><br><span class="line">                    df[col] = df[col].astype(np.float32)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    df[col] = df[col].astype(np.float64)</span><br><span class="line">        <span class="keyword">else</span>:  <span class="comment"># 如果不是数值型的话,转化成category类型</span></span><br><span class="line">            df[col] = df[col].astype(<span class="string">&#x27;category&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    end_mem = df.memory_usage().sum() / 1024 ** 2    <span class="comment"># 看一下转化后的数据的内存大小</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Memory usage after optimization is &#123;:.2f&#125; MB&#x27;</span>.format(end_mem))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;Decreased by &#123;:.1f&#125;%&#x27;</span>.format(100 * (start_mem - end_mem) / start_mem))  <span class="comment"># 看一下压缩比例</span></span><br><span class="line">    <span class="built_in">return</span> df</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train = reduce_mem_usage(train)</span><br><span class="line">testA = reduce_mem_usage(testA)</span><br><span class="line">del testA[<span class="string">&#x27;n2.2&#x27;</span>]</span><br><span class="line">del testA[<span class="string">&#x27;n2.3&#x27;</span>]</span><br></pre></td></tr></table></figure><p>合并数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data = pd.concat([train, testA], axis=0, ignore_index=True) </span><br></pre></td></tr></table></figure><p>由于数据又是重新开始的，所以这里我们将需要对数据进行简单数据预处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">data.groupby(<span class="string">&#x27;employmentLength&#x27;</span>)[<span class="string">&#x27;id&#x27;</span>].count()</span><br><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;10年以上算10年，1年一下算0年&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;10+ years&#x27;</span>, value=<span class="string">&#x27;10 years&#x27;</span>, inplace=True)</span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;&lt; 1 year&#x27;</span>, value=<span class="string">&#x27;0 year&#x27;</span>, inplace=True)</span><br><span class="line"></span><br><span class="line">def employmentLength_to_int(s):</span><br><span class="line">    <span class="keyword">if</span> pd.isnull(s):</span><br><span class="line">        <span class="built_in">return</span> s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> np.int8(s.split()[0])</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>] = data[<span class="string">&#x27;employmentLength&#x27;</span>].apply(employmentLength_to_int)</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(lambda x: x[-4:])</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(lambda x: x[0:3])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def month_re(x):</span><br><span class="line">    <span class="keyword">if</span> x == <span class="string">&#x27;Jan&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;01&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Feb&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;02&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Mar&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;03&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Apr&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;04&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;May&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;05&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jun&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;06&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Jul&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;07&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Aug&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;08&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Sep&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;09&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Oct&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;10&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> x == <span class="string">&#x27;Nov&#x27;</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;11&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="built_in">return</span> <span class="string">&#x27;12&#x27;</span></span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>].apply(lambda x: month_re(x))</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>] + data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line">data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine_date&#x27;</span>].astype(<span class="string">&#x27;int&#x27;</span>)</span><br><span class="line">del data[<span class="string">&#x27;earliesCreditLine&#x27;</span>]</span><br><span class="line">del data[<span class="string">&#x27;earliesCreditLine_year&#x27;</span>]</span><br><span class="line">del data[<span class="string">&#x27;earliesCreditLine_month&#x27;</span>]</span><br><span class="line"></span><br><span class="line">data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>], format=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">data[<span class="string">&#x27;issueDateDt&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(lambda x: x - startdate).dt.days</span><br><span class="line">del data[<span class="string">&#x27;issueDate&#x27;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;</span><span class="string">&#x27;将1类以上且非高维稀疏的特征进行one-hot编码&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>,</span><br><span class="line">                 <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> cate <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(cate, <span class="string">&#x27;类型数&#x27;</span>, data[cate].nunique())</span><br><span class="line">del data[<span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"></span><br><span class="line">data = pd.get_dummies(data, columns=[<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>,</span><br><span class="line">                                     <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>], drop_first=True)</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> [<span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, <span class="string">&#x27;title&#x27;</span>]:</span><br><span class="line">    data[f + <span class="string">&#x27;_counts&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].transform(<span class="string">&#x27;count&#x27;</span>)</span><br><span class="line">    data[f + <span class="string">&#x27;_rank&#x27;</span>] = data.groupby([f])[<span class="string">&#x27;id&#x27;</span>].rank(ascending=False).astype(int)</span><br><span class="line">    del data[f]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data.columns <span class="keyword">if</span> f not <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;isDefault&#x27;</span>]]</span><br><span class="line">train = data[data.isDefault.notnull()].reset_index(drop=True)</span><br><span class="line">testA = data[data.isDefault.isnull()].reset_index(drop=True)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>得到数据</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_train = train[features]</span><br><span class="line">y_train = train[<span class="string">&#x27;isDefault&#x27;</span>]</span><br><span class="line">x_testA = testA[features]</span><br></pre></td></tr></table></figure><p>5次交叉验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">folds = 5</span><br><span class="line">seed = 2020</span><br><span class="line">kf = KFold(n_splits=folds, shuffle=True, random_state=seed)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>建立模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">X_train_split, X_val, y_train_split, y_val = train_test_split(x_train, y_train, test_size=0.2)</span><br><span class="line">train_split_matrix = lgb.Dataset(X_train_split, label=y_train_split)</span><br><span class="line">val_matrix = lgb.Dataset(X_val, label=y_val)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">params = &#123;</span><br><span class="line">    <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>, <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>, <span class="string">&#x27;learning_rate&#x27;</span>: 0.1, <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: 1e-3,</span><br><span class="line">    <span class="string">&#x27;num_leaves&#x27;</span>: 31, <span class="string">&#x27;max_depth&#x27;</span>: -1, <span class="string">&#x27;reg_lambda&#x27;</span>: 0, <span class="string">&#x27;reg_alpha&#x27;</span>: 0, <span class="string">&#x27;feature_fraction&#x27;</span>: 1, <span class="string">&#x27;bagging_fraction&#x27;</span>: 1,</span><br><span class="line">    <span class="string">&#x27;bagging_freq&#x27;</span>: 0, <span class="string">&#x27;seed&#x27;</span>: 2020, <span class="string">&#x27;nthread&#x27;</span>: 8, <span class="string">&#x27;verbose&#x27;</span>: -1</span><br><span class="line">&#125;</span><br><span class="line">model = lgb.train(params, train_set=train_split_matrix, valid_sets=val_matrix, num_boost_round=20000,</span><br><span class="line">                  verbose_eval=1000, early_stopping_rounds=200)</span><br></pre></td></tr></table></figure><p>画出图像</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"> al_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)</span><br><span class="line">fpr, tpr, threshold = metrics.roc_curve(y_val, val_pre_lgb)</span><br><span class="line">roc_auc = metrics.auc(fpr, tpr)</span><br><span class="line">`plt.figure(figsize=(8, 8))</span><br><span class="line">plt.title(<span class="string">&#x27;Val ROC&#x27;</span>)</span><br><span class="line">plt.plot(fpr, tpr, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Val AUC = %0.4f&#x27;</span> % roc_auc)  <span class="comment"># 保留四位小数</span></span><br><span class="line">plt.ylim(0, 1)</span><br><span class="line">plt.xlim(0, 1)</span><br><span class="line">plt.legend(loc=<span class="string">&#x27;best&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;True Positive Rate&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;False Positive Rate&#x27;</span>)</span><br><span class="line">plt.plot([0, 1], [0, 1], <span class="string">&#x27;r--&#x27;</span>)     <span class="comment"># 对角线</span></span><br></pre></td></tr></table></figure><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200924232534496.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"><br>使用5折交叉验证进行模型性能评估</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">cv_scores = []  <span class="comment"># 用于存放每次验证的得分</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ## 五折交叉验证评估模型</span></span><br><span class="line"><span class="keyword">for</span> i, (train_index, val_index) <span class="keyword">in</span> enumerate(kf.split(x_train, y_train)):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*** &#123;&#125; ***&#x27;</span>.format(str(i+1)))</span><br><span class="line"></span><br><span class="line">    X_train_split, y_train_split, X_val, y_val = x_train.iloc[train_index], y_train[train_index], \</span><br><span class="line">                                                 x_train.iloc[val_index], y_train[val_index]</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;划分训练集和验证集&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    train_matrix = lgb.Dataset(X_train_split, label=y_train_split)</span><br><span class="line">    val_matrix = lgb.Dataset(X_val, label=y_val)</span><br><span class="line">    <span class="string">&#x27;&#x27;</span><span class="string">&#x27;转换成lgb训练的数据形式&#x27;</span><span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    params = &#123;</span><br><span class="line">        <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>, <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>, <span class="string">&#x27;learning_rate&#x27;</span>: 0.1, <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>, <span class="string">&#x27;min_child_weight&#x27;</span>: 1e-3,</span><br><span class="line">        <span class="string">&#x27;num_leaves&#x27;</span>: 31, <span class="string">&#x27;max_depth&#x27;</span>: -1, <span class="string">&#x27;reg_lambda&#x27;</span>: 0, <span class="string">&#x27;reg_alpha&#x27;</span>: 0, <span class="string">&#x27;feature_fraction&#x27;</span>: 1,</span><br><span class="line">        <span class="string">&#x27;bagging_fraction&#x27;</span>: 1,</span><br><span class="line">        <span class="string">&#x27;bagging_freq&#x27;</span>: 0, <span class="string">&#x27;seed&#x27;</span>: 2020, <span class="string">&#x27;nthread&#x27;</span>: 8, <span class="string">&#x27;verbose&#x27;</span>: -1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    model = lgb.train(params, train_set=train_matrix, num_boost_round=20000, valid_sets=val_matrix, verbose_eval=1000,</span><br><span class="line">                      early_stopping_rounds=200)</span><br><span class="line">    val_pre_lgb = model.predict(X_val, num_iteration=model.best_iteration)</span><br><span class="line">    cv_scores.append(roc_auc_score(y_val, val_pre_lgb))</span><br><span class="line">    <span class="built_in">print</span>(cv_scores)</span><br></pre></td></tr></table></figure><h4 id="4-5-2、模型调参"><a href="#4-5-2、模型调参" class="headerlink" title="4.5.2、模型调参"></a>4.5.2、模型调参</h4><ul><li>贪心调参</li><li>网格调参</li><li>贝叶斯调参</li></ul><p>后序调参代码将会继续补上，希望读者体谅，本次难度较大，吸收不易，望先把理论搞懂</p><h3 id="4-6、结束总结"><a href="#4-6、结束总结" class="headerlink" title="4.6、结束总结"></a>4.6、结束总结</h3><p>在本次学习中，各种模型都有了一个统称介绍，可能比较粗糙，毕竟篇幅有限，但对此用来复习总结还是不错的。这次task4中。各种模型再一次的简单学习一遍，将会有更加简单深刻认识。希望读者谅解<br>最后。如果文章中有不足之处，请务必指出，一定迅速改正。谢谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;零基础入门金融风控之贷款违约预测Task4：建模和调参&quot;&gt;&lt;a href=&quot;#零基础入门金融风控之贷款违约预测Task4：建模和调参&quot; class=&quot;headerlink&quot; title=&quot;零基础入门金融风控之贷款违约预测Task4：建模和调参&quot;&gt;&lt;/a&gt;零基础入门</summary>
      
    
    
    
    <category term="学习赛" scheme="http://example.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%9B/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
  <entry>
    <title>零基础入门金融风控之贷款违约预测Task3：特征工程</title>
    <link href="http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8BTask3%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    <id>http://example.com/2021/05/03/%E9%9B%B6%E5%9F%BA%E7%A1%80%E5%85%A5%E9%97%A8%E9%87%91%E8%9E%8D%E9%A3%8E%E6%8E%A7%E4%B9%8B%E8%B4%B7%E6%AC%BE%E8%BF%9D%E7%BA%A6%E9%A2%84%E6%B5%8BTask3%EF%BC%9A%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/</id>
    <published>2021-05-03T08:42:09.000Z</published>
    <updated>2021-05-05T11:55:32.353Z</updated>
    
    <content type="html"><![CDATA[<h1 id="零基础入门金融风控之贷款违约预测"><a href="#零基础入门金融风控之贷款违约预测" class="headerlink" title="零基础入门金融风控之贷款违约预测"></a>零基础入门金融风控之贷款违约预测</h1><h1 id="Task3：特征工程"><a href="#Task3：特征工程" class="headerlink" title="Task3：特征工程"></a>Task3：特征工程</h1><p>特征工程是数据处理中最为重要的一部分，也是变化最多一部分。如果把特征工程给处理好，那么就等于成功一半。<br>@[TOC]</p><p>导入需要的包</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"><span class="keyword">import</span> lightgbm <span class="keyword">as</span> lgb</span><br><span class="line"><span class="keyword">from</span> catboost <span class="keyword">import</span> CatBoostRegressor</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold, KFold</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score, f1_score, roc_auc_score, log_loss</span><br><span class="line">warnings.filterwarnings(<span class="string">&#x27;ignore&#x27;</span>)</span><br></pre></td></tr></table></figure><p>读取文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data_train =pd.read_csv(<span class="string">&#x27;./train.csv&#x27;</span>)</span><br><span class="line">data_test_a = pd.read_csv(<span class="string">&#x27;./testA.csv&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="特征预处理"><a href="#特征预处理" class="headerlink" title="特征预处理"></a>特征预处理</h4><p>首先我们查找出数据中的对象特征和数值特征<br>对数据需要有一定了解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">numerical_fea = <span class="built_in">list</span>(data_train.select_dtypes(exclude=[<span class="string">&#x27;object&#x27;</span>]).columns)</span><br><span class="line">category_fea = <span class="built_in">list</span>(<span class="built_in">filter</span>(<span class="keyword">lambda</span> x: x <span class="keyword">not</span> <span class="keyword">in</span> numerical_fea,<span class="built_in">list</span>(data_train.columns)))</span><br><span class="line">label = <span class="string">&#x27;isDefault&#x27;</span></span><br><span class="line">numerical_fea.remove(label)</span><br></pre></td></tr></table></figure><p>数据大概了解</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.info()</span><br></pre></td></tr></table></figure><pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;RangeIndex: 800000 entries, 0 to 799999Data columns (total 47 columns):id                    800000 non-null int64loanAmnt              800000 non-null float64term                  800000 non-null int64interestRate          800000 non-null float64installment           800000 non-null float64grade                 800000 non-null objectsubGrade              800000 non-null objectemploymentTitle       799999 non-null float64employmentLength      753201 non-null objecthomeOwnership         800000 non-null int64annualIncome          800000 non-null float64verificationStatus    800000 non-null int64issueDate             800000 non-null objectisDefault             800000 non-null int64purpose               800000 non-null int64postCode              799999 non-null float64regionCode            800000 non-null int64dti                   799761 non-null float64delinquency_2years    800000 non-null float64ficoRangeLow          800000 non-null float64ficoRangeHigh         800000 non-null float64openAcc               800000 non-null float64pubRec                800000 non-null float64pubRecBankruptcies    799595 non-null float64revolBal              800000 non-null float64revolUtil             799469 non-null float64totalAcc              800000 non-null float64initialListStatus     800000 non-null int64applicationType       800000 non-null int64earliesCreditLine     800000 non-null objecttitle                 799999 non-null float64policyCode            800000 non-null float64n0                    759730 non-null float64n1                    759730 non-null float64n2                    759730 non-null float64n2.1                  759730 non-null float64n4                    766761 non-null float64n5                    759730 non-null float64n6                    759730 non-null float64n7                    759730 non-null float64n8                    759729 non-null float64n9                    759730 non-null float64n10                   766761 non-null float64n11                   730248 non-null float64n12                   759730 non-null float64n13                   759730 non-null float64n14                   759730 non-null float64dtypes: float64(33), int64(9), object(5)memory usage: 286.9+ MB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看数值的特征</span></span><br><span class="line">numerical_fea</span><br></pre></td></tr></table></figure><pre><code>[&#39;id&#39;, &#39;loanAmnt&#39;, &#39;term&#39;, &#39;interestRate&#39;, &#39;installment&#39;, &#39;employmentTitle&#39;, &#39;homeOwnership&#39;, &#39;annualIncome&#39;, &#39;verificationStatus&#39;, &#39;purpose&#39;, &#39;postCode&#39;, &#39;regionCode&#39;, &#39;dti&#39;, &#39;delinquency_2years&#39;, &#39;ficoRangeLow&#39;, &#39;ficoRangeHigh&#39;, &#39;openAcc&#39;, &#39;pubRec&#39;, &#39;pubRecBankruptcies&#39;, &#39;revolBal&#39;, &#39;revolUtil&#39;, &#39;totalAcc&#39;, &#39;initialListStatus&#39;, &#39;applicationType&#39;, &#39;title&#39;, &#39;policyCode&#39;, &#39;n0&#39;, &#39;n1&#39;, &#39;n2&#39;, &#39;n2.1&#39;, &#39;n4&#39;, &#39;n5&#39;, &#39;n6&#39;, &#39;n7&#39;, &#39;n8&#39;, &#39;n9&#39;, &#39;n10&#39;, &#39;n11&#39;, &#39;n12&#39;, &#39;n13&#39;, &#39;n14&#39;]</code></pre><h4 id="缺失值处理"><a href="#缺失值处理" class="headerlink" title="缺失值处理"></a>缺失值处理</h4><ul><li>把所有缺失值替换成指定值</li><li>用上面值填充ffill</li><li>用下面值填充bfill<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看缺失值情况</span></span><br><span class="line">data_train.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure></li></ul><pre><code>id                        0loanAmnt                  0term                      0interestRate              0installment               0grade                     0subGrade                  0employmentTitle           1employmentLength      46799homeOwnership             0annualIncome              0verificationStatus        0issueDate                 0isDefault                 0purpose                   0postCode                  1regionCode                0dti                     239delinquency_2years        0ficoRangeLow              0ficoRangeHigh             0openAcc                   0pubRec                    0pubRecBankruptcies      405revolBal                  0revolUtil               531totalAcc                  0initialListStatus         0applicationType           0earliesCreditLine         0title                     1policyCode                0n0                    40270n1                    40270n2                    40270n2.1                  40270n4                    33239n5                    40270n6                    40270n7                    40270n8                    40271n9                    40270n10                   33239n11                   69752n12                   40270n13                   40270n14                   40270dtype: int64</code></pre><p>这里我们可以平均值填充也可以众数。注意mode方法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这里的mode方法是去每一列的众数</span></span><br><span class="line">data_train[category_fea].mode()</span><br></pre></td></tr></table></figure><div><style scoped>    .dataframe tbody tr th:only-of-type {        vertical-align: middle;    }<pre><code>.dataframe tbody tr th &#123;    vertical-align: top;&#125;.dataframe thead th &#123;    text-align: right;&#125;</code></pre><p></style></p><table border="1" class="dataframe">  <thead>    <tr style="text-align: right;">      <th></th>      <th>grade</th>      <th>subGrade</th>      <th>employmentLength</th>      <th>issueDate</th>      <th>earliesCreditLine</th>    </tr>  </thead>  <tbody>    <tr>      <td>0</td>      <td>B</td>      <td>C1</td>      <td>10+ years</td>      <td>2016-03-01</td>      <td>Aug-2001</td>    </tr>  </tbody></table></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#按照平均数填充数值型特征</span></span><br><span class="line">data_train[numerical_fea] = data_train[numerical_fea].fillna(data_train[numerical_fea].median())</span><br><span class="line">data_test_a[numerical_fea] = data_test_a[numerical_fea].fillna(data_train[numerical_fea].median())</span><br><span class="line"><span class="comment">#按照众数填充类别型特征</span></span><br><span class="line">data_train[category_fea] = data_train[category_fea].fillna(data_train[category_fea].mode())</span><br><span class="line">data_test_a[category_fea] = data_test_a[category_fea].fillna(data_train[category_fea].mode())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train.isnull().<span class="built_in">sum</span>()</span><br></pre></td></tr></table></figure><pre><code>id                        0loanAmnt                  0term                      0interestRate              0installment               0grade                     0subGrade                  0employmentTitle           0employmentLength      46799homeOwnership             0annualIncome              0verificationStatus        0issueDate                 0isDefault                 0purpose                   0postCode                  0regionCode                0dti                       0delinquency_2years        0ficoRangeLow              0ficoRangeHigh             0openAcc                   0pubRec                    0pubRecBankruptcies        0revolBal                  0revolUtil                 0totalAcc                  0initialListStatus         0applicationType           0earliesCreditLine         0title                     0policyCode                0n0                        0n1                        0n2                        0n2.1                      0n4                        0n5                        0n6                        0n7                        0n8                        0n9                        0n10                       0n11                       0n12                       0n13                       0n14                       0dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看类别特征</span></span><br><span class="line">category_fea</span><br></pre></td></tr></table></figure><pre><code>[&#39;grade&#39;, &#39;subGrade&#39;, &#39;employmentLength&#39;, &#39;issueDate&#39;, &#39;earliesCreditLine&#39;]</code></pre><h4 id="时间格式处理"><a href="#时间格式处理" class="headerlink" title="时间格式处理"></a>时间格式处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转化成时间格式</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data[<span class="string">&#x27;issueDate&#x27;</span>] = pd.to_datetime(data[<span class="string">&#x27;issueDate&#x27;</span>],<span class="built_in">format</span>=<span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">    startdate = datetime.datetime.strptime(<span class="string">&#x27;2007-06-01&#x27;</span>, <span class="string">&#x27;%Y-%m-%d&#x27;</span>)</span><br><span class="line">    <span class="comment">#构造时间特征</span></span><br><span class="line">    data[<span class="string">&#x27;issueDateDT&#x27;</span>] = data[<span class="string">&#x27;issueDate&#x27;</span>].apply(<span class="keyword">lambda</span> x: x-startdate).dt.days</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train[<span class="string">&#x27;employmentLength&#x27;</span>].value_counts(dropna=<span class="literal">False</span>).sort_index()</span><br></pre></td></tr></table></figure><pre><code>1 year        5248910+ years    2627532 years       723583 years       641524 years       479855 years       501026 years       372547 years       354078 years       361929 years       30272&lt; 1 year      64237NaN           46799Name: employmentLength, dtype: int64</code></pre><h4 id="对象特征转化数值特征"><a href="#对象特征转化数值特征" class="headerlink" title="对象特征转化数值特征"></a>对象特征转化数值特征</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">employmentLength_to_int</span>(<span class="params">s</span>):</span></span><br><span class="line">    <span class="keyword">if</span> pd.isnull(s):</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> np.int8(s.split()[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(to_replace=<span class="string">&#x27;10+ years&#x27;</span>, value=<span class="string">&#x27;10 years&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    data[<span class="string">&#x27;employmentLength&#x27;</span>].replace(<span class="string">&#x27;&lt; 1 year&#x27;</span>, <span class="string">&#x27;0 years&#x27;</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">    data[<span class="string">&#x27;employmentLength&#x27;</span>] = data[<span class="string">&#x27;employmentLength&#x27;</span>].apply(employmentLength_to_int)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;employmentLength&#x27;</span>].value_counts(dropna=<span class="literal">False</span>).sort_index()</span><br></pre></td></tr></table></figure><pre><code>0.0     159891.0     131822.0     182073.0     160114.0     118335.0     125436.0      93287.0      88238.0      89769.0      759410.0    65772NaN     11742Name: employmentLength, dtype: int64</code></pre><p>sample方法随机取这一列的5个值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_train[<span class="string">&#x27;earliesCreditLine&#x27;</span>].sample(<span class="number">5</span>)</span><br></pre></td></tr></table></figure><pre><code>250886    Nov-2000528953    Sep-2000703638    Jun-198980494     Mar-2009715192    Dec-2003Name: earliesCreditLine, dtype: object</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data[<span class="string">&#x27;earliesCreditLine&#x27;</span>] = data[<span class="string">&#x27;earliesCreditLine&#x27;</span>].apply(<span class="keyword">lambda</span> s: <span class="built_in">int</span>(s[-<span class="number">4</span>:]))</span><br></pre></td></tr></table></figure><h4 id="类别特征处理"><a href="#类别特征处理" class="headerlink" title="类别特征处理"></a>类别特征处理</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 部分类别特征</span></span><br><span class="line">cate_features = [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>, \</span><br><span class="line">                 <span class="string">&#x27;applicationType&#x27;</span>, <span class="string">&#x27;initialListStatus&#x27;</span>, <span class="string">&#x27;title&#x27;</span>, <span class="string">&#x27;policyCode&#x27;</span>]</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> cate_features:</span><br><span class="line">    <span class="built_in">print</span>(f, <span class="string">&#x27;类型数：&#x27;</span>, data[f].nunique())</span><br></pre></td></tr></table></figure><pre><code>grade 类型数： 7subGrade 类型数： 35employmentTitle 类型数： 79282homeOwnership 类型数： 6verificationStatus 类型数： 3purpose 类型数： 14postCode 类型数： 889regionCode 类型数： 51applicationType 类型数： 2initialListStatus 类型数： 2title 类型数： 12058policyCod，e 类型数： 1</code></pre><p>等级这种固定的类别特征可以使用匿名函数或者map去换比较方便。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data[<span class="string">&#x27;grade&#x27;</span>] = data[<span class="string">&#x27;grade&#x27;</span>].<span class="built_in">map</span>(&#123;<span class="string">&#x27;A&#x27;</span>:<span class="number">1</span>,<span class="string">&#x27;B&#x27;</span>:<span class="number">2</span>,<span class="string">&#x27;C&#x27;</span>:<span class="number">3</span>,<span class="string">&#x27;D&#x27;</span>:<span class="number">4</span>,<span class="string">&#x27;E&#x27;</span>:<span class="number">5</span>,<span class="string">&#x27;F&#x27;</span>:<span class="number">6</span>,<span class="string">&#x27;G&#x27;</span>:<span class="number">7</span>&#125;)</span><br></pre></td></tr></table></figure><p>类型数在2之上，又不是高维稀疏的,且纯分类特征</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类型数在2之上，又不是高维稀疏的,且纯分类特征</span></span><br><span class="line"><span class="comment">#get_dummies这个方法可以构建矩阵</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data = pd.get_dummies(data, columns=[<span class="string">&#x27;subGrade&#x27;</span>, <span class="string">&#x27;homeOwnership&#x27;</span>, <span class="string">&#x27;verificationStatus&#x27;</span>, <span class="string">&#x27;purpose&#x27;</span>, <span class="string">&#x27;regionCode&#x27;</span>], drop_first=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="异常值处理"><a href="#异常值处理" class="headerlink" title="异常值处理"></a>异常值处理</h4><ul><li>当我们在发现异常值得时候，一定要分清楚是什么原因导致异常值的。如果<strong>这异常值并不代表一种规律性</strong>话，是<strong>偶然的现象</strong>，或者说你并不想研究这种偶然的现象，这时可以将其<strong>删除</strong>。<strong>如果异常值存在且代表了一种真实存在的现象</strong>，那就不能随便删除。在现有的欺诈场景中很多时候欺诈数据本身相对于正常数据勒说就是异常的，我们要把这些异常点纳入，重新拟合模型，研究其规律。能用监督的用监督模型，不能用的还可以考虑用异常检测的算法来做。</li><li><strong>注意test的数据不能删</strong></li></ul><h5 id="对于检查异常方法之一：均方差"><a href="#对于检查异常方法之一：均方差" class="headerlink" title="对于检查异常方法之一：均方差"></a>对于检查异常方法之一：均方差</h5><p>基本上一些连续的数据都符合正态分布的</p><h5 id="方法二：箱型图"><a href="#方法二：箱型图" class="headerlink" title="方法二：箱型图"></a>方法二：箱型图</h5><p>将数据分成几个部分</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_outliers_by_3segama</span>(<span class="params">data,fea</span>):</span></span><br><span class="line">    data_std = np.std(data[fea])</span><br><span class="line">    data_mean = np.mean(data[fea])</span><br><span class="line">    outliers_cut_off = data_std * <span class="number">3</span></span><br><span class="line">    lower_rule = data_mean - outliers_cut_off</span><br><span class="line">    upper_rule = data_mean + outliers_cut_off</span><br><span class="line">    data[fea+<span class="string">&#x27;_outliers&#x27;</span>] = data[fea].apply(<span class="keyword">lambda</span> x:<span class="built_in">str</span>(<span class="string">&#x27;异常值&#x27;</span>) <span class="keyword">if</span> x &gt; upper_rule <span class="keyword">or</span> x &lt; lower_rule <span class="keyword">else</span> <span class="string">&#x27;正常值&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">data_train = data_train.copy()</span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> numerical_fea:</span><br><span class="line">    data_train = find_outliers_by_3segama(data_train,fea)</span><br><span class="line">    <span class="built_in">print</span>(data_train[fea+<span class="string">&#x27;_outliers&#x27;</span>].value_counts())</span><br><span class="line">    <span class="built_in">print</span>(data_train.groupby(fea+<span class="string">&#x27;_outliers&#x27;</span>)[<span class="string">&#x27;isDefault&#x27;</span>].<span class="built_in">sum</span>())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;*&#x27;</span>*<span class="number">10</span>)</span><br></pre></td></tr></table></figure><pre><code>正常值    800000Name: id_outliers, dtype: int64id_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    800000Name: loanAmnt_outliers, dtype: int64loanAmnt_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    800000Name: term_outliers, dtype: int64term_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    794259异常值      5741Name: interestRate_outliers, dtype: int64interestRate_outliers异常值      2916正常值    156694Name: isDefault, dtype: int64**********正常值    792046异常值      7954Name: installment_outliers, dtype: int64installment_outliers异常值      2152正常值    157458Name: isDefault, dtype: int64**********正常值    800000Name: employmentTitle_outliers, dtype: int64employmentTitle_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    799701异常值       299Name: homeOwnership_outliers, dtype: int64homeOwnership_outliers异常值        62正常值    159548Name: isDefault, dtype: int64**********正常值    793973异常值      6027Name: annualIncome_outliers, dtype: int64annualIncome_outliers异常值       756正常值    158854Name: isDefault, dtype: int64**********正常值    800000Name: verificationStatus_outliers, dtype: int64verificationStatus_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    783003异常值     16997Name: purpose_outliers, dtype: int64purpose_outliers异常值      3635正常值    155975Name: isDefault, dtype: int64**********正常值    798931异常值      1069Name: postCode_outliers, dtype: int64postCode_outliers异常值       221正常值    159389Name: isDefault, dtype: int64**********正常值    799994异常值         6Name: regionCode_outliers, dtype: int64regionCode_outliers异常值         1正常值    159609Name: isDefault, dtype: int64**********正常值    798440异常值      1560Name: dti_outliers, dtype: int64dti_outliers异常值       466正常值    159144Name: isDefault, dtype: int64**********正常值    778245异常值     21755Name: delinquency_2years_outliers, dtype: int64delinquency_2years_outliers异常值      5089正常值    154521Name: isDefault, dtype: int64**********正常值    788261异常值     11739Name: ficoRangeLow_outliers, dtype: int64ficoRangeLow_outliers异常值       778正常值    158832Name: isDefault, dtype: int64**********正常值    788261异常值     11739Name: ficoRangeHigh_outliers, dtype: int64ficoRangeHigh_outliers异常值       778正常值    158832Name: isDefault, dtype: int64**********正常值    790889异常值      9111Name: openAcc_outliers, dtype: int64openAcc_outliers异常值      2195正常值    157415Name: isDefault, dtype: int64**********正常值    792471异常值      7529Name: pubRec_outliers, dtype: int64pubRec_outliers异常值      1701正常值    157909Name: isDefault, dtype: int64**********正常值    794120异常值      5880Name: pubRecBankruptcies_outliers, dtype: int64pubRecBankruptcies_outliers异常值      1423正常值    158187Name: isDefault, dtype: int64**********正常值    790001异常值      9999Name: revolBal_outliers, dtype: int64revolBal_outliers异常值      1359正常值    158251Name: isDefault, dtype: int64**********正常值    799948异常值        52Name: revolUtil_outliers, dtype: int64revolUtil_outliers异常值        23正常值    159587Name: isDefault, dtype: int64**********正常值    791663异常值      8337Name: totalAcc_outliers, dtype: int64totalAcc_outliers异常值      1668正常值    157942Name: isDefault, dtype: int64**********正常值    800000Name: initialListStatus_outliers, dtype: int64initialListStatus_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    784586异常值     15414Name: applicationType_outliers, dtype: int64applicationType_outliers异常值      3875正常值    155735Name: isDefault, dtype: int64**********正常值    775134异常值     24866Name: title_outliers, dtype: int64title_outliers异常值      3900正常值    155710Name: isDefault, dtype: int64**********正常值    800000Name: policyCode_outliers, dtype: int64policyCode_outliers正常值    159610Name: isDefault, dtype: int64**********正常值    782773异常值     17227Name: n0_outliers, dtype: int64n0_outliers异常值      3485正常值    156125Name: isDefault, dtype: int64**********正常值    790500异常值      9500Name: n1_outliers, dtype: int64n1_outliers异常值      2491正常值    157119Name: isDefault, dtype: int64**********正常值    789067异常值     10933Name: n2_outliers, dtype: int64n2_outliers异常值      3205正常值    156405Name: isDefault, dtype: int64**********正常值    789067异常值     10933Name: n2.1_outliers, dtype: int64n2.1_outliers异常值      3205正常值    156405Name: isDefault, dtype: int64**********正常值    788660异常值     11340Name: n4_outliers, dtype: int64n4_outliers异常值      2476正常值    157134Name: isDefault, dtype: int64**********正常值    790355异常值      9645Name: n5_outliers, dtype: int64n5_outliers异常值      1858正常值    157752Name: isDefault, dtype: int64**********正常值    786006异常值     13994Name: n6_outliers, dtype: int64n6_outliers异常值      3182正常值    156428Name: isDefault, dtype: int64**********正常值    788430异常值     11570Name: n7_outliers, dtype: int64n7_outliers异常值      2746正常值    156864Name: isDefault, dtype: int64**********正常值    789625异常值     10375Name: n8_outliers, dtype: int64n8_outliers异常值      2131正常值    157479Name: isDefault, dtype: int64**********正常值    786384异常值     13616Name: n9_outliers, dtype: int64n9_outliers异常值      3953正常值    155657Name: isDefault, dtype: int64**********正常值    788979异常值     11021Name: n10_outliers, dtype: int64n10_outliers异常值      2639正常值    156971Name: isDefault, dtype: int64**********正常值    799434异常值       566Name: n11_outliers, dtype: int64n11_outliers异常值       112正常值    159498Name: isDefault, dtype: int64**********正常值    797585异常值      2415Name: n12_outliers, dtype: int64n12_outliers异常值       545正常值    159065Name: isDefault, dtype: int64**********正常值    788907异常值     11093Name: n13_outliers, dtype: int64n13_outliers异常值      2482正常值    157128Name: isDefault, dtype: int64**********正常值    788884异常值     11116Name: n14_outliers, dtype: int64n14_outliers异常值      3364正常值    156246Name: isDefault, dtype: int64**********</code></pre><p>删除异常值</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#删除异常值</span></span><br><span class="line"><span class="keyword">for</span> fea <span class="keyword">in</span> numerical_fea:</span><br><span class="line">    data_train = data_train[data_train[fea+<span class="string">&#x27;_outliers&#x27;</span>]==<span class="string">&#x27;正常值&#x27;</span>]</span><br><span class="line">    data_train = data_train.reset_index(drop=<span class="literal">True</span>) </span><br></pre></td></tr></table></figure><p>通过除法映射到间隔均匀的分箱中</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过除法映射到间隔均匀的分箱中，每个分箱的取值范围都是loanAmnt/1000</span></span><br><span class="line">data[<span class="string">&#x27;loanAmnt_bin1&#x27;</span>] = np.floor_divide(data[<span class="string">&#x27;loanAmnt&#x27;</span>], <span class="number">1000</span>)</span><br></pre></td></tr></table></figure><p>通过对数函数映射到指数宽度分箱</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 通过对数函数映射到指数宽度分箱</span></span><br><span class="line">data[<span class="string">&#x27;loanAmnt_bin2&#x27;</span>] = np.floor(np.log10(data[<span class="string">&#x27;loanAmnt&#x27;</span>]))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data[<span class="string">&#x27;loanAmnt_bin3&#x27;</span>] = pd.qcut(data[<span class="string">&#x27;loanAmnt&#x27;</span>], <span class="number">10</span>, labels=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">&#x27;grade&#x27;</span>, <span class="string">&#x27;subGrade&#x27;</span>]: </span><br><span class="line">    temp_dict = data_train.groupby([col])[<span class="string">&#x27;isDefault&#x27;</span>].agg([<span class="string">&#x27;mean&#x27;</span>]).reset_index().rename(columns=&#123;<span class="string">&#x27;mean&#x27;</span>: col + <span class="string">&#x27;_target_mean&#x27;</span>&#125;)</span><br><span class="line">    temp_dict.index = temp_dict[col].values</span><br><span class="line">    temp_dict = temp_dict[col + <span class="string">&#x27;_target_mean&#x27;</span>].to_dict()</span><br><span class="line"></span><br><span class="line">    data_train[col + <span class="string">&#x27;_target_mean&#x27;</span>] = data_train[col].<span class="built_in">map</span>(temp_dict)</span><br><span class="line">    data_test_a[col + <span class="string">&#x27;_target_mean&#x27;</span>] = data_test_a[col].<span class="built_in">map</span>(temp_dict)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 其他衍生变量 mean 和 std</span></span><br><span class="line"><span class="keyword">for</span> df <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> [<span class="string">&#x27;n0&#x27;</span>,<span class="string">&#x27;n1&#x27;</span>,<span class="string">&#x27;n2&#x27;</span>,<span class="string">&#x27;n2.1&#x27;</span>,<span class="string">&#x27;n4&#x27;</span>,<span class="string">&#x27;n5&#x27;</span>,<span class="string">&#x27;n6&#x27;</span>,<span class="string">&#x27;n7&#x27;</span>,<span class="string">&#x27;n8&#x27;</span>,<span class="string">&#x27;n9&#x27;</span>,<span class="string">&#x27;n10&#x27;</span>,<span class="string">&#x27;n11&#x27;</span>,<span class="string">&#x27;n12&#x27;</span>,<span class="string">&#x27;n13&#x27;</span>,<span class="string">&#x27;n14&#x27;</span>]:</span><br><span class="line">        df[<span class="string">&#x27;grade_to_mean_&#x27;</span> + item] = df[<span class="string">&#x27;grade&#x27;</span>] / df.groupby([item])[<span class="string">&#x27;grade&#x27;</span>].transform(<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">        df[<span class="string">&#x27;grade_to_std_&#x27;</span> + item] = df[<span class="string">&#x27;grade&#x27;</span>] / df.groupby([item])[<span class="string">&#x27;grade&#x27;</span>].transform(<span class="string">&#x27;std&#x27;</span>)</span><br></pre></td></tr></table></figure><h4 id="特征交互"><a href="#特征交互" class="headerlink" title="特征交互"></a>特征交互</h4><p>想要丰富特征，特别是对于线性模型而言，除了分箱外，另一种方法是添加原始数据的交互特征和多项式特征。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#label-encode:subGrade,postCode,title</span></span><br><span class="line"><span class="comment"># 高维类别特征需要进行转换</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> tqdm([<span class="string">&#x27;employmentTitle&#x27;</span>, <span class="string">&#x27;postCode&#x27;</span>, <span class="string">&#x27;title&#x27;</span>,<span class="string">&#x27;subGrade&#x27;</span>]):</span><br><span class="line">    le = LabelEncoder()</span><br><span class="line">    le.fit(<span class="built_in">list</span>(data_train[col].astype(<span class="built_in">str</span>).values) + <span class="built_in">list</span>(data_test_a[col].astype(<span class="built_in">str</span>).values))</span><br><span class="line">    data_train[col] = le.transform(<span class="built_in">list</span>(data_train[col].astype(<span class="built_in">str</span>).values))</span><br><span class="line">    data_test_a[col] = le.transform(<span class="built_in">list</span>(data_test_a[col].astype(<span class="built_in">str</span>).values))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;Label Encoding 完成&#x27;</span>)</span><br></pre></td></tr></table></figure><pre><code>100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:12&lt;00:00,  3.15s/it]Label Encoding 完成</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除不需要的数据</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> [data_train, data_test_a]:</span><br><span class="line">    data.drop([<span class="string">&#x27;issueDate&#x27;</span>], axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;纵向用缺失值上面的值替换缺失值&quot;</span></span><br><span class="line">data_train = data_train.fillna(axis=<span class="number">0</span>,method=<span class="string">&#x27;ffill&#x27;</span>)</span><br><span class="line">x_train = data_train.drop([<span class="string">&#x27;isDefault&#x27;</span>,<span class="string">&#x27;id&#x27;</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算协方差</span></span><br><span class="line">data_corr = x_train.corrwith(data_train.isDefault) <span class="comment">#计算相关性</span></span><br><span class="line">result = pd.DataFrame(columns=[<span class="string">&#x27;features&#x27;</span>, <span class="string">&#x27;corr&#x27;</span>])</span><br><span class="line">result[<span class="string">&#x27;features&#x27;</span>] = data_corr.index</span><br><span class="line">result[<span class="string">&#x27;corr&#x27;</span>] = data_corr.values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 也可以画图图</span></span><br><span class="line">data_numeric = data_train[numerical_fea]</span><br><span class="line">correlation = data_numeric.corr()</span><br><span class="line"></span><br><span class="line">f , ax = plt.subplots(figsize = (<span class="number">7</span>, <span class="number">7</span>))</span><br><span class="line">plt.title(<span class="string">&#x27;Correlation of Numeric Features with Price&#x27;</span>,y=<span class="number">1</span>,size=<span class="number">16</span>)</span><br><span class="line">sns.heatmap(correlation,square = <span class="literal">True</span>,  vmax=<span class="number">0.8</span>)</span><br></pre></td></tr></table></figure><pre><code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x23837a36f88&gt;</code></pre><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200921220247630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70#pic_center" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">features = [f <span class="keyword">for</span> f <span class="keyword">in</span> data_train.columns <span class="keyword">if</span> f <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">&#x27;id&#x27;</span>,<span class="string">&#x27;issueDate&#x27;</span>,<span class="string">&#x27;isDefault&#x27;</span>] <span class="keyword">and</span> <span class="string">&#x27;_outliers&#x27;</span> <span class="keyword">not</span> <span class="keyword">in</span> f]</span><br><span class="line">x_train = data_train[features]</span><br><span class="line">x_test = data_test_a[features]</span><br><span class="line">y_train = data_train[<span class="string">&#x27;isDefault&#x27;</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cv_model</span>(<span class="params">clf, train_x, train_y, test_x, clf_name</span>):</span></span><br><span class="line">    folds = <span class="number">5</span></span><br><span class="line">    seed = <span class="number">2020</span></span><br><span class="line">    kf = KFold(n_splits=folds, shuffle=<span class="literal">True</span>, random_state=seed)</span><br><span class="line"></span><br><span class="line">    train = np.zeros(train_x.shape[<span class="number">0</span>])</span><br><span class="line">    test = np.zeros(test_x.shape[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">    cv_scores = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, (train_index, valid_index) <span class="keyword">in</span> <span class="built_in">enumerate</span>(kf.split(train_x, train_y)):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;************************************ &#123;&#125; ************************************&#x27;</span>.<span class="built_in">format</span>(<span class="built_in">str</span>(i+<span class="number">1</span>)))</span><br><span class="line">        trn_x, trn_y, val_x, val_y = train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> clf_name == <span class="string">&quot;lgb&quot;</span>:</span><br><span class="line">            train_matrix = clf.Dataset(trn_x, label=trn_y)</span><br><span class="line">            valid_matrix = clf.Dataset(val_x, label=val_y)</span><br><span class="line"></span><br><span class="line">            params = &#123;</span><br><span class="line">                <span class="string">&#x27;boosting_type&#x27;</span>: <span class="string">&#x27;gbdt&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">                <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">                <span class="string">&#x27;num_leaves&#x27;</span>: <span class="number">2</span> ** <span class="number">5</span>,</span><br><span class="line">                <span class="string">&#x27;lambda_l2&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">                <span class="string">&#x27;feature_fraction&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_fraction&#x27;</span>: <span class="number">0.8</span>,</span><br><span class="line">                <span class="string">&#x27;bagging_freq&#x27;</span>: <span class="number">4</span>,</span><br><span class="line">                <span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.1</span>,</span><br><span class="line">                <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>,</span><br><span class="line">                <span class="string">&#x27;nthread&#x27;</span>: <span class="number">28</span>,</span><br><span class="line">                <span class="string">&#x27;n_jobs&#x27;</span>:<span class="number">24</span>,</span><br><span class="line">                <span class="string">&#x27;silent&#x27;</span>: <span class="literal">True</span>,</span><br><span class="line">                <span class="string">&#x27;verbose&#x27;</span>: -<span class="number">1</span>,</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            model = clf.train(params, train_matrix, <span class="number">50000</span>, valid_sets=[train_matrix, valid_matrix], verbose_eval=<span class="number">200</span>,early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line">            val_pred = model.predict(val_x, num_iteration=model.best_iteration)</span><br><span class="line">            test_pred = model.predict(test_x, num_iteration=model.best_iteration)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># print(list(sorted(zip(features, model.feature_importance(&quot;gain&quot;)), key=lambda x: x[1], reverse=True))[:20])</span></span><br><span class="line">                </span><br><span class="line">        <span class="keyword">if</span> clf_name == <span class="string">&quot;xgb&quot;</span>:</span><br><span class="line">            train_matrix = clf.DMatrix(trn_x , label=trn_y)</span><br><span class="line">            valid_matrix = clf.DMatrix(val_x , label=val_y)</span><br><span class="line">            </span><br><span class="line">            params = &#123;<span class="string">&#x27;booster&#x27;</span>: <span class="string">&#x27;gbtree&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;objective&#x27;</span>: <span class="string">&#x27;binary:logistic&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;eval_metric&#x27;</span>: <span class="string">&#x27;auc&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;gamma&#x27;</span>: <span class="number">1</span>,</span><br><span class="line">                      <span class="string">&#x27;min_child_weight&#x27;</span>: <span class="number">1.5</span>,</span><br><span class="line">                      <span class="string">&#x27;max_depth&#x27;</span>: <span class="number">5</span>,</span><br><span class="line">                      <span class="string">&#x27;lambda&#x27;</span>: <span class="number">10</span>,</span><br><span class="line">                      <span class="string">&#x27;subsample&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">                      <span class="string">&#x27;colsample_bytree&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">                      <span class="string">&#x27;colsample_bylevel&#x27;</span>: <span class="number">0.7</span>,</span><br><span class="line">                      <span class="string">&#x27;eta&#x27;</span>: <span class="number">0.04</span>,</span><br><span class="line">                      <span class="string">&#x27;tree_method&#x27;</span>: <span class="string">&#x27;exact&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;seed&#x27;</span>: <span class="number">2020</span>,</span><br><span class="line">                      <span class="string">&#x27;nthread&#x27;</span>: <span class="number">36</span>,</span><br><span class="line">                      <span class="string">&quot;silent&quot;</span>: <span class="literal">True</span>,</span><br><span class="line">                      &#125;</span><br><span class="line">            </span><br><span class="line">            watchlist = [(train_matrix, <span class="string">&#x27;train&#x27;</span>),(valid_matrix, <span class="string">&#x27;eval&#x27;</span>)]</span><br><span class="line">            </span><br><span class="line">            model = clf.train(params, train_matrix, num_boost_round=<span class="number">50000</span>, evals=watchlist, verbose_eval=<span class="number">200</span>, early_stopping_rounds=<span class="number">200</span>)</span><br><span class="line">            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)</span><br><span class="line">            test_pred = model.predict(test_x , ntree_limit=model.best_ntree_limit)</span><br><span class="line">                 </span><br><span class="line">        <span class="keyword">if</span> clf_name == <span class="string">&quot;cat&quot;</span>:</span><br><span class="line">            params = &#123;<span class="string">&#x27;learning_rate&#x27;</span>: <span class="number">0.05</span>, <span class="string">&#x27;depth&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;l2_leaf_reg&#x27;</span>: <span class="number">10</span>, <span class="string">&#x27;bootstrap_type&#x27;</span>: <span class="string">&#x27;Bernoulli&#x27;</span>,</span><br><span class="line">                      <span class="string">&#x27;od_type&#x27;</span>: <span class="string">&#x27;Iter&#x27;</span>, <span class="string">&#x27;od_wait&#x27;</span>: <span class="number">50</span>, <span class="string">&#x27;random_seed&#x27;</span>: <span class="number">11</span>, <span class="string">&#x27;allow_writing_files&#x27;</span>: <span class="literal">False</span>&#125;</span><br><span class="line">            </span><br><span class="line">            model = clf(iterations=<span class="number">20000</span>, **params)</span><br><span class="line">            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),</span><br><span class="line">                      cat_features=[], use_best_model=<span class="literal">True</span>, verbose=<span class="number">500</span>)</span><br><span class="line">            </span><br><span class="line">            val_pred  = model.predict(val_x)</span><br><span class="line">            test_pred = model.predict(test_x)</span><br><span class="line">            </span><br><span class="line">        train[valid_index] = val_pred</span><br><span class="line">        test = test_pred / kf.n_splits</span><br><span class="line">        cv_scores.append(roc_auc_score(val_y, val_pred))</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(cv_scores)</span><br><span class="line">        </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s_scotrainre_list:&quot;</span> % clf_name, cv_scores)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s_score_mean:&quot;</span> % clf_name, np.mean(cv_scores))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s_score_std:&quot;</span> % clf_name, np.std(cv_scores))</span><br><span class="line">    <span class="keyword">return</span> train, test</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lgb_model</span>(<span class="params">x_train, y_train, x_test</span>):</span></span><br><span class="line">    lgb_train, lgb_test = cv_model(lgb, x_train, y_train, x_test, <span class="string">&quot;lgb&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> lgb_train, lgb_test</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">xgb_model</span>(<span class="params">x_train, y_train, x_test</span>):</span></span><br><span class="line">    xgb_train, xgb_test = cv_model(xgb, x_train, y_train, x_test, <span class="string">&quot;xgb&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> xgb_train, xgb_test</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cat_model</span>(<span class="params">x_train, y_train, x_test</span>):</span></span><br><span class="line">    cat_train, cat_test = cv_model(CatBoostRegressor, x_train, y_train, x_test, <span class="string">&quot;cat&quot;</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lgb_train, lgb_test = lgb_model(x_train, y_train, x_test)</span><br></pre></td></tr></table></figure><pre><code>************************************ 1 ************************************[LightGBM] [Warning] num_threads is set with nthread=28, will be overridden by n_jobs=24. Current value: num_threads=24[LightGBM] [Warning] Unknown parameter: silentTraining until validation scores don&#39;t improve for 200 rounds[200]    training&#39;s auc: 0.749114    valid_1&#39;s auc: 0.729275[400]    training&#39;s auc: 0.764716    valid_1&#39;s auc: 0.730125[600]    training&#39;s auc: 0.778489    valid_1&#39;s auc: 0.729928Early stopping, best iteration is:[446]    training&#39;s auc: 0.768137    valid_1&#39;s auc: 0.730186[0.7301862239949224]************************************ 2 ************************************[LightGBM] [Warning] num_threads is set with nthread=28, will be overridden by n_jobs=24. Current value: num_threads=24[LightGBM] [Warning] Unknown parameter: silentTraining until validation scores don&#39;t improve for 200 rounds[200]    training&#39;s auc: 0.748999    valid_1&#39;s auc: 0.731035[400]    training&#39;s auc: 0.764879    valid_1&#39;s auc: 0.731436[600]    training&#39;s auc: 0.778506    valid_1&#39;s auc: 0.730823Early stopping, best iteration is:[414]    training&#39;s auc: 0.765823    valid_1&#39;s auc: 0.731478[0.7301862239949224, 0.7314779648434573]************************************ 3 ************************************[LightGBM] [Warning] num_threads is set with nthread=28, will be overridden by n_jobs=24. Current value: num_threads=24[LightGBM] [Warning] Unknown parameter: silentTraining until validation scores don&#39;t improve for 200 rounds[200]    training&#39;s auc: 0.748145    valid_1&#39;s auc: 0.73253[400]    training&#39;s auc: 0.763814    valid_1&#39;s auc: 0.733272[600]    training&#39;s auc: 0.777895    valid_1&#39;s auc: 0.733354Early stopping, best iteration is:[475]    training&#39;s auc: 0.769215    valid_1&#39;s auc: 0.73355[0.7301862239949224, 0.7314779648434573, 0.7335502065719879]************************************ 4 ************************************[LightGBM] [Warning] num_threads is set with nthread=28, will be overridden by n_jobs=24. Current value: num_threads=24[LightGBM] [Warning] Unknown parameter: silentTraining until validation scores don&#39;t improve for 200 rounds[200]    training&#39;s auc: 0.749417    valid_1&#39;s auc: 0.727507[400]    training&#39;s auc: 0.765066    valid_1&#39;s auc: 0.728261Early stopping, best iteration is:[353]    training&#39;s auc: 0.761647    valid_1&#39;s auc: 0.728349[0.7301862239949224, 0.7314779648434573, 0.7335502065719879, 0.7283491938614568]************************************ 5 ************************************[LightGBM] [Warning] num_threads is set with nthread=28, will be overridden by n_jobs=24. Current value: num_threads=24[LightGBM] [Warning] Unknown parameter: silentTraining until validation scores don&#39;t improve for 200 rounds[200]    training&#39;s auc: 0.748562    valid_1&#39;s auc: 0.73262[400]    training&#39;s auc: 0.764493    valid_1&#39;s auc: 0.733365Early stopping, best iteration is:[394]    training&#39;s auc: 0.764109    valid_1&#39;s auc: 0.733381[0.7301862239949224, 0.7314779648434573, 0.7335502065719879, 0.7283491938614568, 0.7333810157041901]lgb_scotrainre_list: [0.7301862239949224, 0.7314779648434573, 0.7335502065719879, 0.7283491938614568, 0.7333810157041901]lgb_score_mean: 0.7313889209952029lgb_score_std: 0.001966415347937543</code></pre><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20200921220620291.png#pic_center" alt="在这里插入图片描述"></p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>最后在此声明特征工程是数据挖掘中最为重要的一部分，在往往比赛中，还是符合八二原则。占据着最为重要一部分。一定要好好学习</p><p><strong>最后阐述一下特征选择</strong></p><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><p>特征选择技术可以精简掉无用的特征，以降低最终模型的复杂性，它的最终目的是得到一个简约模型，在不降低预测准确率或对预测准确率影响不大的情况下提高计算速度。特征选择不是为了减少训练时间（实际上，一些技术会增加总体训练时间），而是为了减少模型评分时间。<br>特征选择的方法：</p><p><strong>1、 Filter</strong></p><ul><li>方差选择法 ：方差选择法中，先要计算各个特征的方差，然后根据设定的阈值，选择方差大于阈值的特征</li><li>相关系数法（pearson 相关系数）：Pearson 相关系数 皮尔森相关系数是一种最简单的，可以帮助理解特征和响应变量之间关系的方法，该方法衡量的是变量之间的线性相关性。 结果的取值区间为 [-1，1] ， -1 表示完全的负相关， +1表示完全的正相关，0 表示没有线性相关。</li><li>卡方检验：经典的卡方检验是用于检验自变量对因变量的相关性。 假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距。 其统计量如下： χ2=∑(A−T)2T，其中A为实际值，T为理论值<br>(注：卡方只能运用在正定矩阵上，否则会报错Input X must be non-negative)</li><li>互信息法： 经典的互信息也是评价自变量对因变量的相关性的。 在feature_selection库的SelectKBest类结合最大信息系数法可以用于选择特征</li></ul><p><strong>2、Wrapper （RFE）</strong></p><ul><li>递归特征消除法：递归特征消除法 递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。</li></ul><p><strong>3、Embedded</strong> </p><ul><li>基于惩罚项的特征选择法： 基于惩罚项的特征选择法 使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。在feature_selection库的SelectFromModel类结合逻辑回归模型可以用于选择特征</li><li>基于树模型的特征选择：基于树模型的特征选择 树模型中GBDT也可用来作为基模型进行特征选择。 在feature_selection库的SelectFromModel类结合GBDT模型可以用于选择特征</li></ul><h4 id="结束语"><a href="#结束语" class="headerlink" title="结束语"></a>结束语</h4><p>本次任务三学习到不少知识，文章中如有不足之处，请务必指出，一定迅速改正。谢谢</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;a href=&quot;#零基础入门金融风控之贷款违约预测&quot; class=&quot;headerlink&quot; title=&quot;零基础入门金融风控之贷款违约预测&quot;&gt;&lt;/a&gt;零基础入门金融风控之贷款违约预测&lt;/h1&gt;&lt;h1 id=&quot;Task3：特征工</summary>
      
    
    
    
    <category term="学习赛" scheme="http://example.com/categories/%E5%AD%A6%E4%B9%A0%E8%B5%9B/"/>
    
    
  </entry>
  
  <entry>
    <title>爬取前程无忧招聘信息</title>
    <link href="http://example.com/2021/05/01/%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96/"/>
    <id>http://example.com/2021/05/01/%E5%89%8D%E7%A8%8B%E6%97%A0%E5%BF%A7%E6%8B%9B%E8%81%98%E4%BF%A1%E6%81%AF%E7%88%AC%E5%8F%96/</id>
    <published>2021-04-30T16:00:00.000Z</published>
    <updated>2021-05-05T12:22:03.293Z</updated>
    
    <content type="html"><![CDATA[<h1 id="爬取前程无忧招聘信息"><a href="#爬取前程无忧招聘信息" class="headerlink" title="爬取前程无忧招聘信息"></a>爬取前程无忧招聘信息</h1><p>本文是关于招聘数据爬取，我们选取的网站是前程无忧。<br>百度直接搜索前程无忧，或者51job。我们将看到搜索栏，在搜索栏中输入“数据分析师”将可以看到工作信息。<br>至于分析网站在这里就不在解释了，本爬虫只是简单爬取一点数据，所以并没有怎么做出伪装爬虫机制。所以本文仅供参考学习。如果真的对这网站想要爬取，请联系博主，我会详细写出一篇来，下面是代码和数据仅供参考。</p><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20210315212942724.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># !/usr/bin/python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">@File    :   qianchengwu_crab.py</span></span><br><span class="line"><span class="string">@Time    :   2020/03/15 21:21:18</span></span><br><span class="line"><span class="string">@Author  :   Qingxiang Zhang</span></span><br><span class="line"><span class="string">@Version :   1.0</span></span><br><span class="line"><span class="string">@Contact :   344285081@qq.com</span></span><br><span class="line"><span class="string">@Desc    :   </span></span><br><span class="line"><span class="string">@Software:    Vscode</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> urllib</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">60</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;正在爬取第&#123;&#125;页信息&#x27;</span>.<span class="built_in">format</span>(i))</span><br><span class="line"></span><br><span class="line">        baseurl = <span class="string">&quot;https://search.51job.com/list/000000,000000,0130%252c7501%252c7506%252c7502,01%252c32%252c38,9,99,%25E6%2595%25B0%25E6%258D%25AE%25E5%2588%2586%25E6%259E%2590%25E5%25B8%2588,2,&#123;&#125;.html&quot;</span>.<span class="built_in">format</span>(i)<span class="comment">#全国+keyword</span></span><br><span class="line">        html = askURL(baseurl)</span><br><span class="line">        <span class="comment"># print(html)</span></span><br><span class="line">        <span class="comment"># print(bs)</span></span><br><span class="line">        re_soup=re.search(<span class="string">r&#x27;window.__SEARCH_RESULT__ =(.*?)&lt;/script&gt;&#x27;</span>,html)</span><br><span class="line">        json_data=json.loads(re_soup.group(<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># print(json_data)</span></span><br><span class="line">        <span class="keyword">for</span> items <span class="keyword">in</span> json_data[<span class="string">&quot;engine_search_result&quot;</span>]:</span><br><span class="line"></span><br><span class="line">            job_name=items[<span class="string">&quot;job_name&quot;</span>]</span><br><span class="line">            <span class="comment"># print(job_name)</span></span><br><span class="line">            company_name=items[<span class="string">&quot;company_name&quot;</span>]</span><br><span class="line">            jobwelf=items[<span class="string">&quot;jobwelf&quot;</span>]</span><br><span class="line">            providesalary_text=items[<span class="string">&quot;providesalary_text&quot;</span>]           </span><br><span class="line">            <span class="comment">#存储成csv格式</span></span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;./result.csv&quot;</span>,<span class="string">&quot;a&quot;</span>,encoding=<span class="string">&quot;utf-8&quot;</span>,newline=<span class="string">&quot;&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                csv_write=csv.writer(f)</span><br><span class="line">                csv_write.writerow([job_name,company_name,providesalary_text,jobwelf])</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">askURL</span>(<span class="params">url</span>):</span></span><br><span class="line">    head = &#123;</span><br><span class="line">        <span class="string">&quot;User-Agent&quot;</span>: <span class="string">&quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.138 Safari/537.36&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    request = urllib.request.Request(url,headers=head)</span><br><span class="line">    html = <span class="string">&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        response = urllib.request.urlopen(request)</span><br><span class="line">        html = response.read().decode(<span class="string">&#x27;gbk&#x27;</span>, <span class="string">&#x27;ignore&#x27;</span>)</span><br><span class="line">        <span class="comment"># print(html)</span></span><br><span class="line">    <span class="keyword">except</span> urllib.error.URLError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(e, <span class="string">&quot;code&quot;</span>):</span><br><span class="line">            <span class="built_in">print</span>(e.code)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">hasattr</span>(e,<span class="string">&quot;reason&quot;</span>):</span><br><span class="line">            <span class="built_in">print</span>(e.reason)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> html</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>数据样式：</p><p><img src= "/img/loading.gif" data-lazy-src="https://img-blog.csdnimg.cn/20210315213547347.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NTAxNDYzNA==,size_16,color_FFFFFF,t_70" alt="在这里插入图片描述"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;爬取前程无忧招聘信息&quot;&gt;&lt;a href=&quot;#爬取前程无忧招聘信息&quot; class=&quot;headerlink&quot; title=&quot;爬取前程无忧招聘信息&quot;&gt;&lt;/a&gt;爬取前程无忧招聘信息&lt;/h1&gt;&lt;p&gt;本文是关于招聘数据爬取，我们选取的网站是前程无忧。&lt;br&gt;百度直接搜索前程无</summary>
      
    
    
    
    <category term="爬虫" scheme="http://example.com/categories/%E7%88%AC%E8%99%AB/"/>
    
    
    <category term="简单" scheme="http://example.com/tags/%E7%AE%80%E5%8D%95/"/>
    
  </entry>
  
</feed>
